---
title: "Life Expectancy Statistical Analysis"
author: "Marija Cveevska, Dejan Dichoski"
date: "2023-05-24"
output: 
  html_document:
    keep_md: yes
    toc: yes
    df_print: kable
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r, results="hide", message = FALSE, echo=FALSE}
library(ggplot2)
library(ggpubr)
library(GGally)
library(ggcorrplot)
library(plotly)
library(tidyverse)
library(cowplot)
library(psych)
library(lattice)
library(xtable)
library(plyr)
library(dplyr)
library(gridExtra)
library(WVPlots)
library(rcompanion)
library(knitr)
library(DescTools)
library(car)
library(caret)
library(leaps)
library(olsrr)
library(glmnet)
library(lmtest)
library(corrr)
library(ROSE)
library(e1071)
library(class)
library(MLmetrics)
library(pROC)
library(PRROC)
library(vcd)
library(kableExtra)
library(RColorBrewer)

```

<<<<<<< HEAD
## **Project Description**

### **Introduction**
=======
## Why is it important to have knowledge about life expectancy?
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

> Understanding life expectancy, which refers to the average duration a person is projected to live, holds significance as it serves as a comprehensive measure of community well-being. Factors such as elevated infant mortality rates, increased occurrences of suicide, limited access to quality healthcare, and other determinants can contribute to lower life expectancies. In addition, life expectancy finds various applications in the financial domain, encompassing areas such as life insurance, pension planning, and Social Security benefits.

Life expectancy prediction is valuable for insurance businesses and bank loans because it allows them to assess risk and make informed decisions.Accurate life expectancy prediction enables businesses to make more precise risk assessments and pricing decisions. It helps them manage their financial exposure, align their products with the appropriate risk levels, and ensure the long-term sustainability of their operations. 

Insurance Businesses:

 - Life Insurance: Life expectancy prediction helps insurance companies assess the risk associated with providing life insurance policies. By estimating life expectancy, they can determine the likelihood of a policyholder's lifespan exceeding the policy term. This information allows insurers to set appropriate premiums and policy terms based on the individual's life expectancy.
 - Annuities and Pension Plans: For annuities and pension plans, life expectancy prediction helps insurers calculate the expected payout duration. It allows them to estimate the length of time they will need to provide benefits, impacting the pricing and financial sustainability of these products.

Bank Loans:

 - Mortgage and Home Loans: Life expectancy prediction can be used to assess the risk associated with long-term loans such as mortgages. It helps lenders evaluate the probability of borrowers completing their loan terms based on their life expectancy. This information can influence loan approval decisions, interest rates, and loan terms.
 - Personal Loans: In cases where personal loans involve significant sums or extended repayment periods, life expectancy prediction can provide insights into the borrower's ability to repay the loan. It helps lenders evaluate the risk of default or early termination due to the borrower's life expectancy.

### **About the Dataset**

The aim of this work is to analyze a dataset related to life expectancy. It contains data for 193 countries, in a span of 15 years, which has been collected from the WHO data repository website and its corresponding economic data was collected from United Nation website. We obtained this dataset from Kaggle and it is publicly available for educational purposes. (TODO: Fix fillowing sentence when done with the analysis) The analysis will consist of data cleaning, exploratory data analysis (EDA), a simple case of linear regression, a more complete study of multiple linear regression and finally a binary classification problem.

We start our project by loading the dataset from the local drive to a dataframe.

```{r, echo=FALSE}
tryCatch({
  #LifeExp <- read.csv("C:/Users/Dejan/Desktop/StatProject_LifeExpectancy/LifeExpectancyData.csv")
  LifeExp <- read.csv("C:/Users/marij/Desktop/Statistical-Analysis-LifeExpectancy/LifeExpectancyData.csv")
  print("Dataset loaded.")
}, error = function(e) {
  print(paste("An error occurred while loading the dataset:", conditionMessage(e)))
})
cat(paste("Number of rows: ", nrow(LifeExp), "| Number of columns: ", ncol(LifeExp)))
```
<<<<<<< HEAD
Let's take a look at the Dataset:
=======

Let's take a look at the first 5 rows:
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

```{r}
kable(LifeExp) %>%
  kable_styling() %>%
  scroll_box(height = "300px")
```

In order to perform a successful analysis, it is important to properly understand the variables presented in the data. As we saw above, the dataset contains 22 variables (features). Let's start by answering the following question:\
- What does each variable mean and what type of variable is it (Nominal/Ordinal/Interval/Ratio)?


```{r, echo=FALSE}

# Define column names and descriptions
column_names <- c("Country", "Year", "Status", "Life expectancy", "Adult Mortality", "Infant deaths", 
                  "Alcohol", "Percentage expenditure", "Hepatitis B", "Measles", "BMI", "Under-five deaths",
                  "Polio", "Total expenditure", "Diphtheria", "HIV/AIDS", "GDP", "Population",
                  "Thinness 1-19 years", "Thinness 5-9 years", "Income composition of resources",
                  "Schooling")
column_types <- c("Nominal", "Ordinal", "Nominal", rep("Ratio", length(column_names)-3))


column_descriptions <- c("Country",
                         "Data is collected from 2000 - 2015 years",
                         "Developed or Developing status",
                         "Life Expectancy in age",
                         "Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)",
                         "Number of Infant Deaths per 1000 population",
                         "Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)",
                         "Expenditure on health as a percentage of Gross Domestic Product per capita(%)",
                         "Hepatitis B (HepB) immunization coverage among 1-year-olds (%)",
                         "Number of reported cases of Measles per 1000 population",
                         "Average Body Mass Index of the entire population",
                         "Number of under-five deaths per 1000 population",
                         "Polio (Pol3) immunization coverage among 1-year-olds (%)",
                         "General government expenditure on health as a percentage of total government expenditure (%)",
                         "Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)",
                         "Deaths per 1 000 live births due to HIV/AIDS (0-4 years)",
                         "Gross Domestic Product per capita (in USD)",
                         "Population of the country",
                         "Prevalence of thinness among children and adolescents for Age 10 to 19 (%)",
                         "Prevalence of thinness among children for Age 5 to 9 (%)",
                         "Human Development Index in terms of income composition of resources (index ranging from 0 to 1)",
                         "Number of years of Schooling (years)")


# Create a data frame with column names and descriptions
data_info <- data.frame(Column = column_names, Type = column_types, Description = column_descriptions)

# Render the data frame as a formatted table
knitr::kable(data_info, align = "l", col.names = c("Column Name", "Type", "Description"))

```


Let's check whether the columns of our dataframe correspond to the described types above. To do this, we use the command str() to look at the dataset structure:


```{r}
str(LifeExp)
```

The shown result corresponds to the given dataset description, which means that there were no problems when loading the data. So, we can begin the first part of our statistical analysis, which is data preparation (aka cleaning, wrangling).

## **Data Wrangling**

In this section we will carry out the cleaning of the data. First of all we will check whether some of the variables have missing values (in the form of NaN, NA and Null). If so, we will try to answer: what should be done about them? We will determine whether some columns shall be removed and whether some can be modified or created. After, we will focus our efforts on determining outliers and ways to deal with them.

Before we proceed with the missing values analysis, let's perform a couple of quick fixes to our data:\
- Rename the variable thinness..1.19.years to be more representative of its true meaning ("Prevalence of thinness among children and adolescents for Age 10 to 19 (%)").\
- Convert the character columns to factor columns

```{r}
colnames(LifeExp)[colnames(LifeExp) == "thinness..1.19.years"] <- "thinness.10.19.years"

#Convert a character column to a factor column
LifeExp$Year    <- as.factor(LifeExp$Year)
LifeExp$Country <- as.factor(LifeExp$Country)
LifeExp$Status  <- as.factor(LifeExp$Status)

str(LifeExp)
```
<<<<<<< HEAD
### **Checking unique values**
=======

### Checking unique values
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

Let's have a look at the unique values in our dataset.\
We do this just to double check that number of unique values corresponds to the dataset's description.

```{r}
# Apply the length function to unique values in each column using sapply()
num_unique_values <- sapply(LifeExp, function(x) length(unique(x)))

# Print the number of unique values for each column
for (i in 1 : ncol(LifeExp)) {
  column_name <- names(LifeExp)[i]
  num_unique <- num_unique_values[i]
  cat(column_name, ": ", num_unique, "\n")
}
```

<<<<<<< HEAD

### **Missing Values**
=======
### Missing Values
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

To address missing values, the following steps can be taken:\
1. Detection of missing values:
  - Identify null values in the dataset.
  - Consider if there are any alternative representations of missing values, such as zero values.
2. Dealing with missing values:
  - Decide whether to fill the null values through imputation or interpolation techniques.
  - Alternatively, consider removing the null values from the dataset if appropriate.

Lets first check the existence of explicit null values.

```{r}
is.null(LifeExp)
sum(is.na(LifeExp))
sum(is.nan(as.matrix(LifeExp)))
```

```{r}
colSums(is.na(LifeExp))
```

As we can see most of the columns contain missing values (NA), resulting in a total of 2563 missing entries. Before we decide how to deal with them, let's check whether there are some wrong entries, which we call inexplicit nulls. The simplest and fastest way to check for such entries would be to do a quick summary() and look at each variable separately and see if the values make sense based on their descriptions.

```{r}
summary(LifeExp)
```

Judging by the summary for the variables, it seems that some values in the dataset may be inaccurate or require further investigation. Here's a breakdown of the potential issues:

1. Adult mortality of 1: It's highly unlikely for adult mortality to be as low as 1. This could indicate a measurement error. We can set a certain threshold and consider all the values below it as null.

2. Infant deaths of 0 and 1800: Having zero infant deaths per 1000 births is implausible, so it's reasonable to consider such values as null. On the other hand, 1800 infant deaths may be an outlier but could be possible in certain circumstances, such as high birthrates and a relatively small population. Further analysis is needed to determine if it's an outlier or an accurate measurement.

3. BMI of 1 and 87.3: These values seem unrealistic, as it would indicate extreme underweight or obesity across an entire population. These values may be inaccurate or outliers. Considering the nature of this variable, it might be worth investigating further or even disregarding if the data quality is questionable.

4. Under Five Deaths at zero: Similar to infant deaths, reporting zero deaths for children under five is highly unlikely. It's reasonable to treat such values as null or investigate the data source for clarification.

5. GDP per capita of 1.68: Extremely low GDP values like 1.68 (in USD) may be implausible. These low values could be outliers or inaccuracies. Further analysis and comparison with reliable sources could help determine their validity.

6. Population of 34: A population value of 34 for an entire country appears questionable. It's likely an outlier or an error in measurement that requires verification or further investigation.

It's important to note that these are assumptions based on the provided summary statistics and our domain knowledge. More thorough analysis may be necessary to make accurate conclusions about the data's quality and identify the best approach for handling these inconsistencies.

To gain a deeper understanding of these values, let's visualize them using boxplots for each variable.

```{r, fig.align = "center"}
par(mfrow = c(2, 3))
variables <- c('Adult.Mortality', 'infant.deaths', 'BMI', 'under.five.deaths', 'GDP', 'Population')

for (i in 1:length(variables)) {
  boxplot(LifeExp[, variables[i]], main = variables[i])
}
```

After examining the mentioned values, it appears that some of them could potentially be outliers, while others are most likely errors. To address these inconsistencies, the following modifications will be made, considering that the values are implausible:

- Adult mortality rates lower than the 5th percentile will be treated as null.\
- Infant deaths of 0 will be considered null.\
- BMIs below 10 and above 50 will be considered null.\
- Under Five deaths of 0 will be considered null.\
By making these adjustments, we can handle the values that do not align with reasonable expectations.

```{r}
num_na_before <- sum(colSums(is.na(LifeExp)))
mort_5_percentile <- quantile(LifeExp$Adult.Mortality[!is.na(LifeExp$Adult.Mortality)], probs = 0.05)
LifeExp$Adult.Mortality <- ifelse(LifeExp$Adult.Mortality < mort_5_percentile, NA, LifeExp$Adult.Mortality)
LifeExp$infant.deaths <- ifelse(LifeExp$infant.deaths == 0, NA, LifeExp$infant.deaths)
LifeExp$BMI <- ifelse(LifeExp$BMI < 10 | LifeExp$BMI > 50, NA, LifeExp$BMI)
LifeExp$under.five.deaths <- ifelse(LifeExp$under.five.deaths == 0, NA, LifeExp$under.five.deaths)
```

```{r}
cat(num_na_before, sum(colSums(is.na(LifeExp))))
```

As we can see, we increased the number of missing values from 2563 to 5763. There seems to be a significant number of null values in the dataset. For now, it might be helpful to analyze separately just the columns that contain nulls to gain more insights. The following function aims to achieve this. It identifies and presents the columns that have explicit null values, tracks the total count of such columns along with their positions in the dataframe, and provides the count and percentage of nulls for each specified column.


```{r}
nulls_breakdown <- function(df) {
  df_cols <- colnames(df)
  cols_total_count <- length(df_cols)
  cols_count <- 0
  for (loc in 1:length(df_cols)) {
    col <- df_cols[loc]
    null_count <- sum(is.na(df[[col]]))
    total_count <- length(df[[col]])
    percent_null <- round(null_count / total_count * 100, 2)
    if (null_count > 0) {
      cols_count <- cols_count + 1
      cat("[", loc - 1, "] ", col, " has ", null_count, " null values: ", percent_null, "% null\n", sep = "")
    }
  }
  cols_percent_null <- round(cols_count / cols_total_count * 100, 2)
  cat("Out of ", cols_total_count, " total columns, ", cols_count, " contain null values; ", cols_percent_null, "% columns contain null values.\n", sep = "")
}

nulls_breakdown(LifeExp)
```

#### **Dealing with missing values**

Given that approximately half of the values in the BMI variable are null, we decided that it would be the best to exclude this variable from the analysis.

```{r}
LifeExp <- LifeExp[, !(colnames(LifeExp) %in% c('BMI'))]
```

When dealing with NA values, there are multiple approaches that can be considered:

- Fill with the mean value: Replace the NA values with the mean value of the respective variable.
- Dropping:
  - Drop all rows that contain at least one NA value: Remove the rows from the dataset that have any NA values.
  - Drop rows that contain more than 50% NA values: Exclude the rows that have more than 50% NA values, while keeping the remaining rows. The NA values can be filled with the mean value.
- Interpolation: If the data represents a time series, interpolation techniques can be applied to estimate the missing values based on the existing data points.

Since we are dealing with time series data assorted by country, the best approach would be to interpolate the data by country. However, from a careful examination of the dataframe it can be seen that there are columns that have null values for each year for some countries. Therefore, we resort to imputation by year, as the second best possible method. This involves replacing the null values with the mean value of each respective year. Imputation of each year's mean is done in the following code snippet:

```{r}
imputed_data <- list()

for (year in unique(LifeExp$Year)) {
  year_data <- LifeExp[LifeExp$Year == year, ]
  for (col in colnames(year_data)[4:length(colnames(year_data))]) {
    year_data[, col] <- ifelse(is.na(year_data[, col]), mean(year_data[, col], na.rm = TRUE), year_data[, col])
  }
  imputed_data <- append(imputed_data, list(year_data))
}

LifeExp <- bind_rows(imputed_data)

nulls_breakdown(LifeExp)
```

As we can see the missing values have been successfully handled using the interpolation method. We can now proceed to address the issue of outliers. Outliers can have a significant impact on data analysis and modeling, and it is important to identify and handle them appropriately.

### **Outlier analysis**

Outlier analysis is an essential step in exploring and understanding life expectancy data. It helps ensure data quality, identify anomalies and patterns, improve statistical analysis, inform policy decisions, and enhance data visualization and reporting.
Outliers increase the error variance and reduce the power of statistical tests. They can cause bias and/or influence estimates. They can also impact the basic assumption of regression as well as other statistical models.

Similar to missing values, to address the presence of outliers in the data, several steps can be taken:

1. Outlier detection: Identify the outliers in the dataset using appropriate statistical methods or techniques.
  - Visualization: Create boxplots and histograms to visually inspect the distribution of the data and identify any potential outliers.
  - Tukey's method: Apply Tukey's method or other outlier detection algorithms to determine the presence of outliers based on statistical thresholds or criteria.
2. Outlier treatment: Decide on the approach for handling outliers. Options include:
  - Dropping outliers: Remove the outlier observations from the dataset.
  - Limiting/Winsorizing outliers: Cap the extreme values at a predetermined threshold.
  - Transforming the data: Apply transformations such as logarithmic, inverse, or square root transformations to reduce the impact of outliers.

Each of these steps aims to identify, assess, and appropriately handle the outliers in the dataset to ensure robust analysis and accurate interpretations.

<<<<<<< HEAD

#### **Outlier detection**

=======
#### Outlier detection
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

For each continuous variable in the dataset, a boxplot will be generated, displaying the median, quartiles, and any potential outliers. Additionally, histograms will be created to visualize the frequency distribution of the variable.

By examining these plots, we can visually inspect the data and determine if there are any data points that deviate significantly from the majority of the observations, indicating the presence of potential outliers.


```{r,echo = FALSE, message=FALSE, fig.align = "center"}

p1 <- ggplot(LifeExp, aes(x=Life.expectancy)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b1 <- ggplot(LifeExp, aes(x=Life.expectancy)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Life Expectancy")

p2 <- ggplot(LifeExp, aes(x=Adult.Mortality)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b2 <- ggplot(LifeExp, aes(x=Adult.Mortality)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Adult Mortality")

p3 <- ggplot(LifeExp, aes(x=infant.deaths)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b3 <- ggplot(LifeExp, aes(x=infant.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Infant Deaths")

p4 <- ggplot(LifeExp, aes(x=Alcohol)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b4 <- ggplot(LifeExp, aes(x=Alcohol)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Alcohol")

p5 <- ggplot(LifeExp, aes(x=percentage.expenditure)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b5 <- ggplot(LifeExp, aes(x=percentage.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Percentage Expenditure")

p6 <- ggplot(LifeExp, aes(x=Hepatitis.B)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b6 <- ggplot(LifeExp, aes(x=Hepatitis.B)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Hepatitis.B")

p7 <- ggplot(LifeExp, aes(x=Measles)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b7 <- ggplot(LifeExp, aes(x=Measles)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Measles")

p8 <- ggplot(LifeExp, aes(x=Schooling)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b8 <- ggplot(LifeExp, aes(x=Schooling)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Schooling")

p9 <- ggplot(LifeExp, aes(x=under.five.deaths)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b9 <- ggplot(LifeExp, aes(x=under.five.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("under five deaths")

p10 <- ggplot(LifeExp, aes(x=Polio)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b10 <- ggplot(LifeExp, aes(x=Polio)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Polio")

p11 <- ggplot(LifeExp, aes(x=Total.expenditure)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b11 <- ggplot(LifeExp, aes(x=Total.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Total.expenditure")

p12 <- ggplot(LifeExp, aes(x=Diphtheria)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b12 <- ggplot(LifeExp, aes(x=Diphtheria)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Diphtheria")

p13 <- ggplot(LifeExp, aes(x=HIV.AIDS)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b13 <- ggplot(LifeExp, aes(x=HIV.AIDS)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("HIV / AIDS")

p14 <- ggplot(LifeExp, aes(x=GDP)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b14 <- ggplot(LifeExp, aes(x=GDP)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("GDP")

p15 <- ggplot(LifeExp, aes(x=Population)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b15 <- ggplot(LifeExp, aes(x=Population)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Population")

p16 <- ggplot(LifeExp, aes(x=thinness.10.19.years)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b16 <- ggplot(LifeExp, aes(x=thinness.10.19.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.10.19.years")

p17 <- ggplot(LifeExp, aes(x=thinness.5.9.years)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b17 <- ggplot(LifeExp, aes(x=thinness.5.9.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.5.9.years")

p18 <- ggplot(LifeExp, aes(x=Income.composition.of.resources)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b18 <- ggplot(LifeExp, aes(x=Income.composition.of.resources)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Income per resources")

grid.arrange(p1, b1, p2, b2, p3, b3, p4, b4, p5, b5, p6, b6,p7, b7, p8, b8)
grid.arrange(p9, b9,p10, b10, p11, b11, p12, b12, p14, b14,p15, b15, p17, b17, p18, b18)
```

Visually inspecting the data, it is evident that there are several outliers present in the variables, including the target variable (life expectancy). To statistically identify outliers, *Tukey's method* can be applied. This method defines outliers as values that lie outside 1.5 times the interquartile range (IQR). By calculating the quartiles and IQR for each variable, the upper and lower bounds can be determined, and any values outside these bounds can be classified as outliers.


```{r, echo=FALSE}
LifeExp_copy <- LifeExp
```

```{r}
cont_vars <- colnames(LifeExp)[4:length(colnames(LifeExp))]

outlier_counts <- sapply(LifeExp[cont_vars], function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  outliers <- x < lower_bound | x > upper_bound
  sum(outliers, na.rm = TRUE)
})

outlier_percent <- outlier_counts / nrow(LifeExp) * 100

outliers_df <- data.frame(OutlierCount = outlier_counts, OutlierPercent = outlier_percent)
kable(outliers_df, format = "markdown")
```

There seem to be a considerable number of outliers present in this dataset. Now that they have been identified, a question arises: What course of action should be taken regarding them?


#### **Dealing with Outliers**

There are several approaches to handling outliers in a dataset, and the typical options are as follows:

1. Discard Outliers (preferably avoided to retain maximum information):
   - This involves removing the data points that are considered outliers.
  
2. Apply Boundaries (Winsorization):
   - Set upper and/or lower limits for the values, effectively capping extreme values without removing them entirely.

3. Data Transformation:
   - Utilize mathematical transformations such as logarithmic, inverse, square root, etc.
   - Advantages: Can normalize the data and potentially eliminate outliers.
   - Disadvantages: Cannot be applied to variables containing values of zero or below, as it may lead to undefined results.


We attempted to handle outliers using the last approach, which involved applying square root and logarithmic transformations to the LifeExp dataset. However, we found that these methods did not improve the situation. Specifically, when using the log transformation, the number of outliers remained the same and when employing the sqrt transformation, the number of outliers actually increased.

Given that each variable in the dataset exhibits a unique number of outliers and these outliers are distributed on different sides of the data, the most suitable approach would likely involve Winsorizing (limiting) the values for each variable individually until no outliers remain. The provided function bellow enables this process by iterating through each variable and allowing the specification of lower and/or upper limits for Winsorization. By default, the function generates two boxplots side by side for each variable, depicting the original data and the Winsorized data, respectively. Once a satisfactory limit is determined through visual analysis, the Winsorized data is saved in the wins_dict dictionary for convenient future access.

```{r}
test_wins <- function(col, lower_limit = 0, upper_limit = 0, show_plot = TRUE) {
  LifeExp_original <- LifeExp[[col]]
  q_low <- quantile(LifeExp[[col]], lower_limit)
  q_up <- quantile(LifeExp[[col]], 1 - upper_limit)
  LifeExp[[col]] <<- pmin(pmax(LifeExp[[col]], q_low), q_up)
  # <<- Allows us to modify variables outside of the local scope of a function.
  
  if (show_plot) {
    ylim <- range(LifeExp_original, na.rm = TRUE)
    par(mfrow = c(1, 2))
    boxplot(LifeExp_original, main = paste0("original\n ", col), ylim = ylim)
    boxplot(LifeExp[[col]], main = paste0("wins=(", lower_limit, ",", upper_limit, ")\n", col), ylim = ylim)
  }
}

test_wins(cont_vars[1], lower_limit = 0.01, show_plot = FALSE)
test_wins(cont_vars[2], upper_limit = 0.04, show_plot = FALSE)
test_wins(cont_vars[3], upper_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[4], upper_limit = 0.0025, show_plot = FALSE)
test_wins(cont_vars[5], upper_limit = 0.135, show_plot = FALSE)
test_wins(cont_vars[6], lower_limit = 0.1, show_plot = FALSE)
test_wins(cont_vars[7], upper_limit = 0.19, show_plot = FALSE)
test_wins(cont_vars[8], upper_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[9], lower_limit = 0.1, show_plot = FALSE)
test_wins(cont_vars[10], upper_limit = 0.02, show_plot = FALSE)
test_wins(cont_vars[11], lower_limit = 0.105, show_plot = FALSE)
test_wins(cont_vars[12], upper_limit = 0.185, show_plot = FALSE)
test_wins(cont_vars[13], upper_limit = 0.105, show_plot = FALSE)
test_wins(cont_vars[14], upper_limit = 0.07, show_plot = FALSE)
test_wins(cont_vars[15], upper_limit = 0.035, show_plot = FALSE)
test_wins(cont_vars[16], upper_limit = 0.035, show_plot = FALSE)
test_wins(cont_vars[17], lower_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[18], lower_limit = 0.025, upper_limit = 0.005, show_plot = FALSE)
```


```{r,echo = FALSE, message=FALSE, fig.align = "center"}


b1 <- ggplot(LifeExp, aes(x=Life.expectancy)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Life Expectancy")

b1_old <- ggplot(LifeExp_copy, aes(x=Life.expectancy)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Life Expectancy")

b2 <- ggplot(LifeExp, aes(x=Adult.Mortality)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Adult Mortality")

b2_old <- ggplot(LifeExp_copy, aes(x=Adult.Mortality)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Adult Mortality")

b3 <- ggplot(LifeExp, aes(x=infant.deaths)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Infant Deaths")

b3_old <- ggplot(LifeExp_copy, aes(x=infant.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Infant Deaths")

b4 <- ggplot(LifeExp, aes(x=Alcohol)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Alcohol")

b4_old <- ggplot(LifeExp_copy, aes(x=Alcohol)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Alcohol")

b5 <- ggplot(LifeExp, aes(x=percentage.expenditure)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Percentage Expenditure")

b5_old <- ggplot(LifeExp_copy, aes(x=percentage.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Percentage Expenditure")

b6 <- ggplot(LifeExp, aes(x=Hepatitis.B)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Hepatitis.B")

b6_old <- ggplot(LifeExp_copy, aes(x=Hepatitis.B)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Hepatitis.B")

b7 <- ggplot(LifeExp, aes(x=Measles)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Measles")

b7_old <- ggplot(LifeExp_copy, aes(x=Measles)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Measles")

b8 <- ggplot(LifeExp, aes(x=Schooling)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Schooling")

b8_old <- ggplot(LifeExp_copy, aes(x=Schooling)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Schooling")

b9 <- ggplot(LifeExp, aes(x=under.five.deaths)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("under five deaths")

b9_old <- ggplot(LifeExp_copy, aes(x=under.five.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("under five deaths")

b10 <- ggplot(LifeExp, aes(x=Polio)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Polio")

b10_old <- ggplot(LifeExp_copy, aes(x=Polio)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Polio")


b11 <- ggplot(LifeExp, aes(x=Total.expenditure)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Total.expenditure")

b11_old <- ggplot(LifeExp_copy, aes(x=Total.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Total.expenditure")

b12 <- ggplot(LifeExp, aes(x=Diphtheria)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Diphtheria")

b12_old <- ggplot(LifeExp_copy, aes(x=Diphtheria)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Diphtheria")


b13 <- ggplot(LifeExp, aes(x=HIV.AIDS)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("HIV / AIDS")

b13_old <- ggplot(LifeExp_copy, aes(x=HIV.AIDS)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("HIV / AIDS")


b14 <- ggplot(LifeExp, aes(x=GDP)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("GDP")

b14_old <- ggplot(LifeExp_copy, aes(x=GDP)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("GDP")


b15 <- ggplot(LifeExp, aes(x=Population)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Population")

b15_old <- ggplot(LifeExp_copy, aes(x=Population)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Population")


b16 <- ggplot(LifeExp, aes(x=thinness.10.19.years)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("thinness.10.19.years")

b16_old <- ggplot(LifeExp_copy, aes(x=thinness.10.19.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.10.19.years")


b17 <- ggplot(LifeExp, aes(x=thinness.5.9.years)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("thinness.5.9.years")

b17_old <- ggplot(LifeExp_copy, aes(x=thinness.5.9.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.5.9.years")


b18 <- ggplot(LifeExp, aes(x=Income.composition.of.resources)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Income per resources")

b18_old <- ggplot(LifeExp_copy, aes(x=Income.composition.of.resources)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Income per resources")

grid.arrange(b1_old, b1,b3_old, b3, b14_old, b14, b15_old, b15, ncol = 2)
```

Let's also compare the before and after distributions of some of our variables.
We can see that the top two graphs (Life Expectancy and Schooling) along with Alcohol, Hepatitis.B, Total.expenditure, that we decide not to show for simplicity, kept the same distribution even after Winsorization. On the opposite case the bottom two graphs (GDP and Polio) along with infant.deaths,percentage.expenditure,Measles, under.five,deaths, drastically changed after Winsorization. 


```{r, fig.align = "center"}
p1 <- ggplot() +
  geom_density(data = LifeExp_copy, aes(x = Life.expectancy), color = 5, linewidth = 1.2) +
  geom_density(data = LifeExp, aes(x = Life.expectancy), color = 2, linewidth = 1, linetype = "twodash") +
  xlab("Life Expectancy Winsorization")

p2 <- ggplot() +
  geom_density(data = LifeExp_copy, aes(x = Schooling), color = 5, linewidth = 1.2) +
  geom_density(data = LifeExp, aes(x = Schooling), color = 2, linewidth = 1, linetype = "twodash") +
  xlab("Schooling Winsorization")

p3 <- ggplot() +
  geom_density(data = LifeExp_copy, aes(x = GDP), color = 5, linewidth = 1.2) +
  geom_density(data = LifeExp, aes(x = GDP), color = 2, linewidth = 1, linetype = "twodash") +
  xlab("GDP Winsorization")

p4 <- ggplot() +
  geom_density(data = LifeExp_copy, aes(x = Polio), color = 5, linewidth = 1.2) +
  geom_density(data = LifeExp, aes(x = Polio), color = 2, linewidth = 1, linetype = "twodash") +
  xlab("Polio Winsorization")

# Combine the plots using grid.arrange
grid.arrange(p1, p2, p3, p4, ncol = 2)

```


<<<<<<< HEAD
## **Exploratory Data Analysis**

Exploratory Data Analysis (EDA) is a crucial process that involves conducting preliminary investigations on data, utilizing summary statistics and visual representations. Its primary objective is to gain a comprehensive understanding of the data's quality and characteristics before delving into formal statistical modeling.
=======
## Exploratory Data Analysis
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

EDA serves as a means to extract valuable insights from the data by:

- Uncovering Patterns: EDA aids in revealing underlying patterns within the data, offering guidance on how it can be effectively modeled. By examining distributions, correlations, and trends, EDA assists in identifying potential relationships and dependencies among variables.

- Inspiring New Directions: Through EDA, researchers gain inspiration for further scientific inquiries. By exploring various aspects of the data, unexpected relationships or intriguing phenomena may emerge, prompting researchers to pursue novel research directions and hypotheses.

- Detecting Errors: EDA acts as a vigilant eye, capable of identifying evident errors or inconsistencies within the dataset. By visualizing data points, outliers, or illogical values, EDA helps ensure data integrity and accuracy.

- Assessing Assumptions: EDA allows for the assessment of underlying assumptions required for formal analyses. By examining the distributional properties of variables, assessing normality, or exploring heterogeneity, EDA helps evaluate the plausibility of assumptions necessary for subsequent statistical modeling.

In the following part we will be answering some questions related to our Dataset.

### **Relevant Questions**

#### **Q1. Can we say that Developed countries have more average life expectancy than Developing countries?**

```{r, fig.align = "center"}
ggplot(LifeExp, aes(x = Status, fill = Status)) +
  geom_bar(alpha = 0.6) +
  scale_fill_manual(values = c(2, 5)) +
  labs(title = "Status of Country", x = "Status", y = "Count") +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 8),
        axis.text.y = element_text(size = 8),
        axis.title.y = element_text(size = 8))
```

<<<<<<< HEAD

```{r, fig.align = "center"}
=======
```{r}
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6
ggplot(LifeExp, aes(x=Life.expectancy, fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "Life Expectancy by Status", x ="Life Expectancy", y="Density") +
  scale_fill_manual(values = c(2, 5))

```

Due to our lack of knowledge regarding the population variance, we will employ a two-sample T-Test instead of a two-sample Z-Test in order to assess the equality of the two means. Prior to conducting the T-Test, it is necessary to determine whether the variances of the two populations are equal. To accomplish this, we will employ an F-Test.

To start, we will filter and group the data by country, allowing us to calculate the average life expectancy for each country over the span of 16 years.

```{r}

Developing_X <- aggregate(Life.expectancy ~ Country, data = LifeExp %>% filter (Status == "Developing"), FUN = mean)
Developed_Y <- aggregate(Life.expectancy ~ Country, data = LifeExp %>% filter (Status == "Developed"), FUN = mean)
```

```{r}
kable(Developing_X[1:5, ],format = "markdown")

kable(Developed_Y[1:5, ],format = "markdown")
```

```{r}
var.test(Developed_Y$Life.expectancy,Developing_X$Life.expectancy)
```

<<<<<<< HEAD

The observed p-value is less than alpha(0.05 by default). Hence we reject the null hypothesis and accept the alternate statement that the variance of two populations is not equal.
=======
The observed p-value is less than alpha (0.05 by default). Hence we reject the null hypothesis and accept the alternate statement that the variance of two populations is not equal.
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6


```{r}
t.test(Developed_Y$Life.expectancy,Developing_X$Life.expectancy, alternative = "greater", var.equal = FALSE)
```


> The final conclusion is that the null hypothesis is rejected against the alternative hypothesis as the p-value<0.05. Hence we conclude that life expectancy in developed countries is more than that of developing countries with 95% confidence.

<<<<<<< HEAD


#### **Q2. Is there a statistically significant relationship between the average number of schooling years and life expectancy?**

Education creates awareness about healthy living. For example Vaccine hesitancy during this Covid-19 period, especially among the rural population, has highlighted the importance of education. Also educated choices regarding lifestyle and personal health is also influenced by the education level.
=======
Education creates awareness about healthy living. For example Vaccine hesitancy during this COVID-19 period, especially among the rural population, has highlighted the importance of education.
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

We will be using the ANOVA test to test the significance of education on life expectancy. Here we will categorize countries into one of the three categories: ‘Low’ (≤8), ‘Medium’ (>8 and ≤12), ‘High’ (>12) depending upon the country’s average schooling years.

Firstly, we will group the data by country and find the average life expectancy and Schooling for each country over the 16 years.

```{r}
Schooling_X <- aggregate(cbind(Life.expectancy, Schooling) ~ Country, data = LifeExp, FUN = mean)

kable(Schooling_X[1:5, ],format = "markdown")
```
```{r}
x <- Schooling_X %>% filter(Schooling < 8.0)
y <- Schooling_X %>% filter(Schooling > 8.0 & Schooling <= 12.0 )
z <- Schooling_X %>% filter(Schooling < 12.0)

y1 <- data.frame(Life.expectancy = x$Life.expectancy)
y1$Education = 'Low'
y2 <- data.frame(Life.expectancy = y$Life.expectancy)
y2$Education = 'Middle'
y3 <- data.frame(Life.expectancy = z$Life.expectancy)
y3$Education = 'High'

```

```{r}
Schooling_Y <- data.frame(rbind(y1,y2,y3))
kable(Schooling_Y[1:5, ],format = "markdown")
```


Let's apply the ANOVA test.


```{r}
Anova_Results <- aov(Life.expectancy ~ Education, data = Schooling_Y)
summary(Anova_Results)
```


> From the ANOVA summary, we can see that the p-value (Pr(>F)) is slightly above the commonly used threshold of 0.05 (significance level). Therefore, there is weak evidence to suggest that the "Education" factor has a statistically significant effect on the response variable. However, since the p-value is close to 0.05, it is on the borderline of significance.


#### **Q3. Check if countries that spend a higher proportion of their resources on human development have a higher life expectancy?**

The term "Income composition of resources" refers to a component of the Human Development Index (HDI), which is a measure used to assess the overall development and well-being of a country's population. The Income composition of resources specifically focuses on income-related indicators and their contribution to human development.

In the context of the HDI, the Income composition of resources index is a numerical value that ranges from 0 to 1. It reflects the extent to which a country's income distribution contributes to overall human development.

A higher value of the Income composition of resources index indicates a more equitable distribution of income, where a larger proportion of the population has access to resources and opportunities for development. This suggests that income is shared more evenly among individuals within the country.

Conversely, a lower value of the index indicates a more unequal income distribution, with a smaller proportion of the population having access to resources and opportunities for development. This suggests that income is concentrated in the hands of a smaller segment of the population.


```{r, fig.align = "center"}
life_expectancy_vs_incomecomp <- ggplot(LifeExp, aes(Income.composition.of.resources, Life.expectancy)) + geom_jitter(color = 2, alpha = 0.3)

life_expectancy_vs_incomecomp
```

By this scatter plot we can see that The Income composition of resources and their contribution to human development positively influences the Life Expectancy but let's see the correlation also with the Pearson correlation coefficient.

```{r}
in_comp_res <-aggregate(cbind(Life.expectancy, Income.composition.of.resources) ~ Country, data = LifeExp, FUN = mean)

<<<<<<< HEAD
kable(in_comp_res[1:5, ],format = "markdown")
=======
head(in_comp_res, )
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6
```

```{r, fig.align = "center"}
ggscatter(in_comp_res, x = "Income.composition.of.resources", y = "Life.expectancy", add = "reg.line", conf.int = TRUE, cor.coef = TRUE, cor.method = "pearson", xlab = "HDI (Income composition of resources)", ylab = "Life Expectancy")
```

```{r}
cor.test(in_comp_res$Income.composition.of.resources,in_comp_res$Life.expectancy )
```


<<<<<<< HEAD
> A correlation coefficient of 0.8376092 indicates a strong positive linear relationship between the variables being correlated, therefore the countries with higher income composition of resources for human development have better life expectancy. Also the regression line explains 84% of variance in the data.Thus countries should spend more on the human development to achieve higher life expectancy.



#### **Q4 Italian Government has claimed that they have spent an average of around 8.41% of their total expenditure on health for the year 2000–2015. Can we test their claim?**
=======
### Q4. Italian Government has claimed that they have spent an average of around 8.41% of their total expenditure on health for the year 2000–2015. Can we test their claim?
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

We will use a One-Sample t-test and not a One-sample Z-test to test the claim since we have no information about the population variance.

Firstly, we will use a filter to obtain the data for ‘Italy’ and the Total.expenditure column as it denotes % of government expenditure on health out of total government expenditure.

```{r}
Italy_X <- LifeExp %>% filter (Country == "Italy")
Italy_Y <- select(Italy_X,Total.expenditure)

t.test(Italy_Y, mu = 8.41, alternative = "two.sided")
```

We decided to also test India's claim that they have spent an average of around 5.2% of their total expenditure on health for the year 2000–2015.


```{r}
India_X <- LifeExp %>% filter (Country == "India")
India_Y <- select(India_X,Total.expenditure)

t.test(India_Y, mu = 5.2, alternative = "two.sided")
```

<<<<<<< HEAD
=======
Since 5.2 doesn’t lie in the 95% confidence interval range [4.232587, 4.689913], we can say that the sample doesn’t give enough evidence to accept the claim made by the Indian Government. On the other hand Italy's claim of 8.4 lies in the 95% CI range of [8.320883, 9.021617].
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

> Since 5.2 doesn’t lie in the 95% confidence interval range [4.232587, 4.689913], we can say that the sample doesn’t give enough evidence to accept the claim made by the Indian Government. On the other hand Italy's claim of 8.4 lies in the 95% CI range of [8.320883 ,9.021617].



#### **Q5. What are our observations when comparing the proportions of the number of infant deaths and the number of under-five deaths.**

We will conduct a two-proportions z-test to compare the two independent proportions.

```{r}
Mort_X <- aggregate(cbind(Life.expectancy, infant.deaths, under.five.deaths) ~ Country, data = LifeExp, FUN = mean)

kable(Mort_X[1:5, ],format = "markdown")

```

<<<<<<< HEAD
infant_deaths column represents the number of infant deaths per 1000 population and similarly, under_five_deaths represents the number of under-five deaths per 1000 population. We have to use the average value of infant or under-five deaths of all the countries and take its ceiling value.
=======
infant.deaths column represents the number of infant deaths per 1000 population and similarly, under.five.deaths represents the number of under-five deaths per 1000 population. We have to use the average value of infant or under five deaths of all the countries and take its ceiling value.
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

```{r}
mortx <- ceiling(mean(Mort_X$infant.deaths))
morty <- ceiling(mean(Mort_X$under.five.deaths))
argx <- c(mortx,morty)
argy <- c(1000,1000)

prop.test(argx,argy, correct = FALSE)
```


> Since the p- value is greater than 0.05, we see no significant difference in the two independent proportions.The data does not provide sufficient evidence to conclude that there is a meaningful or significant difference between the rates of infant deaths and under-five deaths.



#### **Q6. What is the correlation of Life expectancy with Alcohol drinking habits ?**

Let's use the Person correlation test.

```{r}
Alc_X <- aggregate(cbind(Life.expectancy, Alcohol) ~ Country, data = LifeExp, FUN = mean) 

kable(Alc_X[1:5, ],format = "markdown")
```

```{r, fig.align = "center"}
ggscatter(Alc_X, x = "Alcohol", y = "Life.expectancy", add = "reg.line", conf.int = TRUE, cor.coef = TRUE, cor.method = "pearson", xlab = "Alcohol consumption (in litres of pure alcohol", ylab = "Life Expectancy")
```

```{r}
cor.test(Alc_X$Alcohol, Alc_X$Life.expectancy)
```

<<<<<<< HEAD
<b><i>The results indicate a statistically significant moderate positive linear relationship (correlation coefficient = 0.4323943) between alcohol consumption and life expectancy. This suggests that higher levels of alcohol consumption are associated with increased life expectancy, within the range of data analyzed.
However,before jumping to conclusions we need to also take in consideration other factors.</b></i>
=======
The results indicate a statistically significant moderate positive linear relationship (correlation coefficient = 0.43) between alcohol consumption and life expectancy. This suggests that higher levels of alcohol consumption are associated with increased life expectancy, within the range of data analyzed. However,before jumping to conclusions we need to also take in consideration other factors.
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6

Developed countries tend to have higher levels of alcohol consumption compared to developing countries. This can be attributed to various factors such as higher income levels, greater access to alcohol, more established alcohol industries, and different cultural norms surrounding alcohol. This was also our conclusion from our previous analysis where we concluded that life expectancy in developed countries is more than that of developing countries.

Higher GDP at Developed countries can influence alcohol consumption patterns to some extent. As countries experience economic growth and an increase in GDP, there is often an associated rise in income levels and discretionary spending power. This can lead to increased alcohol consumption. 

<<<<<<< HEAD
We can visualize these correlations on the following graphs.


```{r, fig.align = "center"}
=======
```{r}
>>>>>>> f71a0ab61bcc2e5739a3875e13917ef5c7fd22a6
ggplot(LifeExp, aes(x=Alcohol, fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "Alcohol consumption by Status", x ="Alcohol", y="Density") +
  scale_fill_manual(values = c(2, 5))
```

```{r, fig.align = "center"}
ggplot(LifeExp, aes(x=log(GDP), fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "GDP by Status", x ="GDP", y="Density") +
  scale_fill_manual(values = c(2, 5))
```

```{r, fig.align = "center"}
life_expectancy_vs_GDP  <- ggplot(LifeExp, aes(GDP, Life.expectancy)) + geom_jitter(color = 2, alpha = 0.3)
life_expectancy_vs_GDP 
```


#### **Q7. Correlation between Life Expectancy and Immunization.**

In Covid-19 times we all have seen the importance of immunization against the virus to increase life expectancy. Can we show that immunization against Polio and Diphtheria has a significant effect on life expectancy?

We will use a two-way ANOVA test. Here we will divide the countries into two categories for both Polio and Diphtheria. Countries having values of % immunization coverage for one-year-old greater than the median value will get category ‘High’ else ‘Low’.

```{r}
Immun_X <- aggregate(cbind(Life.expectancy, Polio, Diphtheria) ~ Country, data = LifeExp, FUN = mean)

kable(Immun_X[1:5, ],format = "markdown")
```

```{r}
xx_1 <- Immun_X %>% filter(Polio <= 85)
xx_2 <- Immun_X %>% filter(Polio > 85)
yy_1 <- Immun_X %>% filter(Diphtheria <= 85)
yy_2 <- Immun_X %>% filter(Diphtheria > 85)

zz1 <- data.frame(Life.expectancy = xx_1$Life.expectancy, Country = xx_1$Country)
zz1$Polio = 'Low'
zz2 <- data.frame(Life.expectancy = xx_2$Life.expectancy, Country = xx_2$Country)
zz2$Polio = 'High'
zz3 <- data.frame(Life.expectancy = yy_1$Life.expectancy, Country = yy_1$Country)
zz3$Diphtheria = 'Low'
zz4 <- data.frame(Life.expectancy = yy_2$Life.expectancy, Country = yy_2$Country)
zz4$Diphtheria = 'High'
```

```{r}
Immun_Polio <- data.frame(rbind(zz1,zz2))
kable(Immun_Polio[1:5, ],format = "markdown")
```

```{r}
Immun_Diphtheria <- data.frame(rbind(zz3,zz4))
kable(Immun_Diphtheria[1:5, ],format = "markdown")
```

```{r}
Immun_Y <- merge(Immun_Polio, Immun_Diphtheria, by = "Country")
kable(Immun_Y[1:5, ],format = "markdown")
```

```{r}
Anova_Results_1 <- aov(Life.expectancy.x ~ Polio + Diphtheria, data = Immun_Y)
summary(Anova_Results_1)
```


<b><i>The p-value for both Polio and Diphtheria immunization coverage for one-year old is less than 0.05, hence we can say that immunization has a significant impact on the life expectancy.</b></i>




```{r, fig.align = "center"}
life_expectancy_vs_Diphtheria  <- ggplot(LifeExp, aes(Diphtheria, Life.expectancy)) + geom_jitter(color = 5, alpha = 0.3)
                              
life_expectancy_vs_Polio  <- ggplot(LifeExp, aes(Polio, Life.expectancy)) + geom_jitter(color = 2, alpha = 0.3)

p <- plot_grid(life_expectancy_vs_Diphtheria, life_expectancy_vs_Polio) 
title <- ggdraw() + draw_label("Correlation between Immunizations and life expectancy", fontface='bold')
plot_grid(title,p, ncol=1)
```


### **Constructing Correlation Matrix**

```{r}
# Compute the correlation matrix
numeric_vars <- LifeExp[, sapply(LifeExp, is.numeric)]
cor_matrix <- cor(numeric_vars)

# Plot the correlation matrix using corrplot
color_palette = brewer.pal(11, "PuOr")
corrplot::corrplot(cor_matrix, method = "shade", type = "upper", tl.cex = 0.7, col = color_palette, tl.col = "black" )
#corrplot::corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)
  
# Adjust the text size

```


#### **Network plot**

```{r}
cor_matrix %>% corrr::network_plot(min_cor = .3)
```


## **Linear Regression**

After completing the data cleaning process and exploring and engineering features, we can now move on to the main phase of this project: making predictions using linear regression. Initially, we will examine the effectiveness of a **simple linear regressor** (using only one feature). Then, we will progress to employing all the features through **multiple linear regression** or selecting a subset of features using appropriate techniques for **feature selection**.

Based on the exploratory data analysis (EDA), it is evident that our target variable, "Life expectancy," demonstrates the highest correlation with "Adult mortality." However, upon examining the scatter plot, it appears that this variable may not be suitable for linear regression. Therefore, we proceed by choosing the second most correlated variable, namely "Income.composition.of.resources". By observing the scatter plot for this particular variable, we can notice a linear trend.

Before we start, the question of whether to perform feature scaling arises. Feature scaling is not an absolute necessity for linear regression. The algorithm calculates the coefficients for each feature by computing the differences between the feature values and their mean. Therefore, feature scaling does not have a direct impact on the coefficients obtained from the linear regression model.

However, considering the benefits of feature scaling, such as ensuring consistent and meaningful comparisons, and the fact that it is a mandatory step when using regularization techniques, we have chosen to scale the data at the outset.

Afterward, to ensure unbiased evaluations, we proceed by randomly partitioning our dataset into separate training and test sets. For models that require cross-validation, we will utilize a portion of the training set as a validation set.

```{r}
train_percent <- 0.8
test_percent <- 1 - train_percent
num_rows <- nrow(LifeExp)
train_size <- floor((train_percent) * num_rows)

set.seed(1)

train_indices <- sample.int(n = num_rows, size = train_size)

train <- LifeExp[train_indices, !(names(LifeExp) %in% c("Country", "Year"))]
test <- LifeExp[-train_indices, !(names(LifeExp) %in% c("Country", "Year"))]

train <- train %>% mutate(across(where(is.numeric), scale))
test <- test %>% mutate(across(where(is.numeric), scale))

cat("Train size: ", train_size, "\nTest size: ", num_rows - train_size)

Y_colname <- "Life.expectancy"
X_colnames <- colnames(train)[colnames(train) != Y_colname]
```

```{r}
df_metrics_regression <- data.frame(Model_name = character(),
                                    Adj_R2 = numeric(),
                                    AIC = numeric(),
                                    BIC = numeric(),
                                    n_coef = numeric(),
                                    MSE = numeric(),
                                    RMSE = numeric(),
                                    n_RMSE_sd = numeric(),
                                    n_RMSE_range = numeric())
colnames(df_metrics_regression) <- c("Model_name", "Adj_R2", "AIC", "BIC", "n_coef", "MSE", "RMSE", "n_RMSE_sd", "n_RMSE_range")
```



### **Simple linear regression**

The linear regression equation is given by: $$ Y = \beta_0 + \beta_1 \cdot x + \varepsilon $$,\
where $$\beta_0$$ represents the intercept, $$\beta_1$$ represents the coefficient for the chosen feature "Income.composition.of.resources" and $$\varepsilon$$ represents a random error.

```{r}
simple_lm <- lm(Life.expectancy ~ Income.composition.of.resources, data = train)
summary(simple_lm)
```

```{r, echo=FALSE}
mse <- mean((test$Life.expectancy - predict(simple_lm, newdata = test))^2)

new_row <- data.frame(Model_name = 'Simple linear regression',
                      Adj_R2 = summary(simple_lm)$adj.r.squared,
                      AIC = AIC(simple_lm),
                      BIC = BIC(simple_lm),
                      n_coef = length(coefficients(simple_lm)),
                      MSE = mse,
                      RMSE = sqrt(mse),
                      n_RMSE_sd = sqrt(mse) / sd(test$Life.expectancy),
                      n_RMSE_range = sqrt(mse)/diff(range(test$Life.expectancy)))
df_metrics_regression <- rbind(df_metrics_regression, new_row)
```

The analysis reveals that the variable "Income.composition.of.resources" has a moderate-to-high level of explanatory power, as indicated by an R-squared value of 0.6248. This means that approximately 62.48% of the variability in life expectancy can be attributed to this variable.

Both the intercept and the coefficient associated with "Income.composition.of.resources" are highly significant, as indicated by the p-values being less than 0.05. The F-statistic also supports this finding, with a value of 3910 and an extremely low p-value, suggesting that the observed relationship is not due to chance.

The formula for the regression line can be expressed as follows:\
Life.Expectancy = 41.2191 + 43.7221 * Income.composition.of.resources\
and it is visualized bellow.

```{r}
ggplot(train, aes(x = Income.composition.of.resources, y = Life.expectancy)) +
  geom_point(color = "#80b1d3", size = 2, alpha = 0.9) +
  labs(x = "Income composition of resources", y = "Life expectancy") +
  theme_minimal() +
  theme(axis.text = element_text(size = 8),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12)) +
  geom_abline(slope = coef(simple_lm)["Income.composition.of.resources"],
              intercept = coef(simple_lm)["(Intercept)"],
              color = "red", linetype = "solid", linewidth = 1.2)
```

That’s not the whole picture though. Residuals could show how poorly a model represents data. Residuals are leftover of the outcome variable after fitting a model (predictors) to data, and they could reveal patterns in the data unexplained by the fitted model. Using this information, not only we can check if linear regression assumptions are met, but we can improve our model in an exploratory way.

Let’s now have a look at the **diagnostic plots**, in order to check whether Linear regression assumptions are met. These assumptions include:

1. **Linearity**: There should be a linear relationship between the predictors (x) and the outcome (y). This implies that the relationship between the variables can be adequately captured by a straight line or a linear combination of predictors.

2.  **Homoscedasticity**: The residual errors should have a constant variance across the range of predicted values. This assumption implies that the spread of residuals is consistent, regardless of the predicted values.

3. **Independence of error terms**: The residual errors should be independent of each other and independent of the predictor variables (x). Independence of residuals assumes that the errors are not influenced by any patterns or structures in the data that are not captured by the predictor variables.

4. **Error term with mean zero**: The residual errors (the differences between the observed outcome and the predicted outcome) should have a mean value of zero. This assumption assumes that, on average, the model is unbiased in its predictions.

It is important to assess these assumptions before relying on the results of a linear regression model to ensure the validity of the model and the reliability of the inferences drawn from it.

```{r}
par(mfrow = c(2, 2))
plot(simple_lm)
```

**Residuals vs Fitted**

This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and the outcome variable, and the pattern could show up in this plot if the model doesn’t capture the non-linear relationship. In our case, we find almost equally spread residuals around a horizontal line without distinct patterns. So, this is a good indication that we don’t have non-linear relationships.

**Normal Q-Q**

This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? Thus for small sample sizes, it can't be assumed that the estimator $$\hat{\beta}$$ isn't Gaussian either, meaning the standard confidence intervals and significance tests are invalid. Our plot shows many points lying on the dotted line, however the rightmost and leftmost points are deviating from the line, suggesting slight right and left skewness.

**Scale-Location**

It's is also called a Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how we can check the assumption of equal variance (**homoscedasticity**). It’s good if we see a horizontal line with equally (randomly) spread points. In our case the red line deviates a little from horizontal, showing small presence of heteroskedasticity.

**Residuals vs Leverage**

Leverage refers to the extent to which the coefficients in the regression model would change if a particular observation was removed from the dataset. Here, we watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. When cases are outside of the dashed lines (meaning they have high “Cook’s distance” scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.\
We can observe that there are not any influential points in our regression model.

By the previous analysis, we can conclude that our data is behaving linearly as we expected, but it does show a degree of heteroscedasticity.

```{r}
pred <- predict(simple_lm, newdata = test)
rmse <- sqrt(mean((test$Life.expectancy - pred)^2))
normalized_rmse_std <- sqrt(mean((pred - test$Life.expectancy)^2)) / sd(test$Life.expectancy)
normalized_rmse_range <- sqrt(mean((pred - test$Life.expectancy)^2)) / diff(range(test$Life.expectancy))
cat("RMSE =", rmse)
cat("\nNormalized RMSE (sd)    =", normalized_rmse_std)
cat("\nNormalized RMSE (range) =", normalized_rmse_range)
```
The normalized RMSE (sd) is 0.596, indicating that the model's predictions have an error of around 59.6 % of the standard deviation of the target variable. The normalized RMSE (range) is 0.128, suggesting that the model's predictions have an error of approximately 12.8% of the range of the target variable.

### Multiple linear regression

In this section we will train a regression model using multiple features. As part of the process we will select the most important features for the model using backward and forward selection and also check how well it performs on a test set.

```{r}
full_lm <- lm(train[,Y_colname] ~ ., data = train[, X_colnames])
summary(full_lm)
```

```{r, echo=FALSE}
mse <- mean((test$Life.expectancy - predict(full_lm, newdata = test))^2)

new_row <- data.frame(Model_name = 'Multiple linear regression',
                      Adj_R2 = summary(full_lm)$adj.r.squared,
                      AIC = AIC(full_lm),
                      BIC = BIC(full_lm),
                      n_coef = length(coefficients(full_lm)),
                      MSE = mse,
                      RMSE = sqrt(mse),
                      n_RMSE_sd = sqrt(mse) / sd(test$Life.expectancy),
                      n_RMSE_range = sqrt(mse)/diff(range(test$Life.expectancy)))
df_metrics_regression <- rbind(df_metrics_regression, new_row)
```

The summary indicates that the residuals are symmetrically distributed, with a median almost equal to 0 (0.0793). We will examine the residuals more closely later. The full Linear regression model explains 84.99% of the variance associated with the response variable. The F-statistic is 733.3 (>> 1), and its p-value is nearly 0, providing clear evidence against the null hypothesis that all coefficients are equal to zero. This means that at least one variable is associated with the response. The p-values of the predictor variables allow us to determine their significance. Variables with lower p-values are more significant in relation to the response. Notably, features such as infant.deaths, Alcohol, under.five.deaths, and GDP are not statistically significant in our model.

To check the multicollinearity in our data we will look at the **Variance Inflation Factors (VIF)**:

```{r}
vif_values <- vif(full_lm)
sorted_vif <- sort(vif_values, decreasing = TRUE)
vif_table <- data.frame(VIF = sorted_vif)
vif_table
```

It is generally desirable to have VIF values as small as possible, closer to 1, which indicates low levels of collinearity.  As a rule of thumb VIF = 5 is taken as a threshold, and *any independent variable with VIF > 5 will have to be removed*, due to problematic amount of collinearity.

The VIF values for under.five.deaths, infant.deaths, thinness.10.19.years, thinness.5.9.years and Income.composition.of.resources have high values (> 5) that indicate collinearity problem. Therefore, we've decided to remove them from the model.

```{r}
cols_to_remove <- c("under.five.deaths", "infant.deaths", "thinness.10.19.years", "thinness.5.9.years", "Income.composition.of.resources")
X_colnames_reduced <- X_colnames[!(X_colnames %in% cols_to_remove)]

full_lm_low_vif <- lm(train[,Y_colname] ~ ., data = train[, X_colnames_reduced])
summary(full_lm_low_vif)
```
```{r, echo=FALSE}
mse <- mean((test$Life.expectancy - predict(full_lm_low_vif, newdata = test))^2)

new_row <- data.frame(Model_name = 'MLR - No multicollinearity',
                      Adj_R2 = summary(full_lm_low_vif)$adj.r.squared,
                      AIC = AIC(full_lm_low_vif),
                      BIC = BIC(full_lm_low_vif),
                      n_coef = length(coefficients(full_lm_low_vif)),
                      MSE = mse,
                      RMSE = sqrt(mse),
                      n_RMSE_sd = sqrt(mse) / sd(test$Life.expectancy),
                      n_RMSE_range = sqrt(mse)/diff(range(test$Life.expectancy)))
df_metrics_regression <- rbind(df_metrics_regression, new_row)
```

As we can see after removing columns with high VIF, the R-squared value decreased. However, it is important to note that the primary goal of removing variables with high VIF is to address the issue of collinearity and improve the model's reliability and interpretability. We expect the adjusted model to be more appropriate for analysis and inference.

Let’s now have a look at the diagnostic plots.

```{r}
par(mfrow=c(2,2))
plot(full_lm_low_vif)
```

**Fitted vs Residual graph**\
The red line is very close to zero and the spread of the residuals is approximately the same across the x axis, so we have no discernible non-linear trends or indications of non-constant variance.

**Normal Q-Q Plot**\
After removing columns with high VIF, it is evident that the heteroscedasticity problem has improved. The deviation is not as serious anymore. However, the residuals still deviate from the diagonal line in both the upper and lower tails. This plot suggests that the tails have smaller values than what we would anticipate under the standard modeling assumptions, indicating a "lighter" distribution.

**Scale-Location**\
This plot has also improved. The red line deviates only slightly from the horizontal, indicating a very small presence of heteroscedasticity.

**Residuals vs Leverage**\
In this plot we see no evidence of outliers. The “Cook’s distance” dashed curves don’t even appear on the plot. None of the points come close to having both high residual and leverage.


### Linear model (forward / backward model selection)

Next, we can explore approaches to reduce the variance of the model, namely backward and forward model selection. Empirically, backward selection tends to perform better in this regard. However, we also retain the forward selection method for comparison purposes.

```{r}
nvmax <- 26 # 2 x number of variables

forward_sel_lm = regsubsets(train[,Y_colname] ~., data = train[, X_colnames_reduced], method = 'forward', nvmax = nvmax)
backward_sel_lm = regsubsets(train[,Y_colname] ~., data = train[, X_colnames_reduced], method = 'backward', nvmax = nvmax)
```

```{r}
plot_max_point <- function(values){
  max_idx = which.max(values)
  points(max_idx, values[max_idx], col = 'red', cex = 2, pch = 20)
}

plot_min_point <- function(values){
  min_idx = which.min(values)
  points(min_idx, values[min_idx], col = 'red', cex = 2, pch = 20)
}

plot_subsets_summary <- function(featureSelection_lm){
  
  featureSelection_lm_summary <- summary(featureSelection_lm)

  xlabel = 'Number of iterations'
  
  par(mfrow = c(3, 1))
  
  adjr2_list <- featureSelection_lm_summary$adjr2
  max_adjr2_idx <- which.max(adjr2_list)
  plot(adjr2_list, xlab = xlabel, ylab = 'Adjusted R2', type = 'l')
  plot_max_point(values = adjr2_list) # The model with highest value is the best model.
  text(x = max_adjr2_idx, y = adjr2_list[max_adjr2_idx], labels = max_adjr2_idx, pos = 1)
  
  cp_list <- featureSelection_lm_summary$cp
  min_cp_idx <- which.min(cp_list)
  plot(cp_list, xlab = xlabel, ylab = 'Cp', type = 'l')
  plot_min_point(values = cp_list) # The model with least value is the best model.
  text(x = min_cp_idx, y = cp_list[min_cp_idx], labels = min_cp_idx, pos = 3)
  
  bic_list <- featureSelection_lm_summary$bic
  min_bic_idx <- which.min(bic_list)
  plot(bic_list, xlab = xlabel, ylab = 'BIC', type = 'l')
  plot_min_point(values = bic_list) # The model with least value is the best model.
  text(x = min_bic_idx, y = bic_list[min_bic_idx], labels = min_bic_idx, pos = 3)
}
```

Both feature selection techniques yielded the same results. So, for simplicity, we will show the results only for the backward selection method.

```{r}
#plot_subsets_summary(forward_sel_lm)
plot_subsets_summary(backward_sel_lm)
```

Based on the observed graphs, it is clear that different statistical measures, such as Cp, BIC, and Adjusted R-squared, produce varying results in the feature selection process. The use of Adjusted R-squared led to a model with one less variable compared to the original model. Cp resulted in the removal of two variables, while BIC suggested eliminating four variables from the model. Notably, Cp and AIC are equivalent and select the same model, hence we have focused on presenting the results based on Cp.

```{r}
# Get the chosen variables according to each statistical measure
backward_sel_lm_summary <- summary(backward_sel_lm)
vars_adjr2 <- names(coefficients(backward_sel_lm, which.max(backward_sel_lm_summary$adjr2)))
vars_cp <- names(coefficients(backward_sel_lm, which.min(backward_sel_lm_summary$cp)))
vars_bic <- names(coefficients(backward_sel_lm, which.min(backward_sel_lm_summary$bic)))

# Get the variables in the full model
vars_full_model <- names(coef(full_lm_low_vif))

# Create a data frame to store the included variables
selected_vars_df <- data.frame(Feature = vars_full_model, AdjR2 = 0, Cp = 0, BIC = 0)

# Update the data frame with the variables included in each model
selected_vars_df$AdjR2 <- as.numeric(selected_vars_df$Feature %in% vars_adjr2)
selected_vars_df$Cp <- as.numeric(selected_vars_df$Feature %in% vars_cp)
selected_vars_df$BIC <- as.numeric(selected_vars_df$Feature %in% vars_bic)

# Remove the intercept from the data frame
selected_vars_df <- selected_vars_df[-1, ]

kable(selected_vars_df, row.names = FALSE)
```

The feature selection algorithm consistently suggests that the variable "Total.expenditure" should be eliminated from the final model. This recommendation is supported by all three statistical measures. Furthermore, both Cp and BIC also favor removing the "Population" variable. From a practical standpoint, there are valid reasons for excluding these variables:

1. Population: Including the population of a country in the model is unlikely to have a direct impact on life expectancy. Therefore, it can be deemed unnecessary for predicting life expectancy accurately.

2. Total.expenditure: The information conveyed by the "Total.expenditure" variable is largely captured by the "percentage.expenditure" variable. Hence, including both variables in the model would introduce redundancy without significantly enhancing our understanding.

By excluding these variables, the final model becomes more concise and interpretable without compromising the explained variance to a great extent. However, it is essential to verify the impact on explained variance before drawing a definitive conclusion.

To represent the most significant variables, here we report the variable elimination plots:

```{r}
plot(backward_sel_lm, scale = 'r2')
plot(backward_sel_lm, scale = 'bic')
plot(backward_sel_lm, scale = 'Cp')
```

Based on the regsubsets plot, we reach the same conclusion as before. Therefore, we will proceed with removing the variables "Population" and "Total.expenditure" from our model. After removing these variables, we will retrain the multiple linear regressor using the updated set of features.

```{r}
cols_to_remove <- c("Population", "Total.expenditure")
X_colnames_reduced_featureSel <- X_colnames_reduced[!(X_colnames_reduced %in% cols_to_remove)]

featureSel_lm <- lm(train[,Y_colname] ~ ., data = train[, X_colnames_reduced_featureSel])
summary(featureSel_lm)
```

```{r, echo=FALSE}
mse <- mean((test$Life.expectancy - predict(featureSel_lm, newdata = test))^2)

new_row <- data.frame(Model_name = 'MLR - Feature selection',
                      Adj_R2 = summary(featureSel_lm)$adj.r.squared,
                      AIC = AIC(featureSel_lm),
                      BIC = BIC(featureSel_lm),
                      n_coef = length(coefficients(featureSel_lm)),
                      MSE = mse,
                      RMSE = sqrt(mse),
                      n_RMSE_sd = sqrt(mse) / sd(test$Life.expectancy),
                      n_RMSE_range = sqrt(mse)/diff(range(test$Life.expectancy)))
df_metrics_regression <- rbind(df_metrics_regression, new_row)
```

The variance explained by the model is 83.33%, which is nearly identical to the previous result (83.34%) achieved with the full model that included all variables except those displaying multicollinearity issues.

```{r}
hist(featureSel_lm$residuals)
```

The histogram of the residuals exhibits a pattern that closely resembles a normal distribution. However, let's also check the QQ plot.

```{r}
plot(featureSel_lm, which = 2)
```

We can observe that the distribution has "fat" tails - both the ends of the Q-Q plot deviate from the straight line and its center follows a straight line. We refer to this as positive kurtosis (a measure of “tailedness”).

**Addressing heteroskedasticity**

```{r}
shapiro.test(residuals(featureSel_lm))
```
In the context of the Shapiro-Wilk test, the null hypothesis is that the data is normally distributed. Since the p-value is significantly small (<< 0.05), it suggests strong evidence to reject the null hypothesis. Therefore, based on these results, it can be inferred that *the residuals do not follow a normal distribution*.

```{r}
bptest(featureSel_lm)
```

```{r}
ncvTest(featureSel_lm)
```

Both the Breusch-Pagan test and the Non-constant Variance Score Test indicate strong evidence against the null hypothesis of constant variance of the residuals. Both tests provide strong evidence supporting the presence of heteroskedasticity in the regression model.

**Data Transformation**

To address the issue of constant variance of the residuals, let's attempt to mitigate it through data transformation. Transforming the data is the go-to approach to remove heteroskedasticity. The goal is to stabilize the variance and to bring the distribution closer to the Normal distribution. The log is an effective transformation to do this. Taking the square root or cubic root are two possible alternatives.

Previously, we made attempts to address outliers through data transformation, but unfortunately, it did not yield any improvement. Now, let's focus on utilizing a log transformation to address the non-normality of residuals. Since there are 0s present in the data, we will use the formula log(1+x) to handle these values appropriately. We will only consider the variables that were selected earlier using multicollinearity check and feature selection. Initially, we exclude the binary variable "Status" when computing the log transformation. However, we will add it back later to enhance the linear model.

Additionally, we attempted using the square root transformation but did not observe any significant improvement in the model's performance.

```{r}
X_colnames_reduced_featureSel_noStatus <- X_colnames_reduced_featureSel[!X_colnames_reduced_featureSel %in% "Status"]

train_log <- train[, c(X_colnames_reduced_featureSel_noStatus, "Life.expectancy")]
train_log <- log1p(train_log) # sqrt
train_log$Status <- train$Status

log_featureSel_lm <- lm(Life.expectancy ~ ., data = train_log)
summary(log_featureSel_lm)
```

```{r}
hist(log_featureSel_lm$residuals)
```

```{r}
shapiro.test(residuals(log_featureSel_lm))
```
From the analysis of the histogram of the residuals and the results of the Shapiro-Wilk test, it can be concluded that the data transformation did not effectively address the issue of non-normality in the residuals. The residuals continue to deviate from the assumption of normality.

*Based on our analysis, we can conclude that a slight issue of heteroscedasticity still persists. However, the departure from normality, as observed from the histograms, is not severe. Moreover, considering the sufficiently large sample size, we can still rely on the reliability of our analysis.*

The linear model appears to be suitable for predicting Life.expectancy based on the Adj. R-Squared value. It successfully *passed 2 out of 4 Assumption Checks, namely the Multicollinearity and Linearity Test*, although it did not meet the criteria for Normality and Homoscedasticity.

The Linear Model effectively captures the linear relationship between Life.expectancy and the chosen independent variables. However, it is important to acknowledge the model's sensitivity to outliers, which were prevalent in the initial dataset.

Here is a summary of all of the models tested so far:

```{r}
df_metrics_regression[order(-df_metrics_regression$Adj_R2), ]
```

As we can see from the table above, we have tested a total of four models (excluding the log transformation model). Below, we can also observe the bar graphs for each model, representing different evaluation metrics. The models exhibit varying levels of explanatory power.

Among the tested models, the multiple linear regression model has the highest adjusted R-squared, indicating a better fit to the data compared to the other models. However, it is important to note that this model also includes features with a high level of multicollinearity, which can compromise its reliability.

Considering this multicollinearity issue, we can turn our attention to the model obtained using backward feature selection. This model exhibits a slightly lower adjusted R-squared value but offers a favorable trade-off between goodness of fit and complexity. Moreover, the model displays low values of AIC and BIC, suggesting that it strikes a good balance between performance and simplicity.


```{r}
# Bar graph for Adj_R2
ggplot(df_metrics_regression, aes(x = Model_name, y = Adj_R2, fill = Model_name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(Adj_R2, 2)), vjust = 2.5) +
  labs(x = "Model", y = "Adj_R2") +
  ggtitle("Adjusted R-Squared Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

# Bar graph for AIC
ggplot(df_metrics_regression, aes(x = Model_name, y = AIC, fill = Model_name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(AIC, 2)), vjust = 2.5) +
  labs(x = "Model", y = "AIC") +
  ggtitle("AIC Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

# Bar graph for BIC
ggplot(df_metrics_regression, aes(x = Model_name, y = BIC, fill = Model_name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(BIC, 2)), vjust = 2.5) +
  labs(x = "Model", y = "BIC") +
  ggtitle("BIC Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

# Bar graph for n_coef
ggplot(df_metrics_regression, aes(x = Model_name, y = n_coef, fill = Model_name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = n_coef), vjust = 2.5) +
  labs(x = "Model", y = "Number of Coefficients") +
  ggtitle("Number of Coefficients Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

# Bar graph for MSE
ggplot(df_metrics_regression, aes(x = Model_name, y = MSE, fill = Model_name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(MSE, 2)), vjust = 2.5) +
  labs(x = "Model", y = "MSE") +
  ggtitle("MSE Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_blank())

```


```{r, echo=FALSE}
# Sort the dataframe based on F1_score in descending order
df_metrics_regression <- df_metrics_regression[order(df_metrics_regression$Adj_R2, decreasing = TRUE), ]

df_metrics_regression <- df_metrics_regression[df_metrics_regression$Model_name == "MLR - Feature selection", ]

# Keep just Model name and MSE columns
df_metrics_regression <- df_metrics_regression[, !(names(df_metrics_regression) %in% c("Adj_R2", "AIC", "BIC", "n_coef"))]
```


### Ridge regression

For the Ridge regression, it is necessary to standardize data. However, we already did this step.

```{r, echo=FALSE}
prepare_ridge_lasso <- function(X, Y, alpha){
  grid <- 10^seq(10, -2, length=100)

  model <- glmnet(X, Y, alpha = alpha, standardize = F)
  # alpha = 0 for Ridge Regression
  # alpha = 1 for Lasso Regression
  plot(model, label=TRUE)

  set.seed(1)
  cv.out <- cv.glmnet(X, Y, alpha = alpha, nfold=10, type.measure = "mse", lambda = grid)
  
  plot(cv.out)
  
  i.bestlam <- which.min(cv.out$cvm)
  bestlam <- cv.out$lambda[i.bestlam]
  cat('Best lambda:', bestlam, '\n')
  
  return(list('model' = model, 'lambda' = bestlam))
}
```

```{r, warning=FALSE}
train$Status  <- as.numeric(train$Status) - 1
X = as.matrix(train[, X_colnames_reduced_featureSel])
Y = as.matrix(train[, Y_colname])

ridge_res = prepare_ridge_lasso(X = X,
                                Y = Y,
                                alpha = 0  # Ridge
                                )
```

```{r}
ridge_model <- ridge_res$model
ridge_lambda <- ridge_res$lambda
test$Status  <- as.numeric(test$Status) - 1
ridge_pred <- predict(ridge_model,
                      s = ridge_lambda,
                      newx = as.matrix(test[, X_colnames_reduced_featureSel]))
ridge_mse <- mean((ridge_pred - test$Life.expectancy)^2)
ridge_mse
```

The value of best lambda is equal to 0.09326033
MSE with the best lambda: 28.65416

```{r, echo=FALSE}
new_row <- data.frame(Model_name = 'Ridge regression',
                      MSE = ridge_mse,
                      RMSE = sqrt(ridge_mse),
                      n_RMSE_sd = sqrt(ridge_mse) / sd(test$Life.expectancy),
                      n_RMSE_range = sqrt(ridge_mse)/diff(range(test$Life.expectancy))
                      )
df_metrics_regression <- rbind(df_metrics_regression, new_row)
```

```{r, echo=FALSE}
predictors <- data.matrix(train[, !(names(train) %in% "Life.expectancy")])

best_model <- glmnet(predictors, train$Life.expectancy, alpha = 1, lambda = ridge_lambda)
coef(best_model)
```



### Lasso regression

```{r, warning=FALSE}
lasso_res = prepare_ridge_lasso(X = X,
                                Y = Y,
                                alpha = 1  # Lasso
                                )
```

```{r}
lasso_model <- lasso_res$model
lasso_lambda <- lasso_res$lambda
lasso_pred <- predict(lasso_model,
                      s = lasso_lambda,
                      newx = as.matrix(test[, X_colnames_reduced_featureSel])
                      )
lasso_mse <- mean((lasso_pred - test$Life.expectancy)^2)
lasso_mse
```
The value of best lambda is equal to: 0.01
MSE with the best lambda: 22.82819

We can see that the value of MSE of the lasso model is better than the ridge regularization model. So, we can conclude that the model with the L1 regularization term has slightly better predicting capabilities.

```{r, echo=FALSE}
predictors <- data.matrix(train[, !(names(train) %in% "Life.expectancy")])

best_model <- glmnet(predictors, train$Life.expectancy, alpha = 1, lambda = lasso_lambda)
coef(best_model)
```


```{r, echo=FALSE}
new_row <- data.frame(Model_name = 'Lasso regression',
                      MSE = lasso_mse,
                      RMSE = sqrt(lasso_mse),
                      n_RMSE_sd = sqrt(lasso_mse) / sd(test$Life.expectancy),
                      n_RMSE_range = sqrt(lasso_mse)/diff(range(test$Life.expectancy)))
df_metrics_regression <- rbind(df_metrics_regression, new_row)
```

```{r}
df_metrics_regression[order(df_metrics_regression$MSE), ]
```
```{r}
ggplot(df_metrics_regression, aes(x = reorder(Model_name, MSE), y = MSE, fill = Model_name)) +
  geom_bar(stat = "identity") +
  geom_text(aes(label = round(MSE, 2)), vjust = 2.5) +
  labs(x = "Model", y = "MSE") +
  ggtitle("MSE Comparison") +
  theme_minimal() +
  theme(axis.text.x = element_blank())
```

Here, once again, we can observe that the model obtained using backward feature selection performs the best. It exhibits the lowest Mean Squared Error (MSE) on the test set when compared to the Lasso and Ridge regression models. Let's rerun its summary and interpret the results.

```{r}
summary(featureSel_lm)
```

Finally, let's interpret the results achieved by the best model.

The model's performance is indicated by the multiple R-squared value of 0.8333, which means that approximately 83.33% of the variance in the response variable can be explained by the predictors in the model. The adjusted R-squared value, which accounts for the number of predictors in the model, is 0.8325.

The F-statistic for the model is 1062, with a p-value of < 2.2e-16, indicating that the model as a whole is statistically significant in explaining the variation in the response variable.

The residual standard error of the model is 3.886, which represents the average difference between the observed and predicted values of the response variable.

Also, the model provides insights into the relationships between the predictors and the response variable, allowing us to make interpretations and draw conclusions about the factors influencing the response variable. Here is a summary of the relationships between the predictors and the response variable:

1. StatusDeveloping: Being in a developing status is associated with a decrease in Life Expectancy on average.

2. Adult Mortality: An increase in adult mortality is associated with a decrease in Life Expectancy on average.

3. Alcohol: Higher alcohol consumption is associated with an increase in Life Expectancy on average.

4. Percentage Expenditure: An increase in percentage expenditure is associated with an increase in Life Expectancy on average.

5. Hepatitis B: Higher Hepatitis B vaccination coverage is associated with a decrease in Life Expectancy on average.

6. Measles: For each unit increase in measles cases, Life Expectancy is expected to decrease on average.

7. Polio: Higher Polio vaccination coverage is associated with an increase in Life Expectancy on average.

8. Diphtheria: Higher Diphtheria vaccination coverage is associated with an increase in Life Expectancy on average.

9. HIV/AIDS: A higher prevalence of HIV/AIDS is associated with a decrease in Life Expectancy on average.

10. GDP: An increase in GDP is associated with an increase in Life Expectancy on average.

11. Schooling: An increase in schooling years is associated with an increase in Life Expectancy on average.

In summary, these findings were logical to us, and they highlight the impact of various factors on Life Expectancy. Factors such as adult mortality, vaccination coverage, disease prevalence, socioeconomic indicators, and lifestyle choices, like alcohol consumption, play a significant role in shaping Life Expectancy outcomes.




```{r}
ctable <- as.table(matrix(c(42, 6, 8, 28), nrow = 2, byrow = TRUE))
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")
```




# Classification

In this section, we will address a *binary classification* problem. Our objective will be to determine whether a country's life expectancy is *below or above the (Italian) pension threshold of 67 years*. To approach this, we will generate a binary variable by using a threshold on the Life expectancy column. This parameter can be of great value to businesses, banks, and other industries seeking insights related to retirement planning.

The binary classification of life expectancy can be used to analyze the socioeconomic factors contributing to disparities in longevity. By examining countries below the pension threshold, researchers and organizations can gain insights into underlying causes, such as healthcare access, social support systems, economic conditions, and lifestyle factors. This knowledge can inform policies aimed at addressing disparities and improving public health.

```{r}
standard_pension_age <- 67
LifeExp <- LifeExp %>% mutate(Pension_Status = ifelse(Life.expectancy < standard_pension_age, "Below", "Above"))
LifeExp[,"Pension_Status"] <- as.factor(LifeExp[,"Pension_Status"])
LifeExp <- LifeExp[, !(names(LifeExp) %in% "Life.expectancy")]

LifeExp$Country <- as.numeric(factor(LifeExp$Country, labels = unique(LifeExp$Country)))
LifeExp$Year <- as.numeric(LifeExp$Year)
LifeExp$Year <- LifeExp$Year - min(LifeExp$Year) + 2000

LifeExp_scaled <- LifeExp %>% mutate(across(where(is.numeric), scale))

prop.table(table(LifeExp_scaled$Pension_Status))
```
Upon applying the threshold of 67 to the Life expectancy column, we observe that the resulting categories, "Above" and "Below," contain approximately 66% and 34% of the data, respectively. This indicates a noticeable level of class imbalance. Consequently, we will assess the potential impact of this imbalance on the performance of our classifiers. Nonetheless, we will be vigilant and employ suitable metrics specifically designed for evaluating models trained on unbalanced data.

We proceed by splitting our data into a training set and a test set. To accomplish the split, we have designated the years 2014, 2010, 2007, and 2003 as the test set, while the remaining data will serve as the training set. This division allows us to evaluate the performance of our models on unseen data while using the majority of the data for training. In cases where necessary, we will further partition a portion of the training set to create a validation set.

```{r}
ctrl <- trainControl(method = "cv", number = 5)
test <- LifeExp_scaled$Year %in% c('2014', '2010', '2007', '2003')
df_test <- LifeExp_scaled[test, ]
df_train <- LifeExp_scaled[!test, ]
test_proportion <- nrow(df_test) / nrow(LifeExp_scaled)
cat('Training set dim:', dim(df_train), '\n')
cat('Training set dim:', dim(df_test), '\n')
cat('Training proportion:', 1-test_proportion, '\n')
cat('Test proportion:', test_proportion)
```
As we can see, the train-test strategy resulted in a 75-25% split.

```{r}
prop.table(table(df_train$Pension_Status))
prop.table(table(df_test$Pension_Status))
```
What is more important is that both subsets contain an approximately equal distribution of the Pension Status categories.

```{r, output=FALSE}
df_metrics <- data.frame(Model_name = character(),
                         AIC = numeric(),
                         McFaddens_R2 = numeric(),
                         Accuracy = numeric(),
                         Precision = numeric(),
                         Recall = numeric(),
                         F1_score = numeric(),
                         AUC = numeric())
colnames(df_metrics) <- c("Model_name", "AIC", "McFaddens_R2", "Accuracy", "Precision", "Recall", "F1_score", "AUC")

```

## Logistic regression

Logistic regression is a widely adopted classification algorithm that serves as a common choice when dealing with linearly separable data. Considering this, we have opted to begin by training our model using logistic regression. To ensure a more reliable evaluation of the model's performance, we will employ 5-fold cross-validation, which allows for a more comprehensive assessment. By incorporating this technique, we aim to obtain more accurate estimations and insights regarding the model's effectiveness.

### Unbalanced dataset

```{r}
glm_model <- train(form = Pension_Status ~ .,
                   data = df_train,
                   trControl = ctrl,
                   method = "glm",
                   family = "binomial")
summary(glm_model)
```
Some predictor variables, such as "Year", "Adult.Mortality", and "HIV.AIDS", have statistically significant coefficients (p < 0.05), suggesting a significant association with the likelihood of life expectancy being above or below the threshold.
Other predictor variables, such as "GDP" and "Population", do not show statistically significant associations (p > 0.05).

```{r, echo=FALSE, message=FALSE}
Model_name <- "Logistic regression"

aic <- as.numeric(summary(glm_model)$aic)
mcfaddens_r2 <- as.numeric(with(summary(glm_model), 1 - deviance/null.deviance))

glm_predictions <- predict(glm_model, df_test) 
cm <- confusionMatrix(data=glm_predictions, reference = df_test$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

glm_pred_probs <- predict(glm_model, newdata = df_test, type = "prob")
glm_pred_probs_positive <- glm_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test$Pension_Status, glm_pred_probs_positive)$auc)

new_row <- data.frame(Model_name = Model_name,
                      AIC = aic,
                      McFaddens_R2 = mcfaddens_r2,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

Next, we will proceed with creating a new model that incorporates only the predictors that have been found to be statistically significant. By including only these significant predictors, we aim to simplify the model, making it more interpretable and achieving a higher level of parsimony.

Observation:  This approach does not introduce bias, as the significance level was determined solely based on analyzing the results of the training data.

Note: For the other classification algorithms, we will use both the full model and the model with reduced features, as found here, since they seem reasonable to us for predicting our target variable.

```{r}
# Select the most significant features based on the coefficient significance levels
significant_features <- c("Year", "Adult.Mortality", "percentage.expenditure", "Measles", "Total.expenditure", "Diphtheria", "HIV.AIDS", "Income.composition.of.resources", "Schooling")

# Subset the training and test data using the significant features
df_train_subset_glm <- df_train[, c("Pension_Status", significant_features)]
df_test_subset_glm <- df_test[, c("Pension_Status", significant_features)]

# Train the logistic regression model using the significant features

glm_reduced_model <- train(form = Pension_Status ~ .,
                           data = df_train_subset_glm,
                           trControl = ctrl,
                           method = "glm",
                           family = "binomial")
summary(glm_reduced_model)
```
We can see that AIC decreased from 863.01 to 862.2. Usually, when comparing two models, a difference of more than 2 in their AIC value is enough to say the model with the lower AIC is better. However, in our case this rule of thumb was not met. Therefore, we have decided to proceed with the full model. As a future work,  more sophisticated feature selection methods shall be explored. However, that was not one of the goals of this project.

```{r, echo=FALSE, message=FALSE}
Model_name <- "Logistic regression - Reduced"

aic <- as.numeric(summary(glm_reduced_model)$aic)
mcfaddens_r2 <- as.numeric(with(summary(glm_reduced_model), 1 - deviance/null.deviance))

glm_predictions <- predict(glm_reduced_model, df_test_subset_glm)
cm <- confusionMatrix(data=glm_predictions, reference = df_test_subset_glm$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

glm_reduced_pred_probs <- predict(glm_reduced_model, newdata = df_test_subset_glm, type = "prob")
glm_reduced_pred_probs_positive <- glm_reduced_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test_subset_glm$Pension_Status, glm_reduced_pred_probs_positive)$auc)

new_row <- data.frame(Model_name = Model_name,
                      AIC = aic,
                      McFaddens_R2 = mcfaddens_r2,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

### Balanced dataset

We will now proceed to balance the dataset in order to determine if the classification performance improves with balanced data. We will employ two different approaches: a synthetic method and a sampling method.

For the synthetic method, we will utilize the *"ROSE"* (Random Over Sampling Examples) library. This approach generates artificial data by using sampling methods and a smoothed bootstrap technique. Extensive research has shown that this method often produces superior results compared to traditional sampling methods, making it a reliable oversampling technique.

Next, we will use the *"ovun.sample"* package, which enables us to perform both oversampling and undersampling in a single step.

```{r}
rose_balanced_data <- ROSE(Pension_Status ~ ., data = df_train, seed = 123)$data
table(rose_balanced_data$Pension_Status)
```
After balancing the dataset, we can observe that the number of examples in each class is now more or less the same. This balanced distribution ensures that each class is adequately represented in the training process.

```{r}
glm_rose_model = train(form = Pension_Status ~ .,
                       data = rose_balanced_data,
                       trControl = ctrl,
                       method = "glm",
                       family = "binomial")
summary(glm_rose_model)
```
```{r, echo=FALSE, message=FALSE}
Model_name <- "Logistic regression - ROSE"

aic <- as.numeric(summary(glm_rose_model)$aic)
mcfaddens_r2 <- as.numeric(with(summary(glm_rose_model), 1 - deviance/null.deviance))


predictions <- predict(glm_rose_model, df_test)
cm <- confusionMatrix(data=predictions, reference = df_test$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

glm_rose_pred_probs <- predict(glm_rose_model, newdata = df_test, type = "prob")
glm_rose_pred_probs_positive <- glm_rose_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test$Pension_Status, glm_rose_pred_probs_positive)$auc)

new_row <- data.frame(Model_name = Model_name,
                      AIC = aic,
                      McFaddens_R2 = mcfaddens_r2,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

Here, we will assess the performance of Logistic Regression utilizing the ROSE method and the selected features from above.

```{r}
# Subset the training and test data using the significant features
df_train_subset_rose <- rose_balanced_data[, c("Pension_Status", significant_features)]
df_test_subset_rose  <- rose_balanced_data[, c("Pension_Status", significant_features)]

# Train the logistic regression model using the significant features

glm_rose_reduced_model <- train(form = Pension_Status ~ .,
                                data = df_train_subset_rose,
                                trControl = ctrl,
                                method = "glm",
                                family = "binomial")
summary(glm_rose_reduced_model)
```

```{r, echo=FALSE, message=FALSE}
Model_name <- "Logistic regression - ROSE - Reduced"

aic <- as.numeric(summary(glm_rose_reduced_model)$aic)
mcfaddens_r2 <- as.numeric(with(summary(glm_rose_reduced_model), 1 - deviance/null.deviance))

predictions_rose <- predict(glm_rose_reduced_model, df_test_subset_rose)
cm <- confusionMatrix(data=predictions_rose, reference = df_test_subset_rose$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

glm_rose_reduced_pred_probs <- predict(glm_rose_reduced_model, newdata = df_test_subset_rose, type = "prob")
glm_rose_reduced_pred_probs_positive <- glm_rose_reduced_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test_subset_rose$Pension_Status, glm_rose_reduced_pred_probs_positive)$auc)

new_row <- data.frame(Model_name = Model_name,
                      AIC = aic,
                      McFaddens_R2 = mcfaddens_r2,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

Next, we will compare the performance of the "ovun.sample" package with the previous approach. With "ovun.sample," we can apply a combination of oversampling and undersampling. By setting the "p" parameter to 0.5, we aim to achieve a balanced dataset with a 50% probability of the positive class. This mixed approach helps us evaluate the effectiveness of balancing the dataset.

```{r}
balanced_data_ovun <- ovun.sample(Pension_Status ~ ., data = df_train, method = "both", p=0.5, N = nrow(df_train), seed = 123)$data
# Can play with parameter p: probability of positive class in newly generated sample
table(balanced_data_ovun$Pension_Status)
```
We have achieved an almost ideal distribution of 0.504:0.496.

```{r}
glm_ovun_model = train(form = Pension_Status ~ .,
                       data = balanced_data_ovun,
                       trControl = ctrl,
                       method = "glm",
                       family = "binomial"
                       )
summary(glm_ovun_model)
```
```{r, echo=FALSE, message=FALSE}
Model_name <- "Logistic regression - Ovun"

aic <- as.numeric(summary(glm_ovun_model)$aic)
mcfaddens_r2 <- as.numeric(with(summary(glm_ovun_model), 1 - deviance/null.deviance))

predictions_ovun <- predict(glm_ovun_model, df_test)
cm <- confusionMatrix(data=predictions_ovun, reference = df_test$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

glm_pred_probs <- predict(glm_ovun_model, newdata = df_test, type = "prob")
glm_pred_probs_positive <- glm_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test$Pension_Status, glm_pred_probs_positive)$auc)

new_row <- data.frame(Model_name = Model_name,
                      AIC = aic,
                      McFaddens_R2 = mcfaddens_r2,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

Here, we will evaluate the performance of Logistic Regression using the Ovun package and the selected features from above.

```{r}
# Subset the training and test data using the significant features
df_train_subset_ovun <- balanced_data_ovun[, c("Pension_Status", significant_features)]
df_test_subset_ovun  <- balanced_data_ovun[, c("Pension_Status", significant_features)]

# Train the logistic regression model using the significant features

ovun_reduced_model <- train(form = Pension_Status ~ .,
                           data = df_train_subset_ovun,
                           trControl = ctrl,
                           method = "glm",
                           family = "binomial")
summary(ovun_reduced_model)
```
```{r, echo=FALSE, message=FALSE}
Model_name <- "Logistic regression - Ovun - Reduced"

aic <- as.numeric(summary(ovun_reduced_model)$aic)
mcfaddens_r2 <- as.numeric(with(summary(ovun_reduced_model), 1 - deviance/null.deviance))

predictions_ovun <- predict(ovun_reduced_model, df_test_subset_ovun)
cm <- confusionMatrix(data=predictions_ovun, reference = df_test_subset_ovun$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

ovun_reduced_pred_probs <- predict(ovun_reduced_model, newdata = df_test_subset_ovun, type = "prob", positive = "Above")
ovun_reduced_pred_probs_positive <- ovun_reduced_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test_subset_ovun$Pension_Status, ovun_reduced_pred_probs_positive)$auc)

new_row <- data.frame(Model_name = Model_name,
                      AIC = aic,
                      McFaddens_R2 = mcfaddens_r2,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

Let's now quickly compare the results of the unbalanced logistic regression with the two approaches used to balance the data.

```{r}
df_metrics[order(-df_metrics$F1_score, -df_metrics$AUC), ]
```

The unbalanced model performs remarkably well in comparison to the other models. It exhibits the lowest AIC value while maintaining evaluation metrics that are comparable to the models trained with balanced data.

Both the Ovun.sample and ROSE approaches demonstrate similar performance. However, considering the lower AIC value and higher McFadden's R2, we would prefer the former over the latter.

Also, we can observe that the set of selected features slightly improved the standard Logistic regression model, while the performance of the balanced models (using ROSE and Ovun) dropped.

```{r}
# Sort the dataframe based on F1_score in descending order
df_metrics <- df_metrics[order(df_metrics$F1_score, decreasing = TRUE), ]

# Keep the top two rows
df_metrics <- df_metrics[1:2, ]

# Remove the columns "AIC" and "McFaddens_R2"
df_metrics <- df_metrics[, !(names(df_metrics) %in% c("AIC", "McFaddens_R2"))]
```

## Naive Bayes

Next, we will compare our Logistic regression model with a Naive Bayes classifier. We will use the unbalanced data for the Naive Bayes classifier since it has shown good performance with Logistic regression.

```{r}
set.seed(123)
nb_model <- naiveBayes(Pension_Status ~ ., data = df_train)

# Confusion Matrix
nb_pred <- predict(nb_model, newdata = df_test)
table(df_test$Pension_Status, nb_pred)
```
The results from the confusion matrix look decent. 
 
```{r, echo=FALSE, message=FALSE}
Model_name <- "Naive Bayes"

cm <- confusionMatrix(data = nb_pred, reference = df_test$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

nb_pred_probs <- predict(nb_model, newdata = df_test, type = "raw")
nb_pred_probs_positive <- nb_pred_probs[, "Above"]
auc_nb <- as.numeric(roc(df_test$Pension_Status, nb_pred_probs_positive)$auc)


new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc_nb)

df_metrics <- rbind(df_metrics, new_row) 
``` 
 
AIC and McFadden's R2 are metrics that are calculated based on the likelihood of the model. Because Naive Bayes does not have a likelihood function, these metrics cannot be calculated for it. This is because the likelihood function is used to measure how well the model fits the data, and Naive Bayes does not have a way to measure this.

Here, we will assess and compare the performance of Naive Bayes using the selected features from above against the full model.

```{r}
set.seed(123)
nb_model_reduced <- naiveBayes(Pension_Status ~ ., data = df_train_subset_glm)

# Confusion Matrix
nb_reduced_pred <- predict(nb_model_reduced, newdata = df_test_subset_glm)
table(df_test_subset_glm$Pension_Status, nb_reduced_pred)
```
The confusion matrix shows an improvement compared to the previous results.

Naive Bayes is a probabilistic classifier that assumes that the features are independent of each other. This assumption may not hold true when there are a large number of features, as the features may be correlated with each other. By reducing the number of features, the correlation between the features is reduced, and the assumption that the features are independent is more likely to be valid. In addition, Naive Bayes is a relatively simple model, and it can be sensitive to noise in the data. By reducing the number of features, the noise in the data is reduced. This can lead to improved performance of the model.

```{r, echo=FALSE, message=FALSE}
Model_name <- "Naive Bayes - Reduced"

cm <- confusionMatrix(data = nb_reduced_pred, reference = df_test_subset_glm$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

nb_reduced_pred_probs <- predict(nb_model_reduced, newdata = df_test_subset_glm, type = "raw")
nb_reduced_pred_probs_positive <- nb_reduced_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test_subset_glm$Pension_Status, nb_reduced_pred_probs_positive)$auc)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```


## Linear Discriminant Analysis

Now, let's explore a different supervised classification technique called Linear Discriminant Analysis (LDA). This classifier utilizes the Bayes theorem to make accurate classifications.

```{r, warning=FALSE}
lda_model = train(form = Pension_Status ~ .,
                  data = df_train,
                  trControl = ctrl,
                  method = "lda")
lda_model
```

```{r, echo=FALSE, message=FALSE}
Model_name <- "Linear Discriminant Analysis"

lda_pred <- predict(lda_model, newdata = df_test)
cm <- confusionMatrix(data = lda_pred, reference = df_test$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

auc_lda <- as.numeric(roc.curve(df_test$Pension_Status, lda_pred)$auc)

lda_pred_probs <- predict(lda_model, newdata = df_test, type = "prob")
lda_pred_probs_positive <- lda_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test$Pension_Status, lda_pred_probs_positive)$auc)


# Add new row to the df_metrics dataframe
new_row <- data.frame(Model_name = "Linear Discriminant Analysis",
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
``` 

Here, we will evaluate the performance of LDA using the selected features from above in comparison to the full model.

```{r}
lda_reduced_model = train(form = Pension_Status ~ .,
                          data = df_train_subset_glm,
                          trControl = ctrl,
                          method = "lda")
lda_reduced_model
```
The full LDA model worked better than the reduced model, which is an interesting observation. However, it is possible that the difference in performance between the two models was not statistically significant. In order to determine this, we would need to conduct a statistical test.  But for the purpose of our analysis, we deemed it unnecessary.

```{r, echo=FALSE, message=FALSE}
Model_name <- "LDA - Reduced"

lda_reduced_pred <- predict(lda_reduced_model, newdata = df_test_subset_glm)
cm <- confusionMatrix(data = lda_reduced_pred, reference = df_test_subset_glm$Pension_Status)

accuracy <- as.numeric(cm$overall["Accuracy"])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

lda_reduced_pred_probs <- predict(lda_reduced_model, newdata = df_test_subset_glm, type = "prob")
lda_reduced_pred_probs_positive <- lda_reduced_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test_subset_glm$Pension_Status, lda_reduced_pred_probs_positive)$auc)


new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```


## Quadratic discriminant analysis

For the sake of completeness, we will also employ quadratic discriminant analysis to train the classification model.

```{r}
qda_model = train(form = Pension_Status ~ . - Status,
                  data = df_train,
                  trControl = ctrl,
                  method = "qda")
qda_model
```
```{r, echo=FALSE, message=FALSE}
Model_name <- "Linear Discriminant Analysis"

qda_pred <- predict(qda_model, newdata = df_test)
cm <- confusionMatrix(data = qda_pred, reference = df_test$Pension_Status)

accuracy <- as.numeric(cm$overal['Accuracy'])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

qda_pred_probs <- predict(qda_model, newdata = df_test, type = "prob")
qda_pred_probs_positive <- qda_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test$Pension_Status, qda_pred_probs_positive)$auc)

# Add new row to the df_metrics dataframe
new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
``` 

Reason for excluding Status:
Warning message:
In predict.lm(model, df) :
  prediction from a rank-deficient fit may be misleading
  
Possible problem: Two predictor variables are perfectly correlated.
Let's analyze this issue.

```{r}
# Create a contingency table of the two variables
cont_table <- table(df_train$Status, df_train$Pension_Status)

# Perform the chi-squared test of independence
chi_test <- chisq.test(cont_table)

# Print the test result
print(chi_test)

ggplot(df_train, aes(x = Status, fill = Pension_Status)) +  geom_bar()
```
As done previously, we will train a model using only a subset of the original features.

```{r}
qda_reduced_model = train(form = Pension_Status ~ .,
                    data = df_train_subset_glm,
                    trControl = ctrl,
                    method = "qda")
qda_reduced_model
```
We can observe that QDA with a reduced feature subset performed better than the full model. When utilizing QDA, the model assumes that each class follows a quadratic decision boundary. However, this assumption may not hold true when there is a large number of features, as it increases the complexity of the decision boundary.

```{r, echo=FALSE, message=FALSE}
Model_name <- "Quadratic Discriminant Analysis - Reduced"

qda_reduced_pred <- predict(qda_reduced_model, newdata = df_test_subset_glm)
cm <- confusionMatrix(data = qda_reduced_pred, reference = df_test_subset_glm$Pension_Status)

accuracy <- as.numeric(cm$overall["Accuracy"])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

qda_reduced_pred_probs <- predict(qda_reduced_model, newdata = df_test_subset_glm, type = "prob")
qda_reduced_pred_probs_positive <- qda_reduced_pred_probs[, "Above"]
auc <- as.numeric(roc(df_test_subset_glm$Pension_Status, qda_reduced_pred_probs_positive)$auc)


new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

## k-NN

To conclude the classification part, we will utilize a non-parametric classifier: k-NN (k-Nearest Neighbors).

```{r}
df_train$Status <- as.numeric(df_train$Status)
df_test$Status <- as.numeric(df_test$Status)

# Set the seed for reproducibility
set.seed(123)

# Define the cross-validation parameters
ctrl <- trainControl(method = "cv", number = 5, summaryFunction = prSummary, classProbs = TRUE)

# Perform cross-validation to select the best k
k_values <- seq(3, sqrt(nrow(df_train)), by = 2)
f1_scores <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  # Train the model using cross-validation
  knn_model <- train(
    x = df_train[, -ncol(df_train)],
    y = df_train$Pension_Status,
    method = "knn",
    trControl = ctrl,
    tuneGrid = data.frame(k = k),
    metric = "F"
  )
  f1_scores[i] <- knn_model$results$F[1]
}

best_k <- k_values[which.max(f1_scores)]
cat("Best k value:", best_k, "\n")

# Train the model on the entire training set using the best k
final_knn_pred <- knn(train = df_train[, -ncol(df_train)],
                      test  = df_test[, -ncol(df_test)],
                      cl    = df_train$Pension_Status,
                      k     = best_k
)

table(final_knn_pred, df_test$Pension_Status)
```
To determine the optimal number of neighbors (k), we conducted cross-validation using a range of values from 3 to $sqrt(n)$, where n is the number of training examples.

In our case, the best value for k was found to be 27. However, it is worth noting that the k-NN classifier performs poorly when predicting the 'Below' class, displaying lower performance compared to random chance.

```{r, echo=FALSE, message=FALSE}
Model_name <- "K-Nearest Neighbors"

cm_knn <- confusionMatrix(data = final_knn_pred, reference = df_test$Pension_Status)

accuracy <- as.numeric(cm_knn$overall["Accuracy"])
precision <- as.numeric(cm_knn$byClass["Precision"])
recall <- as.numeric(cm_knn$byClass["Recall"])
f1_score <- as.numeric(cm_knn$byClass["F1"])

knn_pred_probs <- knn(train = df_train[, -ncol(df_train)],
                      test  = df_test[, -ncol(df_test)],
                      cl    = df_train$Pension_Status,
                      k     = best_k,
                      prob  = TRUE
)

auc <- as.numeric(roc.curve(df_test$Pension_Status, knn_pred_probs)$auc)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

Similarly as before, we will attempt to improve the performance of the k-NN classifier by utilizing the selected subset of features.

```{r}
# Set the seed for reproducibility
set.seed(123)

# Define the cross-validation parameters
ctrl <- trainControl(method = "cv", number = 5, summaryFunction = prSummary, classProbs = TRUE)

# Perform cross-validation to select the best k
k_values <- seq(3, sqrt(nrow(df_train_subset_glm)), by = 2)
f1_scores <- numeric(length(k_values))

for (i in seq_along(k_values)) {
  k <- k_values[i]
  
  # Train the model using cross-validation
  knn_reduced_model <- train(x <- df_train_subset_glm[, -1],
                             y = df_train_subset_glm$Pension_Status,
                             method = "knn",
                             trControl = ctrl,
                             tuneGrid = data.frame(k = k),
                             metric = "F")
  f1_scores[i] <- knn_reduced_model$results$F[1]
}

best_k <- k_values[which.max(f1_scores)]
cat("Best k value:", best_k, "\n")

# Train the model on the entire training set using the best k
final_knn_reduced_pred <- knn(train = df_train_subset_glm[, -1],
                              test  = df_test_subset_glm[, -1],
                              cl    = df_train_subset_glm$Pension_Status,
                              k     = best_k)

table(final_knn_reduced_pred, df_test_subset_glm$Pension_Status)
```
Now, with the model selecting 11 as the optimal value for k, it is evident from the confusion matrix that the model's performance has significantly improved. In general, a lower number of features can be beneficial for k-NN classification. With a reduced feature set, the curse of dimensionality is mitigated, which can lead to improved performance of the k-NN algorithm. 

```{r, echo=FALSE, message=FALSE}
Model_name <- "K-Nearest Neighbors - Reduced"

cm <- confusionMatrix(data = final_knn_reduced_pred, reference = df_test_subset_glm$Pension_Status)

accuracy <- as.numeric(cm$overall["Accuracy"])
precision <- as.numeric(cm$byClass["Precision"])
recall <- as.numeric(cm$byClass["Recall"])
f1_score <- as.numeric(cm$byClass["F1"])

knn_pred_probs <- knn(train = df_train_subset_glm[, -1],
                      test  = df_test_subset_glm[, -1],
                      cl    = df_train_subset_glm$Pension_Status,
                      k     = best_k,
                      prob  = TRUE
)

auc <- as.numeric(roc.curve(df_test_subset_glm$Pension_Status, knn_pred_probs)$auc)

new_row <- data.frame(Model_name = Model_name,
                      Accuracy = accuracy,
                      Precision = precision,
                      Recall = recall,
                      F1_score = f1_score,
                      AUC = auc)

df_metrics <- rbind(df_metrics, new_row)
```

```{r}
df_metrics[order(-df_metrics$F1_score, -df_metrics$AUC), ]
```

In our final analysis, we have chosen to prioritize the F1 score as a significant metric, especially for evaluating models with unbalanced datasets. Additionally, we have incorporated the AUC metric to gauge overall model performance. Taking into account the values of these two metrics, we have identified the top-performing models as follows:

1. Logistic Regression - Reduced
2. Linear Discriminant Analysis
3. Quadratic Discriminant Analysis - Reduced
4. Naive Bayes - Reduced

To determine the overall winner, it is important to carefully assess the strengths of each model in comparison to the others. The Logistic Regression - Reduced model displays superior performance in terms of both F1 score and AUC. This model exhibits good precision, recall, and accuracy, while effectively balancing false positives and false negatives. Additionally, its reduced feature set helps address the curse of dimensionality and improves computational efficiency.

Moreover, the Logistic Regression - Reduced model offers the added advantage of being relatively straightforward to interpret, which can be valuable in certain scenarios.

In conclusion, considering the emphasis on F1 score and AUC, the Logistic Regression - Reduced model emerges as the potential winner due to its robust performance across multiple metrics. It not only achieves high predictive accuracy but also provides interpretability, making it an apt choice for our analysis.




