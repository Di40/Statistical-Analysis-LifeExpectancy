---
title: "Life Expectancy Statistical Analysis"
author: "Marija Cveevska, Dejan Dichoski"
date: "2023-05-24"
output: 
  html_document:
    keep_md: yes
    toc: yes
    df_print: kable
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r, results="hide", message = FALSE, echo=FALSE}
library(ggplot2)
library(ggpubr)
library(GGally)
library(ggcorrplot)
library(plotly)
library(tidyverse)
library(cowplot)
library(psych)
library(lattice)
library(xtable)
library(plyr)
library(dplyr)
library(gridExtra)
library(WVPlots)
library(rcompanion)
library(knitr)
library(DescTools)
library(car)
library(caret)
library(leaps)
library(olsrr)
library(glmnet)
library(lmtest)
library(corrr)
```
## Why is it important to have knowledge about life expectancy?

> Understanding life expectancy, which refers to the average duration a person is projected to live, holds significance as it serves as a comprehensive measure of community well-being. Factors such as elevated infant mortality rates, increased occurrences of suicide, limited access to quality healthcare, and other determinants can contribute to lower life expectancies. In addition, life expectancy finds various applications in the financial domain, encompassing areas such as life insurance, pension planning, and Social Security benefits.

Life expectancy prediction is valuable for insurance businesses and bank loans because it allows them to assess risk and make informed decisions.Accurate life expectancy prediction enables businesses to make more precise risk assessments and pricing decisions. It helps them manage their financial exposure, align their products with the appropriate risk levels, and ensure the long-term sustainability of their operations. 

Insurance Businesses:

 - Life Insurance: Life expectancy prediction helps insurance companies assess the risk associated with providing life insurance policies. By estimating life expectancy, they can determine the likelihood of a policyholder's lifespan exceeding the policy term. This information allows insurers to set appropriate premiums and policy terms based on the individual's life expectancy.
 - Annuities and Pension Plans: For annuities and pension plans, life expectancy prediction helps insurers calculate the expected payout duration. It allows them to estimate the length of time they will need to provide benefits, impacting the pricing and financial sustainability of these products.

Bank Loans:

 - Mortgage and Home Loans: Life expectancy prediction can be used to assess the risk associated with long-term loans such as mortgages. It helps lenders evaluate the probability of borrowers completing their loan terms based on their life expectancy. This information can influence loan approval decisions, interest rates, and loan terms.
 - Personal Loans: In cases where personal loans involve significant sums or extended repayment periods, life expectancy prediction can provide insights into the borrower's ability to repay the loan. It helps lenders evaluate the risk of default or early termination due to the borrower's life expectancy.

## About the Dataset

The aim of this work is to analyze a dataset related to life expectancy. It contains data for 193 countries, in a span of 15 years, which has been collected from the WHO data repository website and its corresponding economic data was collected from United Nation website. We obtained this dataset from Kaggle and it is publicly available for educational purposes. (TODO: Fix fillowing sentence when done with the analysis) The analysis will consist of data cleaning, exploratory data analysis (EDA), a simple case of linear regression, a more complete study of multiple linear regression and finally a binary classification problem.

We start our project by loading the dataset from the local drive to a dataframe.

```{r, echo=FALSE}
tryCatch({
  # LifeExp <- read.csv("C:/Users/Dejan/Desktop/StatProject_LifeExpectancy/LifeExpectancyData.csv")
  LifeExp <- read.csv("C:/Users/marij/Desktop/Statistical-Analysis-LifeExpectancy/LifeExpectancyData.csv")
  print("Dataset loaded.")
}, error = function(e) {
  print(paste("An error occurred while loading the dataset:", conditionMessage(e)))
})
cat(paste("Number of rows: ", nrow(LifeExp), "| Number of columns: ", ncol(LifeExp)))
```
Let's take a look at the first 5 rows:

```{r}
head(LifeExp, 5)
```

In order to perform a successful analysis, it is important to properly understand the variables presented in the data. As we saw above, the dataset contains 22 variables (features). Let's start by answering the following question:\
- What does each variable mean and what type of variable is it (Nominal/Ordinal/Interval/Ratio)?

```{r, echo=FALSE}

# Define column names and descriptions
column_names <- c("Country", "Year", "Status", "Life expectancy", "Adult Mortality", "Infant deaths", 
                  "Alcohol", "Percentage expenditure", "Hepatitis B", "Measles", "BMI", "Under-five deaths",
                  "Polio", "Total expenditure", "Diphtheria", "HIV/AIDS", "GDP", "Population",
                  "Thinness 1-19 years", "Thinness 5-9 years", "Income composition of resources",
                  "Schooling")
column_types <- c("Nominal", "Ordinal", "Nominal", rep("Ratio", length(column_names)-3))


column_descriptions <- c("Country",
                         "Data is collected from 2000 - 2015 years",
                         "Developed or Developing status",
                         "Life Expectancy in age",
                         "Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)",
                         "Number of Infant Deaths per 1000 population",
                         "Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)",
                         "Expenditure on health as a percentage of Gross Domestic Product per capita(%)",
                         "Hepatitis B (HepB) immunization coverage among 1-year-olds (%)",
                         "Number of reported cases of Measles per 1000 population",
                         "Average Body Mass Index of the entire population",
                         "Number of under-five deaths per 1000 population",
                         "Polio (Pol3) immunization coverage among 1-year-olds (%)",
                         "General government expenditure on health as a percentage of total government expenditure (%)",
                         "Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)",
                         "Deaths per 1 000 live births due to HIV/AIDS (0-4 years)",
                         "Gross Domestic Product per capita (in USD)",
                         "Population of the country",
                         "Prevalence of thinness among children and adolescents for Age 10 to 19 (%)",
                         "Prevalence of thinness among children for Age 5 to 9 (%)",
                         "Human Development Index in terms of income composition of resources (index ranging from 0 to 1)",
                         "Number of years of Schooling (years)")


# Create a data frame with column names and descriptions
data_info <- data.frame(Column = column_names, Type = column_types, Description = column_descriptions)

# Render the data frame as a formatted table
knitr::kable(data_info, align = "l", col.names = c("Column Name", "Type", "Description"))

```

Let's check whether the columns of our dataframe correspond to the described types above. To do this, we use the command str() to look at the dataset structure:

```{r}
str(LifeExp)
```
The shown result corresponds to the given dataset description, which means that there were no problems when loading the data. So, we can begin the first part of our statistical analysis, which is data preparation (aka cleaning, wrangling).

## Data Wrangling 

In this section we will carry out the cleaning of the data. First of all we will check whether some of the variables have missing values (in the form of NaN, NA and Null). If so, we will try to answer: what should be done about them? We will determine whether some columns shall be removed and whether some can be modified or created. After, we will focus our efforts on determining outliers and ways to deal with them.

Before we proceed with the missing values analysis, let's perform a couple of quick fixes to our data:\
- Rename the variable thinness..1.19.years to be more representative of its true meaning ("Prevalence of thinness among children and adolescents for Age 10 to 19 (%)").\
- Convert the character columns to factor columns (ToDo: Explain why this is useful)

```{r}
colnames(LifeExp)[colnames(LifeExp) == "thinness..1.19.years"] <- "thinness.10.19.years"

#Convert a character column to a factor column
LifeExp$Year    <- as.factor(LifeExp$Year)
LifeExp$Country <- as.factor(LifeExp$Country)
LifeExp$Status  <- as.factor(LifeExp$Status)

str(LifeExp)
```
### Checking unique values

Let's have a look at the unique values in our dataset.\
We do this just to double check that number of unique values corresponds to the dataset's description.

```{r}
# Apply the length function to unique values in each column using sapply()
num_unique_values <- sapply(LifeExp, function(x) length(unique(x)))

# Print the number of unique values for each column
for (i in 1 : ncol(LifeExp)) {
  column_name <- names(LifeExp)[i]
  num_unique <- num_unique_values[i]
  cat(column_name, ": ", num_unique, "\n")
}
```


### Missing Values

To address missing values, the following steps can be taken:\
1. Detection of missing values:
  - Identify null values in the dataset.
  - Consider if there are any alternative representations of missing values, such as zero values.
2. Dealing with missing values:
  - Decide whether to fill the null values through imputation or interpolation techniques.
  - Alternatively, consider removing the null values from the dataset if appropriate.

Lets first check the existence of explicit null values.

```{r}
is.null(LifeExp)
sum(is.na(LifeExp))
sum(is.nan(as.matrix(LifeExp)))
```

```{r}
colSums(is.na(LifeExp))
```

As we can see most of the columns contain missing values (NA), resulting in a total of 2563 missing entries. Before we decide how to deal with them, let's check whether there are some wrong entries, which we call inexplicit nulls. The simplest and fastest way to check for such entries would be to do a quick summary() and look at each variable separately and see if the values make sense based on their descriptions.

```{r}
summary(LifeExp)
```
(ToDo: Rewrite the interpretations if needed)

Judging by the summary for the variables, it seems that some values in the dataset may be inaccurate or require further investigation. Here's a breakdown of the potential issues:

1. Adult mortality of 1: It's highly unlikely for adult mortality to be as low as 1. This could indicate a measurement error. We can set a certain threshold and consider all the values below it as null.

2. Infant deaths of 0 and 1800: Having zero infant deaths per 1000 births is implausible, so it's reasonable to consider such values as null. On the other hand, 1800 infant deaths may be an outlier but could be possible in certain circumstances, such as high birthrates and a relatively small population. Further analysis is needed to determine if it's an outlier or an accurate measurement.

3. BMI of 1 and 87.3: These values seem unrealistic, as it would indicate extreme underweight or obesity across an entire population. These values may be inaccurate or outliers. Considering the nature of this variable, it might be worth investigating further or even disregarding if the data quality is questionable.

4. Under Five Deaths at zero: Similar to infant deaths, reporting zero deaths for children under five is highly unlikely. It's reasonable to treat such values as null or investigate the data source for clarification.

5. GDP per capita of 1.68: Extremely low GDP values like 1.68 (in USD) may be implausible. These low values could be outliers or inaccuracies. Further analysis and comparison with reliable sources could help determine their validity.

6. Population of 34: A population value of 34 for an entire country appears questionable. It's likely an outlier or an error in measurement that requires verification or further investigation.

It's important to note that these are assumptions based on the provided summary statistics and our domain knowledge. More thorough analysis may be necessary to make accurate conclusions about the data's quality and identify the best approach for handling these inconsistencies.

To gain a deeper understanding of these values, let's visualize them using boxplots for each variable.

```{r}
par(mfrow = c(2, 3))
variables <- c('Adult.Mortality', 'infant.deaths', 'BMI', 'under.five.deaths', 'GDP', 'Population')

for (i in 1:length(variables)) {
  boxplot(LifeExp[, variables[i]], main = variables[i])
}
```

(ToDo: Rewrite the following part if needed)

After examining the mentioned values, it appears that some of them could potentially be outliers, while others are most likely errors. To address these inconsistencies, the following modifications will be made, considering that the values are implausible:

- Adult mortality rates lower than the 5th percentile will be treated as null.\
- Infant deaths of 0 will be considered null.\
- BMIs below 10 and above 50 will be considered null.\
- Under Five deaths of 0 will be considered null.\
By making these adjustments, we can handle the values that do not align with reasonable expectations.

```{r}
num_na_before <- sum(colSums(is.na(LifeExp)))
mort_5_percentile <- quantile(LifeExp$Adult.Mortality[!is.na(LifeExp$Adult.Mortality)], probs = 0.05)
LifeExp$Adult.Mortality <- ifelse(LifeExp$Adult.Mortality < mort_5_percentile, NA, LifeExp$Adult.Mortality)
LifeExp$infant.deaths <- ifelse(LifeExp$infant.deaths == 0, NA, LifeExp$infant.deaths)
LifeExp$BMI <- ifelse(LifeExp$BMI < 10 | LifeExp$BMI > 50, NA, LifeExp$BMI)
LifeExp$under.five.deaths <- ifelse(LifeExp$under.five.deaths == 0, NA, LifeExp$under.five.deaths)
```

```{r}
cat(num_na_before, sum(colSums(is.na(LifeExp))))
```
As we can see, we increased the number of missing values from 2563 to 5763. There seems to be a significant number of null values in the dataset. For now, it might be helpful to analyze separately just the columns that contain nulls to gain more insights. The following function aims to achieve this. It identifies and presents the columns that have explicit null values, tracks the total count of such columns along with their positions in the dataframe, and provides the count and percentage of nulls for each specified column.

```{r}
nulls_breakdown <- function(df) {
  df_cols <- colnames(df)
  cols_total_count <- length(df_cols)
  cols_count <- 0
  for (loc in 1:length(df_cols)) {
    col <- df_cols[loc]
    null_count <- sum(is.na(df[[col]]))
    total_count <- length(df[[col]])
    percent_null <- round(null_count / total_count * 100, 2)
    if (null_count > 0) {
      cols_count <- cols_count + 1
      cat("[", loc - 1, "] ", col, " has ", null_count, " null values: ", percent_null, "% null\n", sep = "")
    }
  }
  cols_percent_null <- round(cols_count / cols_total_count * 100, 2)
  cat("Out of ", cols_total_count, " total columns, ", cols_count, " contain null values; ", cols_percent_null, "% columns contain null values.\n", sep = "")
}

nulls_breakdown(LifeExp)
```
#### Dealing with missing values

Given that approximately half of the values in the BMI variable are null, we decided that it would be the best to exclude this variable from the analysis.

```{r}
LifeExp <- LifeExp[, !(colnames(LifeExp) %in% c('BMI'))]
```

When dealing with NA values, there are multiple approaches that can be considered:

- Fill with the mean value: Replace the NA values with the mean value of the respective variable.
- Dropping:
  - Drop all rows that contain at least one NA value: Remove the rows from the dataset that have any NA values.
  - Drop rows that contain more than 50% NA values: Exclude the rows that have more than 50% NA values, while keeping the remaining rows. The NA values can be filled with the mean value.
- Interpolation: If the data represents a time series, interpolation techniques can be applied to estimate the missing values based on the existing data points.

Since we are dealing with time series data assorted by country, the best approach would be to interpolate the data by country. However, from a careful examination of the dataframe it can be seen that there are columns that have null values for each year for some countries. Therefore, we resort to imputation by year, as the second best possible method. This involves replacing the null values with the mean value of each respective year. Imputation of each year's mean is done in the following code snippet:

```{r}
imputed_data <- list()

for (year in unique(LifeExp$Year)) {
  year_data <- LifeExp[LifeExp$Year == year, ]
  for (col in colnames(year_data)[4:length(colnames(year_data))]) {
    year_data[, col] <- ifelse(is.na(year_data[, col]), mean(year_data[, col], na.rm = TRUE), year_data[, col])
  }
  imputed_data <- append(imputed_data, list(year_data))
}

LifeExp <- bind_rows(imputed_data)

nulls_breakdown(LifeExp)
```
As we can see the missing values have been successfully handled using the interpolation method. We can now proceed to address the issue of outliers. Outliers can have a significant impact on data analysis and modeling, and it is important to identify and handle them appropriately.


### Outlier analysis

Outlier analysis is an essential step in exploring and understanding life expectancy data. It helps ensure data quality, identify anomalies and patterns, improve statistical analysis, inform policy decisions, and enhance data visualization and reporting.
Outliers increase the error variance and reduce the power of statistical tests. They can cause bias and/or influence estimates. They can also impact the basic assumption of regression as well as other statistical models.

Similar to missing values, to address the presence of outliers in the data, several steps can be taken:

1. Outlier detection: Identify the outliers in the dataset using appropriate statistical methods or techniques.
  - Visualization: Create boxplots and histograms to visually inspect the distribution of the data and identify any potential outliers.
  - Tukey's method: Apply Tukey's method or other outlier detection algorithms to determine the presence of outliers based on statistical thresholds or criteria.
2. Outlier treatment: Decide on the approach for handling outliers. Options include:
  - Dropping outliers: Remove the outlier observations from the dataset.
  - Limiting/Winsorizing outliers: Cap the extreme values at a predetermined threshold.
  - Transforming the data: Apply transformations such as logarithmic, inverse, or square root transformations to reduce the impact of outliers.

Each of these steps aims to identify, assess, and appropriately handle the outliers in the dataset to ensure robust analysis and accurate interpretations.


#### Outlier detection

For each continuous variable in the dataset, a boxplot will be generated, displaying the median, quartiles, and any potential outliers. Additionally, histograms will be created to visualize the frequency distribution of the variable.

By examining these plots, we can visually inspect the data and determine if there are any data points that deviate significantly from the majority of the observations, indicating the presence of potential outliers.

ToDo: Improve the plots here. Idea: place the boxplot above/bellow the histogram - combo plot.

```{r}
cont_vars <- colnames(LifeExp)[4:length(colnames(LifeExp))]
num_vars <- length(cont_vars)
num_rows <- ceiling(sqrt(num_vars))
num_cols <- ceiling(num_vars / num_rows)

par(oma = c(0, 0, 0, 0))

par(mfrow = c(num_rows, num_cols))

for (i in 1:num_vars) {
  col <- cont_vars[i]
  
  # Reduce the inner margins to make the plots larger
  par(mar = c(2, 2, 1, 1))
  
  boxplot(LifeExp[, col], horizontal = TRUE, col = rgb(0.8, 0.8, 0, 0.5), frame = FALSE, main = paste(col))
  
  # Increase the inner margins for the histogram to make it taller
  par(mar = c(4, 2, 1, 1))
  
  hist(LifeExp[, col], col = rgb(0.2, 0.8, 0.5, 0.5), main = paste(col), xlab = "")
}
```
```{r,echo = FALSE, message=FALSE}

p1 <- ggplot(LifeExp, aes(x=Life.expectancy)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b1 <- ggplot(LifeExp, aes(x=Life.expectancy)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Life Expectancy")

p2 <- ggplot(LifeExp, aes(x=Adult.Mortality)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b2 <- ggplot(LifeExp, aes(x=Adult.Mortality)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Adult Mortality")

p3 <- ggplot(LifeExp, aes(x=infant.deaths)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b3 <- ggplot(LifeExp, aes(x=infant.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Infant Deaths")

p4 <- ggplot(LifeExp, aes(x=Alcohol)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b4 <- ggplot(LifeExp, aes(x=Alcohol)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Alcohol")

p5 <- ggplot(LifeExp, aes(x=percentage.expenditure)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b5 <- ggplot(LifeExp, aes(x=percentage.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Percentage Expenditure")

p6 <- ggplot(LifeExp, aes(x=Hepatitis.B)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b6 <- ggplot(LifeExp, aes(x=Hepatitis.B)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Hepatitis.B")

p7 <- ggplot(LifeExp, aes(x=Measles)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b7 <- ggplot(LifeExp, aes(x=Measles)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Measles")

p8 <- ggplot(LifeExp, aes(x=Schooling)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b8 <- ggplot(LifeExp, aes(x=Schooling)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Schooling")

p9 <- ggplot(LifeExp, aes(x=under.five.deaths)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b9 <- ggplot(LifeExp, aes(x=under.five.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("under five deaths")

p10 <- ggplot(LifeExp, aes(x=Polio)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b10 <- ggplot(LifeExp, aes(x=Polio)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Polio")

p11 <- ggplot(LifeExp, aes(x=Total.expenditure)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b11 <- ggplot(LifeExp, aes(x=Total.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Total.expenditure")

p12 <- ggplot(LifeExp, aes(x=Diphtheria)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b12 <- ggplot(LifeExp, aes(x=Diphtheria)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Diphtheria")

p13 <- ggplot(LifeExp, aes(x=HIV.AIDS)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b13 <- ggplot(LifeExp, aes(x=HIV.AIDS)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("HIV / AIDS")

p14 <- ggplot(LifeExp, aes(x=GDP)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b14 <- ggplot(LifeExp, aes(x=GDP)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("GDP")

p15 <- ggplot(LifeExp, aes(x=Population)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b15 <- ggplot(LifeExp, aes(x=Population)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Population")

p16 <- ggplot(LifeExp, aes(x=thinness.10.19.years)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b16 <- ggplot(LifeExp, aes(x=thinness.10.19.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.10.19.years")

p17 <- ggplot(LifeExp, aes(x=thinness.5.9.years)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b17 <- ggplot(LifeExp, aes(x=thinness.5.9.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.5.9.years")

p18 <- ggplot(LifeExp, aes(x=Income.composition.of.resources)) +
geom_histogram(aes(y = ..density..),
colour = 1, fill = "white") +
geom_density(lwd = 1, colour = 2,
fill = 2, alpha = 0.25)

b18 <- ggplot(LifeExp, aes(x=Income.composition.of.resources)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Income per resources")



grid.arrange(p1, b1, p2, b2, p3, b3, p4, b4, p5, b5, p6, b6,p7, b7, p8, b8)
grid.arrange(p9, b9,p10, b10, p11, b11, p12, b12, p14, b14,p15, b15, p17, b17, p18, b18)



```

Visually inspecting the data, it is evident that there are several outliers present in the variables, including the target variable (life expectancy). To statistically identify outliers, Tukey's method can be applied. This method defines outliers as values that lie outside 1.5 times the interquartile range (IQR). By calculating the quartiles and IQR for each variable, the upper and lower bounds can be determined, and any values outside these bounds can be classified as outliers.

```{r}
LifeExp_old <- LifeExp

```



```{r}
cont_vars <- colnames(LifeExp)[4:length(colnames(LifeExp))]

outlier_counts <- sapply(LifeExp[cont_vars], function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  outliers <- x < lower_bound | x > upper_bound
  sum(outliers, na.rm = TRUE)
})

outlier_percent <- outlier_counts / nrow(LifeExp) * 100

outliers_df <- data.frame(OutlierCount = outlier_counts, OutlierPercent = outlier_percent)
kable(outliers_df, format = "markdown")
```

There seem to be a considerable number of outliers present in this dataset. Now that they have been identified, a question arises: What course of action should be taken regarding them?


#### Dealing with Outliers

There are several approaches to handling outliers in a dataset, and the typical options are as follows:

1. Discard Outliers (preferably avoided to retain maximum information):
   - This involves removing the data points that are considered outliers.
  
2. Apply Boundaries (Winsorization):
   - Set upper and/or lower limits for the values, effectively capping extreme values without removing them entirely.

3. Data Transformation:
   - Utilize mathematical transformations such as logarithmic, inverse, square root, etc.
   - Advantages: Can normalize the data and potentially eliminate outliers.
   - Disadvantages: Cannot be applied to variables containing values of zero or below, as it may lead to undefined results.


Given that each variable in the dataset exhibits a unique number of outliers and these outliers are distributed on different sides of the data, the most suitable approach would likely involve Winsorizing (limiting) the values for each variable individually until no outliers remain. The provided function bellow enables this process by iterating through each variable and allowing the specification of lower and/or upper limits for Winsorization. By default, the function generates two boxplots side by side for each variable, depicting the original data and the Winsorized data, respectively. Once a satisfactory limit is determined through visual analysis, the Winsorized data is saved in the wins_dict dictionary for convenient future access.



```{r}
test_wins <- function(col, lower_limit = 0, upper_limit = 0, show_plot = TRUE) {
  LifeExp_original <- LifeExp[[col]]
  q_low <- quantile(LifeExp[[col]], lower_limit)
  q_up <- quantile(LifeExp[[col]], 1 - upper_limit)
  LifeExp[[col]] <<- pmin(pmax(LifeExp[[col]], q_low), q_up)
  # <<- Allows us to modify variables outside of the local scope of a function.
  
  if (show_plot) {
    ylim <- range(LifeExp_original, na.rm = TRUE)
    par(mfrow = c(1, 2))
    boxplot(LifeExp_original, main = paste0("original\n ", col), ylim = ylim)
    boxplot(LifeExp[[col]], main = paste0("wins=(", lower_limit, ",", upper_limit, ")\n", col), ylim = ylim)
  }
}

test_wins(cont_vars[1], lower_limit = 0.01, show_plot = FALSE)
test_wins(cont_vars[2], upper_limit = 0.04, show_plot = FALSE)
test_wins(cont_vars[3], upper_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[4], upper_limit = 0.0025, show_plot = FALSE)
test_wins(cont_vars[5], upper_limit = 0.135, show_plot = FALSE)
test_wins(cont_vars[6], lower_limit = 0.1, show_plot = FALSE)
test_wins(cont_vars[7], upper_limit = 0.19, show_plot = FALSE)
test_wins(cont_vars[8], upper_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[9], lower_limit = 0.1, show_plot = FALSE)
test_wins(cont_vars[10], upper_limit = 0.02, show_plot = FALSE)
test_wins(cont_vars[11], lower_limit = 0.105, show_plot = FALSE)
test_wins(cont_vars[12], upper_limit = 0.185, show_plot = FALSE)
test_wins(cont_vars[13], upper_limit = 0.105, show_plot = FALSE)
test_wins(cont_vars[14], upper_limit = 0.07, show_plot = FALSE)
test_wins(cont_vars[15], upper_limit = 0.035, show_plot = FALSE)
test_wins(cont_vars[16], upper_limit = 0.035, show_plot = FALSE)
test_wins(cont_vars[17], lower_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[18], lower_limit = 0.025, upper_limit = 0.005, show_plot = FALSE)

```


```{r,echo = FALSE, message=FALSE}


b1 <- ggplot(LifeExp, aes(x=Life.expectancy)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Life Expectancy")

b1_old <- ggplot(LifeExp_old, aes(x=Life.expectancy)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Life Expectancy")

b2 <- ggplot(LifeExp, aes(x=Adult.Mortality)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Adult Mortality")

b2_old <- ggplot(LifeExp_old, aes(x=Adult.Mortality)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Adult Mortality")

b3 <- ggplot(LifeExp, aes(x=infant.deaths)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Infant Deaths")

b3_old <- ggplot(LifeExp_old, aes(x=infant.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Infant Deaths")

b4 <- ggplot(LifeExp, aes(x=Alcohol)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Alcohol")

b4_old <- ggplot(LifeExp_old, aes(x=Alcohol)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Alcohol")

b5 <- ggplot(LifeExp, aes(x=percentage.expenditure)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Percentage Expenditure")

b5_old <- ggplot(LifeExp_old, aes(x=percentage.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Percentage Expenditure")

b6 <- ggplot(LifeExp, aes(x=Hepatitis.B)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Hepatitis.B")

b6_old <- ggplot(LifeExp_old, aes(x=Hepatitis.B)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Hepatitis.B")

b7 <- ggplot(LifeExp, aes(x=Measles)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Measles")

b7_old <- ggplot(LifeExp_old, aes(x=Measles)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Measles")

b8 <- ggplot(LifeExp, aes(x=Schooling)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Schooling")

b8_old <- ggplot(LifeExp_old, aes(x=Schooling)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Schooling")

b9 <- ggplot(LifeExp, aes(x=under.five.deaths)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("under five deaths")

b9_old <- ggplot(LifeExp_old, aes(x=under.five.deaths)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("under five deaths")

b10 <- ggplot(LifeExp, aes(x=Polio)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Polio")

b10_old <- ggplot(LifeExp_old, aes(x=Polio)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Polio")


b11 <- ggplot(LifeExp, aes(x=Total.expenditure)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Total.expenditure")

b11_old <- ggplot(LifeExp_old, aes(x=Total.expenditure)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Total.expenditure")

b12 <- ggplot(LifeExp, aes(x=Diphtheria)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Diphtheria")

b12_old <- ggplot(LifeExp_old, aes(x=Diphtheria)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Diphtheria")


b13 <- ggplot(LifeExp, aes(x=HIV.AIDS)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("HIV / AIDS")

b13_old <- ggplot(LifeExp_old, aes(x=HIV.AIDS)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("HIV / AIDS")


b14 <- ggplot(LifeExp, aes(x=GDP)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("GDP")

b14_old <- ggplot(LifeExp_old, aes(x=GDP)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("GDP")


b15 <- ggplot(LifeExp, aes(x=Population)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Population")

b15_old <- ggplot(LifeExp_old, aes(x=Population)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Population")


b16 <- ggplot(LifeExp, aes(x=thinness.10.19.years)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("thinness.10.19.years")

b16_old <- ggplot(LifeExp_old, aes(x=thinness.10.19.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.10.19.years")


b17 <- ggplot(LifeExp, aes(x=thinness.5.9.years)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("thinness.5.9.years")

b17_old <- ggplot(LifeExp_old, aes(x=thinness.5.9.years)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("thinness.5.9.years")


b18 <- ggplot(LifeExp, aes(x=Income.composition.of.resources)) +
geom_boxplot(fill=2, alpha=0.2) +
xlab("Income per resources")

b18_old <- ggplot(LifeExp_old, aes(x=Income.composition.of.resources)) +
geom_boxplot(fill=5, alpha=0.2) +
xlab("Income per resources")



grid.arrange(b1_old, b1,b3_old, b3, b14_old, b14, b15_old, b15, ncol = 2)



```

Let's also compare the before and after distributions of some of our variables

```{r}
p1 <- ggplot() +
  geom_density(data = LifeExp_old, aes(x = Life.expectancy), color = 5, lwd = 1.2) +
  geom_density(data = LifeExp, aes(x = Life.expectancy), color = 2, lwd = 0.8) +
  xlab("Life Expectancy Winsorization")

p2 <- ggplot() +
  geom_density(data = LifeExp_old, aes(x = infant.deaths), color = 5, lwd = 1.2) +
  geom_density(data = LifeExp, aes(x = infant.deaths), color = 2, lwd = 0.8) +
  xlab("Infant Deaths Winsorization")

p3 <- ggplot() +
  geom_density(data = LifeExp_old, aes(x = GDP), color = 5, lwd = 1.2) +
  geom_density(data = LifeExp, aes(x = GDP), color = 2, lwd = 0.8) +
  xlab("GDP Winsorization")

p4 <- ggplot() +
  geom_density(data = LifeExp_old, aes(x = Population), color = 5, lwd = 1.2) +
  geom_density(data = LifeExp, aes(x = Population), color = 2, lwd = 0.8) +
  xlab("Population Winsorization")

grid.arrange(p1,p2,p3,p4, ncol = 2)
```



## Exploratory Data Analysis


### Q1. Does the sample gives enough evidence to say that Developed countries have more average life expectancy than Developing countries?
```{r}
ggplot(LifeExp, aes(x = Status, fill = Status)) +
  geom_bar(alpha = 0.6) +
  scale_fill_manual(values = c(2, 5)) +
  labs(title = "Status of Country", x = "Status", y = "Count") +
  theme(axis.text = element_text(size = 8),
        axis.title = element_text(size = 8),
        axis.text.y = element_text(size = 8),
        axis.title.y = element_text(size = 8))
```


```{r}
ggplot(LifeExp, aes(x=Life.expectancy, fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "Life Expectancy by Status", x ="Life Expectancy", y="Density") +
  scale_fill_manual(values = c(2, 5))

```

Due to our lack of knowledge regarding the population variance, we will employ a two-sample T-Test instead of a two-sample Z-Test in order to assess the equality of the two means. Prior to conducting the T-Test, it is necessary to determine whether the variances of the two populations are equal. To accomplish this, we will employ an F-Test.

To start, we will filter and group the data by country, allowing us to calculate the average life expectancy for each country over the span of 16 years.


```{r}

Developing_X <- aggregate(Life.expectancy ~ Country, data = LifeExp %>% filter (Status == "Developing"), FUN = mean)

Developed_Y <- aggregate(Life.expectancy ~ Country, data = LifeExp %>% filter (Status == "Developed"), FUN = mean)
```
```{r}
head(Developing_X,5)
head(Developed_Y,5)
```


```{r}
var.test(Developed_Y$Life.expectancy,Developing_X$Life.expectancy)
```
The observed p-value is less than alpha(0.05 by default). Hence we reject the null hypothesis and accept the alternate statement that the variance of two populations is not equal.


```{r}
t.test(Developed_Y$Life.expectancy,Developing_X$Life.expectancy, alternative = "greater", var.equal = FALSE)
```
The final conclusion is that the null hypothesis is rejected against the alternative hypothesis as the p-value<0.05. Hence we conclude that life expectancy in developed countries is more than that of developing countries with 95% confidence.

### Q2 Using the sample, test whether schooling years (average) has a significant impact on life expectancy?

Education creates awareness about healthy living. For example Vaccine hesitancy during this Covid-19 period, especially among the rural population, has highlighted the importance of education.

We will be using the ANOVA test to test the significance of education on life expectancy. Here we will categorize countries into one of the three categories: ‘Low’ (≤8), ‘Medium’(>8 and ≤12), ‘High’ (>12) depending upon the country’s average schooling years.

Firstly, we will group the data by country and find the average life expectancy and Schooling for each country over the 16 years.

```{r}
Schooling_X <- aggregate(cbind(Life.expectancy, Schooling) ~ Country, data = LifeExp, FUN = mean)

head(Schooling_X,5)
```
```{r}
x <- Schooling_X %>% filter(Schooling < 8.0)
y <- Schooling_X %>% filter(Schooling > 8.0 & Schooling <= 12.0 )
z <- Schooling_X %>% filter(Schooling < 12.0)

y1 <- data.frame(Life.expectancy = x$Life.expectancy)
y1$Education = 'Low'
y2 <- data.frame(Life.expectancy = y$Life.expectancy)
y2$Education = 'Middle'
y3 <- data.frame(Life.expectancy = z$Life.expectancy)
y3$Education = 'High'

```
```{r}
Schooling_Y <- data.frame(rbind(y1,y2,y3))
head(Schooling_Y,5)
```
Let's apply the ANOVA test.

```{r}
Anova_Results <- aov(Life.expectancy ~ Education, data = Schooling_Y)
summary(Anova_Results)
```
Df (Degrees of Freedom): The "Education" factor has 2 degrees of freedom, indicating that there were two levels or groups within this factor.

Sum Sq (Sum of Squares): The sum of squares for "Education" is 391. This value represents the variability in the response variable ("Education") explained by the factor "Education."

Mean Sq (Mean Sum of Squares): The mean sum of squares for "Education" is 195.33. This value represents the average variability explained by the factor "Education" across the degrees of freedom.

F value: The F value is a ratio of the variability between groups (mean sum of squares) to the variability within groups (residual mean sum of squares). In this case, the F value is 2.977.

Pr(>F) (p-value): The p-value associated with the F value is 0.0535. This p-value indicates the probability of obtaining an F value as extreme as the one observed, assuming there is no effect of "Education." It represents the level of statistical significance.

Interpretation:
In this ANOVA summary, the p-value (Pr(>F)) is slightly above the commonly used threshold of 0.05 (significance level). Therefore, there is weak evidence to suggest that the "Education" factor has a statistically significant effect on the response variable. However, since the p-value is close to 0.05, it is on the borderline of significance. Further investigation or additional data may be needed to draw more conclusive results.


### Q3 Check if countries that spend a higher proportion of their resources on human development have a higher life expectancy?

The term "Income composition of resources" refers to a component of the Human Development Index (HDI), which is a measure used to assess the overall development and well-being of a country's population. The Income composition of resources specifically focuses on income-related indicators and their contribution to human development.

In the context of the HDI, the Income composition of resources index is a numerical value that ranges from 0 to 1. It reflects the extent to which a country's income distribution contributes to overall human development.

A higher value of the Income composition of resources index indicates a more equitable distribution of income, where a larger proportion of the population has access to resources and opportunities for development. This suggests that income is shared more evenly among individuals within the country.

Conversely, a lower value of the index indicates a more unequal income distribution, with a smaller proportion of the population having access to resources and opportunities for development. This suggests that income is concentrated in the hands of a smaller segment of the population.


```{r}
life_expectancy_vs_incomecomp <- ggplot(LifeExp, aes(Income.composition.of.resources, Life.expectancy)) + geom_jitter(color = 2, alpha = 0.3)

life_expectancy_vs_incomecomp
```


By this scatterplot we can see that The Income composition of resources and their contribution to human development positively influences the Life Expectancy but let's see the correlation also with the Pearson correlation coefficient.


```{r}
in_comp_res <-aggregate(cbind(Life.expectancy, Income.composition.of.resources) ~ Country, data = LifeExp, FUN = mean)

head(in_comp_res,5)
```
```{r}
ggscatter(in_comp_res, x = "Income.composition.of.resources", y = "Life.expectancy", add = "reg.line", conf.int = TRUE, cor.coef = TRUE, cor.method = "pearson", xlab = "HDI (Income composition of resources)", ylab = "Life Expectancy")
```
```{r}
cor.test(in_comp_res$Income.composition.of.resources,in_comp_res$Life.expectancy )
```
A correlation coefficient of 0.8376092 indicates a strong positive linear relationship between the variables being correlated, therfore the countries with higher income composition of resources for human development have better life expectancy. Also the regression line explains 82% of variance in the data. Thus countries should spend more on the human development to achieve higher life expectancy.

### Q4 Italian Government has claimed that they have spent an average of around 8.41% of their total expenditure on health for the year 2000–2015. Can we test their claim?

We will use a One-Sample t-test and not a One-sample Z-test to test the claim since we have no information about the population variance.

Firstly, we will use a filter to obtain the data for ‘Italy’ and the Total.expenditure column as it denotes % of government expenditure on health out of total government expenditure.

```{r}
Italy_X <- LifeExp %>% filter (Country == "Italy")
Italy_Y <- select(Italy_X,Total.expenditure)

t.test(Italy_Y, mu = 8.41, alternative = "two.sided")
```
We decided to also test India's claim that they have spent an average of around 5.2% of their total expenditure on health for the year 2000–2015.
```{r}
India_X <- LifeExp %>% filter (Country == "India")
India_Y <- select(India_X,Total.expenditure)

t.test(India_Y, mu = 5.2, alternative = "two.sided")
```
Since 5.2 doesn’t lie in the 95% confidence interval range [4.232587, 4.689913], we can say that the sample doesn’t give enough evidence to accept the claim made by the Indian Government. On the other hand Italy's claim of 8.4 lies in the 95% CI range of [8.320883 ,9.021617].

### Q5 What are our oservations when comparing the proportions of the number of infant deaths and the number of under-five deaths.

We will conduct a two-proportions z-test to compare the two independent proportions.

```{r}
Mort_X <- aggregate(cbind(Life.expectancy, infant.deaths, under.five.deaths) ~ Country, data = LifeExp, FUN = mean)

head(Mort_X,5)

```
infant_deaths column represents the number of infant deaths per 1000 population and similarly, under_five_deaths represents the number of under-five deaths per 1000 population. We have to use the average value of infant or under-five deaths of all the countries and take its ceiling value.
```{r}
mortx <- ceiling(mean(Mort_X$infant.deaths))
morty <- ceiling(mean(Mort_X$under.five.deaths))
argx <- c(mortx,morty)
argy <- c(1000,1000)

prop.test(argx,argy, correct = FALSE)


```
Since the p- value is greater than 0.05, we see no significant difference in the two independent proportions.


### Q6 What is the correlation of Life expectancy with Alcohol drinking habits ?

Let's use the Person correlation test.

```{r}
Alc_X <- aggregate(cbind(Life.expectancy, Alcohol) ~ Country, data = LifeExp, FUN = mean) 

head(Alc_X,5)
```

```{r}
ggscatter(Alc_X, x = "Alcohol", y = "Life.expectancy", add = "reg.line", conf.int = TRUE, cor.coef = TRUE, cor.method = "pearson", xlab = "Alcohol consumption (in litres of pure alcohol", ylab = "Life Expectancy")


```

```{r}
cor.test(Alc_X$Alcohol, Alc_X$Life.expectancy)
```
the results indicate a statistically significant moderate positive linear relationship (correlation coefficient = 0.4323943) between alcohol consumption and life expectancy. This suggests that higher levels of alcohol consumption are associated with increased life expectancy, within the range of data analyzed. However,before jumping to conclusions we need to also take in consideration other factors.

Developed countries tend to have higher levels of alcohol consumption compared to developing countries. This can be attributed to various factors such as higher income levels, greater access to alcohol, more established alcohol industries, and different cultural norms surrounding alcohol. This was also our conclusion from our previous analysis where we concluded that life expectancy in developed countries is more than that of developing countries.

Higher GDP at Developed countries can influence alcohol consumption patterns to some extent. As countries experience economic growth and an increase in GDP, there is often an associated rise in income levels and discretionary spending power. This can lead to increased alcohol consumption. 


```{r}
ggplot(LifeExp, aes(x=Alcohol, fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "Alcohol consumption by Status", x ="Alcohol", y="Density") +
  scale_fill_manual(values = c(2, 5))
```
```{r}
ggplot(LifeExp, aes(x=log(GDP), fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "GDP by Status", x ="GDP", y="Density") +
  scale_fill_manual(values = c(2, 5))
```

```{r}
life_expectancy_vs_GDP  <- ggplot(LifeExp, aes(GDP, Life.expectancy)) + geom_jitter(color = 2, alpha = 0.3)
life_expectancy_vs_GDP 
```


### Q7 Correlation between Life Expectancy and Immunization.

In Covid-19 times we all have seen the importance of immunization against the virus to increase life expectancy.Can we show that immunization against Polio and Diphtheria has a significant effect on life expectancy?

We will use a two-way ANOVA test. Here we will divide the countries into two categories for both Polio and Diphtheria. Countries having values of % immunization coverage for one-year-old greater than the median value will get category ‘High’ else ‘Low’.

```{r}
Immun_X <- aggregate(cbind(Life.expectancy, Polio, Diphtheria) ~ Country, data = LifeExp, FUN = mean)

head(Immun_X,5)
```
```{r}
xx_1 <- Immun_X %>% filter(Polio <= 85)
xx_2 <- Immun_X %>% filter(Polio > 85)
yy_1 <- Immun_X %>% filter(Diphtheria <= 85)
yy_2 <- Immun_X %>% filter(Diphtheria > 85)

zz1 <- data.frame(Life.expectancy = xx_1$Life.expectancy, Country = xx_1$Country)
zz1$Polio = 'Low'
zz2 <- data.frame(Life.expectancy = xx_2$Life.expectancy, Country = xx_2$Country)
zz2$Polio = 'High'
zz3 <- data.frame(Life.expectancy = yy_1$Life.expectancy, Country = yy_1$Country)
zz3$Diphtheria = 'Low'
zz4 <- data.frame(Life.expectancy = yy_2$Life.expectancy, Country = yy_2$Country)
zz4$Diphtheria = 'High'

```
```{r}
Immun_Polio <- data.frame(rbind(zz1,zz2))
head(Immun_Polio,5)
```

```{r}
Immun_Diphtheria <- data.frame(rbind(zz3,zz4))
head(Immun_Diphtheria,5)
```

```{r}
Immun_Y <- merge(Immun_Polio, Immun_Diphtheria, by = "Country")
head(Immun_Y,5)
```

```{r}
Anova_Results_1 <- aov(Life.expectancy.x ~ Polio + Diphtheria, data = Immun_Y)
summary(Anova_Results_1)
```
P-value for both Polio and Diphtheria immunization coverage for one-year old is less than 0.05, hence we can say that immunization has a significant impact on the life expectancy.


```{r}
life_expectancy_vs_Diphtheria  <- ggplot(LifeExp, aes(Diphtheria, Life.expectancy)) + geom_jitter(color = 5, alpha = 0.3)
                              
life_expectancy_vs_Polio  <- ggplot(LifeExp, aes(Polio, Life.expectancy)) + geom_jitter(color = 2, alpha = 0.3)

p <- plot_grid(life_expectancy_vs_Diphtheria, life_expectancy_vs_Polio) 
title <- ggdraw() + draw_label("Correlation between Immunizations and life expectancy", fontface='bold')
plot_grid(title,p, ncol=1)
```


## Constructing Correlation Matrix

```{r}
# Compute the correlation matrix
numeric_vars <- LifeExp[, sapply(LifeExp, is.numeric)]
cor_matrix <- cor(numeric_vars)

# Plot the correlation matrix using corrplot
corrplot::corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)
  
# Adjust the text size

```
#### Network plot

```{r}
cor_matrix %>% corrr::network_plot(min_cor = .3)
```



## Linear Regression

After completing the data cleaning process and exploring and engineering features, we can now move on to the main phase of this project: making predictions using linear regression. Initially, we will examine the effectiveness of a *simple linear regressor* (using only one feature). Then, we will progress to employing all the features through *multiple linear regression* or selecting a subset of features using appropriate techniques for *feature selection*.

Based on the exploratory data analysis (EDA), it is evident that our target variable, "Life expectancy," demonstrates the highest correlation with "Adult mortality." However, upon examining the scatter plot, it appears that this variable may not be suitable for linear regression. Therefore, we proceed by choosing the second most correlated variable, namely "Income.composition.of.resources". By observing the scatter plot for this particular variable, we can notice a linear trend.

Before we start, the question of whether to perform feature scaling arises. Feature scaling is not an absolute necessity for linear regression. The algorithm calculates the coefficients for each feature by computing the differences between the feature values and their mean. Therefore, feature scaling does not have a direct impact on the coefficients obtained from the linear regression model.

However, considering the benefits of feature scaling, such as ensuring consistent and meaningful comparisons, and the fact that it is a mandatory step when using regularization techniques, we have chosen to scale the data at the outset.

Afterward, to ensure unbiased evaluations, we proceed by randomly partitioning our dataset into separate training and test sets. For models that require cross-validation, we will utilize a portion of the training set as a validation set.


```{r}
LifeExp_scaled <- LifeExp %>% mutate(across(where(is.numeric), scale))
LifeExp_scaled_regression <- LifeExp_scaled %>% select(-Country, -Year)

train_percent <- 0.8
test_percent <- 1 - train_percent
num_rows <- nrow(LifeExp_scaled_regression)
train_size <- floor((train_percent) * nrow(LifeExp_scaled_regression))

set.seed(1)

train_indices <- sample.int(n = num_rows, size = train_size)

train <- LifeExp_scaled_regression[train_indices,]
test <- LifeExp_scaled_regression[-train_indices,]

cat("Train size: ", train_size, "\nTest size: ", num_rows - train_size)

Y_colname <- "Life.expectancy"
X_colnames <- colnames(LifeExp_scaled_regression)[colnames(LifeExp_scaled_regression) != Y_colname]

```

### Simple linear regression

The linear regression equation is given by: $$ Y = \beta_0 + \beta_1 \cdot x + \varepsilon $$,\
where $$\beta_0$$ represents the intercept, $$\beta_1$$ represents the coefficient for the chosen feature "Income.composition.of.resources" and $$\varepsilon$$ represents a random error.

```{r}
simple_lm <- lm(Life.expectancy ~ Income.composition.of.resources, data = train)
summary(simple_lm)
```

The analysis reveals that the variable "Income.composition.of.resources" has a moderate-to-high level of explanatory power, as indicated by an R-squared value of 0.6248. This means that approximately 62.48% of the variability in life expectancy can be attributed to this variable.

Both the intercept and the coefficient associated with "Income.composition.of.resources" are highly significant, as indicated by the p-values being less than 2e-16. The F-statistic also supports this finding, with a value of 3910 and an extremely low p-value (< 2.2e-16), suggesting that the observed relationship is not due to chance.

The formula for the regression line can be expressed as follows:\
Life.Expectancy = 41.2191 + 43.7221 * Income.composition.of.resources\
and it is visualized bellow.

```{r}
simple_lm_no_intercept <- lm(Life.expectancy ~ 0 + Income.composition.of.resources, data = train)
summary(simple_lm_no_intercept)
```


```{r}
ggplot(train, aes(x = Income.composition.of.resources, y = Life.expectancy)) +
  geom_point(color = "#80b1d3", size = 2, alpha = 0.9) +
  labs(x = "Income composition of resources", y = "Life expectancy") +
  theme_minimal() +
  theme(axis.text = element_text(size = 8),
        axis.title.x = element_text(size = 12),
        axis.title.y = element_text(size = 12)) +
  geom_abline(slope = coef(simple_lm)["Income.composition.of.resources"],
              intercept = coef(simple_lm)["(Intercept)"],
              color = "red", linetype = "solid", linewidth = 1.2)

```
That’s not the whole picture though. Residuals could show how poorly a model represents data. Residuals are leftover of the outcome variable after fitting a model (predictors) to data, and they could reveal patterns in the data unexplained by the fitted model. Using this information, not only we can check if linear regression assumptions are met, but we can improve our model in an exploratory way.

Let’s now have a look at the *diagnostic plots*, in order to check whether Linear regression assumptions are met. These assumptions include:

1. **Linearity**: There should be a linear relationship between the predictors (x) and the outcome (y). This implies that the relationship between the variables can be adequately captured by a straight line or a linear combination of predictors.

2. **Independence of predictors**: The predictors (x) should be independent of each other, meaning there should be no significant correlation or multicollinearity among the predictor variables. This assumption ensures that the predictors contribute unique information to the model.

3. **No perfect multicollinearity**: The predictors should not exhibit perfect multicollinearity, which means that they should not be perfectly correlated with each other. Perfect multicollinearity can cause numerical instability and difficulties in estimating the regression coefficients.

4. **Error term with mean zero**: The residual errors (the differences between the observed outcome and the predicted outcome) should have a mean value of zero. This assumption assumes that, on average, the model is unbiased in its predictions.

5. **Constant variance of residuals (homoscedasticity)**: The residual errors should have a constant variance across the range of predicted values. This assumption implies that the spread of residuals is consistent, regardless of the predicted values.

6. **Independence of residuals**: The residual errors should be independent of each other and independent of the predictor variables (x). Independence of residuals assumes that the errors are not influenced by any patterns or structures in the data that are not captured by the predictor variables.

It is important to assess these assumptions before relying on the results of a linear regression model to ensure the validity of the model and the reliability of the inferences drawn from it.

```{r}
par(mfrow = c(2, 2))
plot(simple_lm)
```
*Residuals vs Fitted*

This plot shows if residuals have non-linear patterns. There could be a non-linear relationship between predictor variables and the outcome variable, and the pattern could show up in this plot if the model doesn’t capture the non-linear relationship. In our case, we find almost equally spread residuals around a horizontal line without distinct patterns. So, this is a good indication that we don’t have non-linear relationships.

*Normal Q-Q*

This plot shows if residuals are normally distributed. Do residuals follow a straight line well or do they deviate severely? Thus for small sample sizes, it can't be assumed that the estimator $$\hat{\beta}$$ isn't Gaussian either, meaning the standard confidence intervals and significance tests are invalid. Our plot shows many points lying on the dotted line, however the rightmost and leftmost points are deviating from the line, suggesting slight right and left skewness.

*Scale-Location*

It's is also called a Spread-Location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how we can check the assumption of equal variance (*homoscedasticity*). It’s good if we see a horizontal line with equally (randomly) spread points, which is true in our case (the line is roughly horizontal).

*Residuals vs Leverage*

Leverage refers to the extent to which the coefficients in the regression model would change if a particular observation was removed from the dataset. Here, we watch out for outlying values at the upper right corner or at the lower right corner. Those spots are the places where cases can be influential against a regression line. When cases are outside of the dashed lines (meaning they have high “Cook’s distance” scores), the cases are influential to the regression results. The regression results will be altered if we exclude those cases.\
We can observe that there are not any influential points in our regression model.

By the previous analysis, we can conclude that our data is behaving linearly as we expected and it does have homoscedasticity.

```{r}
pred <- predict(simple_lm, newdata = test)
rmse <- sqrt(mean((test$Life.expectancy - pred)^2))
normalized_rmse_std <- sqrt(mean((pred - test$Life.expectancy)^2)) / sd(test$Life.expectancy)
normalized_rmse_range <- sqrt(mean((pred - test$Life.expectancy)^2)) / diff(range(test$Life.expectancy))
cat("RMSE =", rmse)
cat("\nNormalized RMSE (sd)    =", normalized_rmse_std)
cat("\nNormalized RMSE (range) =", normalized_rmse_range)
```
The normalized RMSE (sd) is 0.5961559, indicating that the model's predictions have an error of around 59.6 % of the standard deviation of the target variable. The normalized RMSE (range) is 0.1281596, suggesting that the model's predictions have an error of approximately 12.8% of the range of the target variable.

### Multiple linear regression

In this section we will train a regression model using multiple features. As part of the process we will select the most important features for the model using backward and forward selection and also check how well it performs on a test set.

```{r}
full_lm <- lm(train[,Y_colname] ~ ., data = train[, X_colnames])
summary(full_lm)
```
The summary indicates that the residuals are symmetrically distributed, with a median almost equal to 0 (0.0793). We will examine the residuals more closely later. The full Linear regression model explains 84.99% of the variance associated with the response variable. The F-statistic is 733.3 (>> 1), and its p-value is nearly 0, providing clear evidence against the null hypothesis that all coefficients are equal to zero. This means that at least one variable is associated with the response. The p-values of the predictor variables allow us to determine their significance. Variables with lower p-values are more significant in relation to the response. Notably, features such as infant.deaths, Alcohol, under.five.deaths, and GDP are not statistically significant in our model.

To check the multicollinearity in our data we will look at the Variance Inflation Factors (VIF):

```{r}
vif_values <- vif(full_lm)
sorted_vif <- sort(vif_values, decreasing = TRUE)
vif_table <- data.frame(VIF = sorted_vif)
vif_table
```

It is generally desirable to have VIF values as small as possible, closer to 1, which indicates low levels of collinearity.  As a rule of thumb VIF = 5 is taken as a threshold, and any independent variable with VIF > 5 will have to be removed, due to problematic amount of collinearity.

The VIF values for under.five.deaths, infant.deaths, thinness.10.19.years, thinness.5.9.years and Income.composition.of.resources have high values (> 5) that indicate collinearity problem. Therefore, we've decided to remove them from the model.

```{r}
cols_to_remove <- c("under.five.deaths", "infant.deaths", "thinness.10.19.years", "thinness.5.9.years", "Income.composition.of.resources")
X_colnames_reduced <- X_colnames[!(X_colnames %in% cols_to_remove)]


full_lm_low_vif <- lm(train[,Y_colname] ~ ., data = train[, X_colnames_reduced])
summary(full_lm_low_vif)
```
As we can see after removing columns with high VIF, the R-squared value decreased. However, it is important to note that the primary goal of removing variables with high VIF is to address the issue of collinearity and improve the model's reliability and interpretability. We expect the adjusted model to be more appropriate for analysis and inference.

Let’s now have a look at the diagnostic plots.

```{r}
par(mfrow=c(2,2))
plot(full_lm_low_vif)
```

Fitted vs Residual graph\
The red line is very close to zero and the spread of the residuals is approximately the same across the x axis, so we have no discernible non-linear trends or indications of non-constant variance.

Normal Q-Q Plot\
The residuals deviate from the diagonal line in both the upper and lower tail. This plot indicates that the tails are ‘lighter’ (have smaller values) than what we would expect under the standard modeling assumptions.

Scale-Location\
The red line deviates just a little from horizontal, showing very small presence of heteroskedasticity.

Residuals vs Leverage\
In this plot we see no evidence of outliers. The “Cook’s distance” dashed curves don’t even appear on the plot. None of the points come close to having both high residual and leverage.


### Linear model (forward / backward model selection)

Next, we can explore approaches to reduce the variance of the model, namely backward and forward model selection. Empirically, backward selection tends to perform better in this regard. However, we also retain the forward selection method for comparison purposes.


```{r, echo=FALSE}
nvmax <- 26 # 2 x number of variables

forward_sel_lm = regsubsets(train[,Y_colname] ~., data = train[, X_colnames_reduced], method = 'forward', nvmax = nvmax)
backward_sel_lm = regsubsets(train[,Y_colname] ~., data = train[, X_colnames_reduced], method = 'backward', nvmax = nvmax)
```

```{r, echo=FALSE}
plot_max_point <- function(values){
  max_idx = which.max(values)
  points(max_idx, values[max_idx], col = 'red', cex = 2, pch = 20)
}

plot_min_point <- function(values){
  min_idx = which.min(values)
  points(min_idx, values[min_idx], col = 'red', cex = 2, pch = 20)
}

plot_subsets_summary <- function(featureSelection_lm){
  
  featureSelection_lm_summary <- summary(featureSelection_lm)

  xlabel = 'Number of iterations'
  
  par(mfrow = c(3, 1))
  
  adjr2_list <- featureSelection_lm_summary$adjr2
  max_adjr2_idx <- which.max(adjr2_list)
  plot(adjr2_list, xlab = xlabel, ylab = 'Adjusted R2', type = 'l')
  plot_max_point(values = adjr2_list) # The model with highest value is the best model.
  text(x = max_adjr2_idx, y = adjr2_list[max_adjr2_idx], labels = max_adjr2_idx, pos = 1)
  
  cp_list <- featureSelection_lm_summary$cp
  min_cp_idx <- which.min(cp_list)
  plot(cp_list, xlab = xlabel, ylab = 'Cp', type = 'l')
  plot_min_point(values = cp_list) # The model with least value is the best model.
  text(x = min_cp_idx, y = cp_list[min_cp_idx], labels = min_cp_idx, pos = 3)
  
  bic_list <- featureSelection_lm_summary$bic
  min_bic_idx <- which.min(bic_list)
  plot(bic_list, xlab = xlabel, ylab = 'BIC', type = 'l')
  plot_min_point(values = bic_list) # The model with least value is the best model.
  text(x = min_bic_idx, y = bic_list[min_bic_idx], labels = min_bic_idx, pos = 3)
}
```

Both feature selection techniques yielded the same results. So, for simplicity, we will show the results only for the backward selection method.

```{r}
#plot_subsets_summary(forward_sel_lm)
plot_subsets_summary(backward_sel_lm)
```

Based on the observed graphs, it is clear that different statistical measures, such as Cp, BIC, and Adjusted R-squared, produce varying results in the feature selection process. The use of Adjusted R-squared led to a model with one less variable compared to the original model. Cp resulted in the removal of two variables, while BIC suggested eliminating four variables from the model. Notably, Cp and AIC are equivalent and select the same model, hence we have focused on presenting the results based on Cp.

```{r}
# Get the chosen variables according to each statistical measure
backward_sel_lm_summary <- summary(backward_sel_lm)
vars_adjr2 <- names(coefficients(backward_sel_lm, which.max(backward_sel_lm_summary$adjr2)))
vars_cp <- names(coefficients(backward_sel_lm, which.min(backward_sel_lm_summary$cp)))
vars_bic <- names(coefficients(backward_sel_lm, which.min(backward_sel_lm_summary$bic)))

# Get the variables in the full model
vars_full_model <- names(coef(full_lm_low_vif))

# Create a data frame to store the included variables
selected_vars_df <- data.frame(Feature = vars_full_model, AdjR2 = 0, Cp = 0, BIC = 0)

# Update the data frame with the variables included in each model
selected_vars_df$AdjR2 <- as.numeric(selected_vars_df$Feature %in% vars_adjr2)
selected_vars_df$Cp <- as.numeric(selected_vars_df$Feature %in% vars_cp)
selected_vars_df$BIC <- as.numeric(selected_vars_df$Feature %in% vars_bic)

# Remove the intercept from the data frame
selected_vars_df <- selected_vars_df[-1, ]

kable(selected_vars_df, row.names = FALSE)
```

The feature selection algorithm consistently suggests that the variable "Total.expenditure" should be eliminated from the final model. This recommendation is supported by all three statistical measures. Furthermore, both Cp and BIC also favor removing the "Population" variable. From a practical standpoint, there are valid reasons for excluding these variables:

1. Population: Including the population of a country in the model is unlikely to have a direct impact on life expectancy. Therefore, it can be deemed unnecessary for predicting life expectancy accurately.

2. Total.expenditure: The information conveyed by the "Total.expenditure" variable is largely captured by the "percentage.expenditure" variable. Hence, including both variables in the model would introduce redundancy without significantly enhancing our understanding.

By excluding these variables, the final model becomes more concise and interpretable without compromising the explained variance to a great extent. However, it is essential to verify the impact on explained variance before drawing a definitive conclusion.

To represent the most significant variables, here we report the variable elimination plots:

```{r}
plot(backward_sel_lm, scale = 'r2')
plot(backward_sel_lm, scale = 'bic')
plot(backward_sel_lm, scale = 'Cp')
```

Based on the regsubsets plot, we reach the same conclusion as before. Therefore, we will proceed with removing the variables "Population" and "Total.expenditure" from our model. After removing these variables, we will retrain the multiple linear regressor using the updated set of features.


```{r}
cols_to_remove <- c("Population", "Total.expenditure")
X_colnames_reduced_featureSel <- X_colnames_reduced[!(X_colnames_reduced %in% cols_to_remove)]

featureSel_lm <- lm(train[,Y_colname] ~ ., data = train[, X_colnames_reduced_featureSel])
summary(featureSel_lm)
```
The variance explained by the model is 83.33%, which is nearly identical to the previous result (83.34%) achieved with the full model that included all variables except those displaying multicollinearity issues.

```{r}
hist(featureSel_lm$residuals)
```
The histogram of the residuals exhibits a pattern that closely resembles a normal distribution. However, let's also check the QQ plot.

```{r}
plot(featureSel_lm, which = 2)
```
We can observe that the distribution has "fat" tails - both the ends of the Q-Q plot deviate from the straight line and its center follows a straight line. We refer to this as positive kurtosis (a measure of “Tailedness”).

Shapiro Test

```{r}
shapiro.test(residuals(featureSel_lm))
```
In the context of the Shapiro-Wilk test, the null hypothesis is that the data is normally distributed. Since the p-value is significantly small (2.916e-11 << 0.05), it suggests strong evidence to reject the null hypothesis. Therefore, based on these results, it can be inferred that the residuals do not follow a normal distribution.


#### Data Transformation

To transform the data, we will apply the log transformation. Since there are 0s present in the data, we will use the formula log(1+x) to handle these values appropriately. We will only consider the variables that were selected earlier using multicollinearity check and feature selection. Initially, we exclude the binary variable "Status" when computing the log transformation. However, we will add it back later to enhance the linear model.

Additionally, we attempted using the square root transformation but did not observe any significant improvement in the model's performance.


```{r}
X_colnames_reduced_featureSel_noStatus <- X_colnames_reduced_featureSel[!X_colnames_reduced_featureSel %in% "Status"]

train_log <- train[, c(X_colnames_reduced_featureSel_noStatus, "Life.expectancy")]
train_log <- log1p(train_log) # sqrt
train_log$Status <- train$Status

log_featureSel_lm <- lm(Life.expectancy ~ ., data = train_log)
summary(log_featureSel_lm)
```
```{r}
hist(log_featureSel_lm$residuals)
```



```{r}
shapiro.test(residuals(log_featureSel_lm))
```
From the analysis of the histogram of the residuals and the results of the Shapiro-Wilk test, it can be concluded that the data transformation did not effectively address the issue of non-normality in the residuals. The residuals continue to deviate from the assumption of normality.


### Ridge regression

For the Ridge regression, it is necessary to standardize data. However, we already did this step.

```{r, echo=FALSE}
prepare_ridge_lasso <- function(X, Y, alpha){
  grid <- 10^seq(10, -2, length=100)

  model <- glmnet(X, Y, alpha = alpha, standardize = F)
  # alpha = 0 for Ridge Regression
  # alpha = 1 for Lasso Regression
  plot(model, label=TRUE)

  set.seed(1)
  cv.out <- cv.glmnet(X, Y, alpha = alpha, nfold=10, type.measure = "mse") # Add lambda = grid,
  
  plot(cv.out)
  
  i.bestlam <- which.min(cv.out$cvm)
  bestlam <- cv.out$lambda[i.bestlam]
  cat('Best lambda:', bestlam, '\n')
  
  return(list('model' = model, 'lambda' = bestlam))
}

```

```{r, warning=FALSE}
train$Status  <- as.numeric(train$Status) - 1
X = as.matrix(train[, X_colnames_reduced_featureSel])
Y = as.matrix(train[, Y_colname])

ridge_res = prepare_ridge_lasso(X = X,
                                Y = Y,
                                alpha = 0  # Ridge
                                )
```
```{r}
ridge_model <- ridge_res$model
ridge_lambda <- ridge_res$lambda
test$Status  <- as.numeric(test$Status) - 1
ridge_pred <- predict(ridge_model,
                      s = ridge_lambda,
                      newx = as.matrix(test[, X_colnames_reduced_featureSel])
                      )
mean((ridge_pred - test$Life.expectancy)^2)
```

The value of best lambda is equal to 0.7588397
MSE with the best lambda: 28.65004

### Lasso regression

```{r, warning=FALSE}
lasso_res = prepare_ridge_lasso(X = X,
                                Y = Y,
                                alpha = 1  # Lasso
                                )
```
```{r}
lasso_model <- lasso_res$model
lasso_lambda <- lasso_res$lambda
lasso_pred <- predict(lasso_model,
                      s = lasso_lambda,
                      newx = as.matrix(test[, X_colnames_reduced_featureSel])
                      )
mean((lasso_pred - test$Life.expectancy)^2)
```

The value of best lambda is equal to: 0.02161203
MSE with the best lambda: 22.82819


We can see that the value of MSE of the lasso model is better than the ridge regularization model. So, we can conclude that the model with the L1 regularization term has slightly better predicting capabilities.


### Homoscedasticity

```{r}
bptest(featureSel_lm)
```
```{r}
ols_test_breusch_pagan(featureSel_lm)
```
Using 2 different function to test the homoscedasticity, we still get conclusion that the residuals variance is not constant.


The linear model appears to be suitable for predicting Life.expectancy based on the Adj. R-Squared value. It successfully passed 2 out of 4 Assumption Checks, namely the Multicollinearity and Linearity Test, although it did not meet the criteria for Normality and Homoscedasticity.

The Linear Model can effectively capture the linear relationship between Life.expectancy and the chosen independent variables. Nevertheless, it's crucial to acknowledge that this model is particularly sensitive to outliers, which were prevalent in the initial dataset.



# TODO: Add comparison of models

- Bar graph for different statistics of models and a table summary

# TODO: Add best model summary
- Discuss the results obtained from the summary (check Anna B.'s analysis)
- Discuss whether the assumption checks are met

```{r}
ctable <- as.table(matrix(c(42, 6, 8, 28), nrow = 2, byrow = TRUE))
fourfoldplot(ctable, color = c("#CC6666", "#99CC99"),
             conf.level = 0, margin = 1, main = "Confusion Matrix")
```





