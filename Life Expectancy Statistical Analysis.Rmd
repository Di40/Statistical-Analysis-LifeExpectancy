---
title: "Life Expectancy Statistical Analysis"
author: "Marija Cveevska, Dejan Dichoski"
date: "2023-05-24"
output: 
  html_document:
    keep_md: yes
    toc: yes
    df_print: kable
  pdf_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(warn=-1)
```

```{r, results="hide", message = FALSE, echo=FALSE}
library(ggplot2)
library(GGally)
library(ggcorrplot)
library(plotly)
library(tidyverse)
library(cowplot)
library(psych)
library(lattice)
library(xtable)
library(plyr)
library(dplyr)
library(gridExtra)
library(WVPlots)
library(rcompanion)
library(knitr)
library(DescTools)
```
## Why is it important to have knowledge about life expectancy?

> Understanding life expectancy, which refers to the average duration a person is projected to live, holds significance as it serves as a comprehensive measure of community well-being. Factors such as elevated infant mortality rates, increased occurrences of suicide, limited access to quality healthcare, and other determinants can contribute to lower life expectancies. In addition, life expectancy finds various applications in the financial domain, encompassing areas such as life insurance, pension planning, and Social Security benefits.

Life expectancy prediction is valuable for insurance businesses and bank loans because it allows them to assess risk and make informed decisions.Accurate life expectancy prediction enables businesses to make more precise risk assessments and pricing decisions. It helps them manage their financial exposure, align their products with the appropriate risk levels, and ensure the long-term sustainability of their operations. 

Insurance Businesses:

 - Life Insurance: Life expectancy prediction helps insurance companies assess the risk associated with providing life insurance policies. By estimating life expectancy, they can determine the likelihood of a policyholder's lifespan exceeding the policy term. This information allows insurers to set appropriate premiums and policy terms based on the individual's life expectancy.
 - Annuities and Pension Plans: For annuities and pension plans, life expectancy prediction helps insurers calculate the expected payout duration. It allows them to estimate the length of time they will need to provide benefits, impacting the pricing and financial sustainability of these products.

Bank Loans:

 - Mortgage and Home Loans: Life expectancy prediction can be used to assess the risk associated with long-term loans such as mortgages. It helps lenders evaluate the probability of borrowers completing their loan terms based on their life expectancy. This information can influence loan approval decisions, interest rates, and loan terms.
 - Personal Loans: In cases where personal loans involve significant sums or extended repayment periods, life expectancy prediction can provide insights into the borrower's ability to repay the loan. It helps lenders evaluate the risk of default or early termination due to the borrower's life expectancy.

## About the Dataset

The aim of this work is to analyze a dataset related to life expectancy. It contains data for 193 countries, in a span of 15 years, which has been collected from the WHO data repository website and its corresponding economic data was collected from United Nation website. We obtained this dataset from Kaggle and it is publicly available for educational purposes. (TODO: Fix fillowing sentence when done with the analysis) The analysis will consist of data cleaning, exploratory data analysis (EDA), a simple case of linear regression, a more complete study of multiple linear regression and finally a binary classification problem.

We start our project by loading the dataset from the local drive to a dataframe.

```{r, echo=FALSE}
tryCatch({
  LifeExp <- read.csv("C:/Users/Dejan/Desktop/StatProject_LifeExpectancy/LifeExpectancyData.csv")
  print("Dataset loaded.")
}, error = function(e) {
  print(paste("An error occurred while loading the dataset:", conditionMessage(e)))
})
cat(paste("Number of rows: ", nrow(LifeExp), "| Number of columns: ", ncol(LifeExp)))
```
Let's take a look at the first 5 rows:

```{r}
head(LifeExp, 5)
```

In order to perform a successful analysis, it is important to properly understand the variables presented in the data. As we saw above, the dataset contains 22 variables (features). Let's start by answering the following question:\
- What does each variable mean and what type of variable is it (Nominal/Ordinal/Interval/Ratio)?

```{r, echo=FALSE}

# Define column names and descriptions
column_names <- c("Country", "Year", "Status", "Life expectancy", "Adult Mortality", "Infant deaths", 
                  "Alcohol", "Percentage expenditure", "Hepatitis B", "Measles", "BMI", "Under-five deaths",
                  "Polio", "Total expenditure", "Diphtheria", "HIV/AIDS", "GDP", "Population",
                  "Thinness 1-19 years", "Thinness 5-9 years", "Income composition of resources",
                  "Schooling")
column_types <- c("Nominal", "Ordinal", "Nominal", rep("Ratio", length(column_names)-3))


column_descriptions <- c("Country",
                         "Data is collected from 2000 - 2015 years",
                         "Developed or Developing status",
                         "Life Expectancy in age",
                         "Adult Mortality Rates of both sexes (probability of dying between 15 and 60 years per 1000 population)",
                         "Number of Infant Deaths per 1000 population",
                         "Alcohol, recorded per capita (15+) consumption (in litres of pure alcohol)",
                         "Expenditure on health as a percentage of Gross Domestic Product per capita(%)",
                         "Hepatitis B (HepB) immunization coverage among 1-year-olds (%)",
                         "Number of reported cases of Measles per 1000 population",
                         "Average Body Mass Index of the entire population",
                         "Number of under-five deaths per 1000 population",
                         "Polio (Pol3) immunization coverage among 1-year-olds (%)",
                         "General government expenditure on health as a percentage of total government expenditure (%)",
                         "Diphtheria tetanus toxoid and pertussis (DTP3) immunization coverage among 1-year-olds (%)",
                         "Deaths per 1 000 live births due to HIV/AIDS (0-4 years)",
                         "Gross Domestic Product per capita (in USD)",
                         "Population of the country",
                         "Prevalence of thinness among children and adolescents for Age 10 to 19 (%)",
                         "Prevalence of thinness among children for Age 5 to 9 (%)",
                         "Human Development Index in terms of income composition of resources (index ranging from 0 to 1)",
                         "Number of years of Schooling (years)")


# Create a data frame with column names and descriptions
data_info <- data.frame(Column = column_names, Type = column_types, Description = column_descriptions)

# Render the data frame as a formatted table
knitr::kable(data_info, align = "l", col.names = c("Column Name", "Type", "Description"))

```

Let's check whether the columns of our dataframe correspond to the described types above. To do this, we use the command str() to look at the dataset structure:

```{r}
str(LifeExp)
```
The shown result corresponds to the given dataset description, which means that there were no problems when loading the data. So, we can begin the first part of our statistical analysis, which is data preparation (aka cleaning, wrangling).

## Data Wrangling 

In this section we will carry out the cleaning of the data. First of all we will check whether some of the variables have missing values (in the form of NaN, NA and Null). If so, we will try to answer: what should be done about them? We will determine whether some columns shall be removed and whether some can be modified or created. After, we will focus our efforts on determining outliers and ways to deal with them.

Before we proceed with the missing values analysis, let's perform a couple of quick fixes to our data:\
- Rename the variable thinness..1.19.years to be more representative of its true meaning ("Prevalence of thinness among children and adolescents for Age 10 to 19 (%)").\
- Convert the character columns to factor columns (ToDo: Explain why this is useful)

```{r}

colnames(LifeExp)[colnames(LifeExp) == "thinness..1.19.years"] <- "thinness.10.19.years"

# Convert a character column to a factor column
LifeExp$Year    <- factor(LifeExp$Year)
LifeExp$Country <- factor(LifeExp$Country)
LifeExp$Status  <- factor(LifeExp$Status)

str(LifeExp)
```
### Checking unique values

Let's have a look at the unique values in our dataset.\
We do this just to double check that number of unique values corresponds to the dataset's description.

```{r}
# Apply the length function to unique values in each column using sapply()
num_unique_values <- sapply(LifeExp, function(x) length(unique(x)))

# Print the number of unique values for each column
for (i in 1 : ncol(LifeExp)) {
  column_name <- names(LifeExp)[i]
  num_unique <- num_unique_values[i]
  cat(column_name, ": ", num_unique, "\n")
}
```


### Missing Values

To address missing values, the following steps can be taken:\
1. Detection of missing values:
  - Identify null values in the dataset.
  - Consider if there are any alternative representations of missing values, such as zero values.
2. Dealing with missing values:
  - Decide whether to fill the null values through imputation or interpolation techniques.
  - Alternatively, consider removing the null values from the dataset if appropriate.

Lets first check the existence of explicit null values.

```{r}
is.null(LifeExp)
sum(is.na(LifeExp))
sum(is.nan(as.matrix(LifeExp)))
```

```{r}
colSums(is.na(LifeExp))
```

As we can see most of the columns contain missing values (NA), resulting in a total of 2563 missing entries. Before we decide how to deal with them, let's check whether there are some wrong entries, which we call inexplicit nulls. The simplest and fastest way to check for such entries would be to do a quick summary() and look at each variable separately and see if the values make sense based on their descriptions.

```{r}
summary(LifeExp)
```
(ToDo: Rewrite the interpretations if needed)

Judging by the summary for the variables, it seems that some values in the dataset may be inaccurate or require further investigation. Here's a breakdown of the potential issues:

1. Adult mortality of 1: It's highly unlikely for adult mortality to be as low as 1. This could indicate a measurement error. We can set a certain threshold and consider all the values below it as null.

2. Infant deaths of 0 and 1800: Having zero infant deaths per 1000 births is implausible, so it's reasonable to consider such values as null. On the other hand, 1800 infant deaths may be an outlier but could be possible in certain circumstances, such as high birthrates and a relatively small population. Further analysis is needed to determine if it's an outlier or an accurate measurement.

3. BMI of 1 and 87.3: These values seem unrealistic, as it would indicate extreme underweight or obesity across an entire population. These values may be inaccurate or outliers. Considering the nature of this variable, it might be worth investigating further or even disregarding if the data quality is questionable.

4. Under Five Deaths at zero: Similar to infant deaths, reporting zero deaths for children under five is highly unlikely. It's reasonable to treat such values as null or investigate the data source for clarification.

5. GDP per capita of 1.68: Extremely low GDP values like 1.68 (in USD) may be implausible. These low values could be outliers or inaccuracies. Further analysis and comparison with reliable sources could help determine their validity.

6. Population of 34: A population value of 34 for an entire country appears questionable. It's likely an outlier or an error in measurement that requires verification or further investigation.

It's important to note that these are assumptions based on the provided summary statistics and our domain knowledge. More thorough analysis may be necessary to make accurate conclusions about the data's quality and identify the best approach for handling these inconsistencies.

To gain a deeper understanding of these values, let's visualize them using boxplots for each variable.

```{r}
par(mfrow = c(2, 3))
variables <- c('Adult.Mortality', 'infant.deaths', 'BMI', 'under.five.deaths', 'GDP', 'Population')

for (i in 1:length(variables)) {
  boxplot(LifeExp[, variables[i]], main = variables[i])
}
```

(ToDo: Rewrite the following part if needed)

After examining the mentioned values, it appears that some of them could potentially be outliers, while others are most likely errors. To address these inconsistencies, the following modifications will be made, considering that the values are implausible:

- Adult mortality rates lower than the 5th percentile will be treated as null.\
- Infant deaths of 0 will be considered null.\
- BMIs below 10 and above 50 will be considered null.\
- Under Five deaths of 0 will be considered null.\
By making these adjustments, we can handle the values that do not align with reasonable expectations.

```{r}
num_na_before <- sum(colSums(is.na(LifeExp)))
mort_5_percentile <- quantile(LifeExp$Adult.Mortality[!is.na(LifeExp$Adult.Mortality)], probs = 0.05)
LifeExp$Adult.Mortality <- ifelse(LifeExp$Adult.Mortality < mort_5_percentile, NA, LifeExp$Adult.Mortality)
LifeExp$infant.deaths <- ifelse(LifeExp$infant.deaths == 0, NA, LifeExp$infant.deaths)
LifeExp$BMI <- ifelse(LifeExp$BMI < 10 | LifeExp$BMI > 50, NA, LifeExp$BMI)
LifeExp$under.five.deaths <- ifelse(LifeExp$under.five.deaths == 0, NA, LifeExp$under.five.deaths)
```

```{r}
cat(num_na_before, sum(colSums(is.na(LifeExp))))
```
As we can see, we increased the number of missing values from 2563 to 5763. There seems to be a significant number of null values in the dataset. For now, it might be helpful to analyze separately just the columns that contain nulls to gain more insights. The following function aims to achieve this. It identifies and presents the columns that have explicit null values, tracks the total count of such columns along with their positions in the dataframe, and provides the count and percentage of nulls for each specified column.

```{r}
nulls_breakdown <- function(df) {
  df_cols <- colnames(df)
  cols_total_count <- length(df_cols)
  cols_count <- 0
  for (loc in 1:length(df_cols)) {
    col <- df_cols[loc]
    null_count <- sum(is.na(df[[col]]))
    total_count <- length(df[[col]])
    percent_null <- round(null_count / total_count * 100, 2)
    if (null_count > 0) {
      cols_count <- cols_count + 1
      cat("[", loc - 1, "] ", col, " has ", null_count, " null values: ", percent_null, "% null\n", sep = "")
    }
  }
  cols_percent_null <- round(cols_count / cols_total_count * 100, 2)
  cat("Out of ", cols_total_count, " total columns, ", cols_count, " contain null values; ", cols_percent_null, "% columns contain null values.\n", sep = "")
}

nulls_breakdown(LifeExp)
```
#### Dealing with missing values

Given that approximately half of the values in the BMI variable are null, we decided that it would be the best to exclude this variable from the analysis.

```{r}
LifeExp <- LifeExp[, !(colnames(LifeExp) %in% c('BMI'))]
```

When dealing with NA values, there are multiple approaches that can be considered:

- Fill with the mean value: Replace the NA values with the mean value of the respective variable.
- Dropping:
  - Drop all rows that contain at least one NA value: Remove the rows from the dataset that have any NA values.
  - Drop rows that contain more than 50% NA values: Exclude the rows that have more than 50% NA values, while keeping the remaining rows. The NA values can be filled with the mean value.
- Interpolation: If the data represents a time series, interpolation techniques can be applied to estimate the missing values based on the existing data points.

Since we are dealing with time series data assorted by country, the best approach would be to interpolate the data by country. However, from a careful examination of the dataframe it can be seen that there are columns that have null values for each year for some countries. Therefore, we resort to imputation by year, as the second best possible method. This involves replacing the null values with the mean value of each respective year. Imputation of each year's mean is done in the following code snippet:

```{r}
imputed_data <- list()

for (year in unique(LifeExp$Year)) {
  year_data <- LifeExp[LifeExp$Year == year, ]
  for (col in colnames(year_data)[4:length(colnames(year_data))]) {
    year_data[, col] <- ifelse(is.na(year_data[, col]), mean(year_data[, col], na.rm = TRUE), year_data[, col])
  }
  imputed_data <- append(imputed_data, list(year_data))
}

LifeExp <- bind_rows(imputed_data)

nulls_breakdown(LifeExp)
```
As we can see the missing values have been successfully handled using the interpolation method. We can now proceed to address the issue of outliers. Outliers can have a significant impact on data analysis and modeling, and it is important to identify and handle them appropriately.


### Outlier analysis

Outlier analysis is an essential step in exploring and understanding life expectancy data. It helps ensure data quality, identify anomalies and patterns, improve statistical analysis, inform policy decisions, and enhance data visualization and reporting.
Outliers increase the error variance and reduce the power of statistical tests. They can cause bias and/or influence estimates. They can also impact the basic assumption of regression as well as other statistical models.

Similar to missing values, to address the presence of outliers in the data, several steps can be taken:

1. Outlier detection: Identify the outliers in the dataset using appropriate statistical methods or techniques.
  - Visualization: Create boxplots and histograms to visually inspect the distribution of the data and identify any potential outliers.
  - Tukey's method: Apply Tukey's method or other outlier detection algorithms to determine the presence of outliers based on statistical thresholds or criteria.
2. Outlier treatment: Decide on the approach for handling outliers. Options include:
  - Dropping outliers: Remove the outlier observations from the dataset.
  - Limiting/Winsorizing outliers: Cap the extreme values at a predetermined threshold.
  - Transforming the data: Apply transformations such as logarithmic, inverse, or square root transformations to reduce the impact of outliers.

Each of these steps aims to identify, assess, and appropriately handle the outliers in the dataset to ensure robust analysis and accurate interpretations.


#### Outlier detection

For each continuous variable in the dataset, a boxplot will be generated, displaying the median, quartiles, and any potential outliers. Additionally, histograms will be created to visualize the frequency distribution of the variable.

By examining these plots, we can visually inspect the data and determine if there are any data points that deviate significantly from the majority of the observations, indicating the presence of potential outliers.

ToDo: Improve the plots here. Idea: place the boxplot above/bellow the histogram - combo plot.

```{r}
# Layout to split the screen
# Draw the boxplot and the histogram 
cont_vars <- colnames(LifeExp)[4:length(colnames(LifeExp))]
num_vars <- length(cont_vars)
num_rows <- ceiling(sqrt(num_vars))
num_cols <- ceiling(num_vars / num_rows)

par(oma = c(0, 0, 0, 0))

par(mfrow = c(num_rows, num_cols))

for (i in 1:num_vars) {
  col <- cont_vars[i]
  
  # Reduce the inner margins to make the plots larger
  par(mar = c(2, 2, 1, 1))
  
  boxplot(LifeExp[, col], horizontal = TRUE, col = rgb(0.8, 0.8, 0, 0.5), frame = FALSE, main = paste(col))
  
  # Increase the inner margins for the histogram to make it taller
  par(mar = c(4, 2, 1, 1))
  
  hist(LifeExp[, col], col = rgb(0.2, 0.8, 0.5, 0.5), main = paste(col), xlab = "")
}

```

Visually inspecting the data, it is evident that there are several outliers present in the variables, including the target variable (life expectancy). To statistically identify outliers, Tukey's method can be applied. This method defines outliers as values that lie outside 1.5 times the interquartile range (IQR). By calculating the quartiles and IQR for each variable, the upper and lower bounds can be determined, and any values outside these bounds can be classified as outliers.

```{r}
cont_vars <- colnames(LifeExp)[4:length(colnames(LifeExp))]

outlier_counts <- sapply(LifeExp[cont_vars], function(x) {
  q1 <- quantile(x, 0.25)
  q3 <- quantile(x, 0.75)
  iqr <- q3 - q1
  lower_bound <- q1 - 1.5 * iqr
  upper_bound <- q3 + 1.5 * iqr
  outliers <- x < lower_bound | x > upper_bound
  sum(outliers, na.rm = TRUE)
})

outlier_percent <- outlier_counts / nrow(LifeExp) * 100

outliers_df <- data.frame(OutlierCount = outlier_counts, OutlierPercent = outlier_percent)
kable(outliers_df, format = "markdown")

```

There seem to be a considerable number of outliers present in this dataset. Now that they have been identified, a question arises: What course of action should be taken regarding them?


#### Dealing with Outliers

There are several approaches to handling outliers in a dataset, and the typical options are as follows:

1. Discard Outliers (preferably avoided to retain maximum information):
   - This involves removing the data points that are considered outliers.
  
2. Apply Boundaries (Winsorization):
   - Set upper and/or lower limits for the values, effectively capping extreme values without removing them entirely.

3. Data Transformation:
   - Utilize mathematical transformations such as logarithmic, inverse, square root, etc.
   - Advantages: Can normalize the data and potentially eliminate outliers.
   - Disadvantages: Cannot be applied to variables containing values of zero or below, as it may lead to undefined results.


Given that each variable in the dataset exhibits a unique number of outliers and these outliers are distributed on different sides of the data, the most suitable approach would likely involve Winsorizing (limiting) the values for each variable individually until no outliers remain. The provided function bellow enables this process by iterating through each variable and allowing the specification of lower and/or upper limits for Winsorization. By default, the function generates two boxplots side by side for each variable, depicting the original data and the Winsorized data, respectively. Once a satisfactory limit is determined through visual analysis, the Winsorized data is saved in the wins_dict dictionary for convenient future access.

```{r}
test_wins <- function(col, lower_limit = 0, upper_limit = 0, show_plot = TRUE) {
  LifeExp_original <- LifeExp[[col]]
  q_low <- quantile(LifeExp[[col]], lower_limit)
  q_up <- quantile(LifeExp[[col]], 1 - upper_limit)
  LifeExp[[col]] <<- pmin(pmax(LifeExp[[col]], q_low), q_up)
  # <<- Allows us to modify variables outside of the local scope of a function.
  
  if (show_plot) {
    ylim <- range(LifeExp_original, na.rm = TRUE)
    par(mfrow = c(1, 2))
    boxplot(LifeExp_original, main = paste0("original\n ", col), ylim = ylim)
    boxplot(LifeExp[[col]], main = paste0("wins=(", lower_limit, ",", upper_limit, ")\n", col), ylim = ylim)
  }
}

test_wins(cont_vars[1], lower_limit = 0.01, show_plot = FALSE)
test_wins(cont_vars[2], upper_limit = 0.04, show_plot = FALSE)
test_wins(cont_vars[3], upper_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[4], upper_limit = 0.0025, show_plot = FALSE)
test_wins(cont_vars[5], upper_limit = 0.135, show_plot = FALSE)
test_wins(cont_vars[6], lower_limit = 0.1, show_plot = FALSE)
test_wins(cont_vars[7], upper_limit = 0.19, show_plot = FALSE)
test_wins(cont_vars[8], upper_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[9], lower_limit = 0.1, show_plot = FALSE)
test_wins(cont_vars[10], upper_limit = 0.02, show_plot = FALSE)
test_wins(cont_vars[11], lower_limit = 0.105, show_plot = FALSE)
test_wins(cont_vars[12], upper_limit = 0.185, show_plot = FALSE)
test_wins(cont_vars[13], upper_limit = 0.105, show_plot = FALSE)
test_wins(cont_vars[14], upper_limit = 0.07, show_plot = FALSE)
test_wins(cont_vars[15], upper_limit = 0.035, show_plot = FALSE)
test_wins(cont_vars[16], upper_limit = 0.035, show_plot = FALSE)
test_wins(cont_vars[17], lower_limit = 0.05, show_plot = FALSE)
test_wins(cont_vars[18], lower_limit = 0.025, upper_limit = 0.005, show_plot = FALSE)
```

(ToDo: Create prettier grahs)

```{r}
# Layout to split the screen
# Draw the boxplot and the histogram 
cont_vars <- colnames(LifeExp)[4:length(colnames(LifeExp))]
num_vars <- length(cont_vars)
num_rows <- ceiling(2)
num_cols <- ceiling(num_vars / num_rows)

par(oma = c(0, 0, 0, 0))

par(mfrow = c(num_rows, num_cols))

for (i in 1:num_vars) {
  col <- cont_vars[i]
  
  # Reduce the inner margins to make the plots larger
  par(mar = c(2, 2, 1, 1))
  
  boxplot(LifeExp[, col], col = rgb(0.8, 0.8, 0, 0.5), frame = FALSE, main = paste(col))
  
  # Increase the inner margins for the histogram to make it taller
  par(mar = c(4, 2, 1, 1))
}
```


## Exploratory Data Analysis

### Histogram visualization
```{r}
library(rcompanion)

par(mfrow=c(2,2))

pnh_life_exp <- plotNormalHistogram(LifeExp$Life.expectancy, prob = FALSE, col="#B9D9EB", border="#003f5c",main = "Life Expectancy Histogram",length = 10000, linecol="#ffa600", lwd=3)


pnh_alch <- plotNormalHistogram(LifeExp$Alcohol, prob = FALSE, col="#B9D9EB", border="#003f5c",main = "Alcohol Histogram",length = 10000, linecol="#ffa600", lwd=3)

pnh_school <-plotNormalHistogram(LifeExp$Schooling, prob = FALSE, col="#B9D9EB", border="#003f5c",main = "Schooling",length = 10000, linecol="#ffa600", lwd=3)

pnh_gdp <- plotNormalHistogram(LifeExp$GDP, prob = FALSE, col="#B9D9EB", border="#003f5c",main = "GDP Histogram",length = 10000, linecol="#ffa600", lwd=3)

pnh_gdp_log <- plotNormalHistogram(log(LifeExp$GDP), prob = FALSE, col="#B9D9EB", border="#003f5c",main = "GDP Log Histogram",length = 10000, linecol="#ffa600", lwd=3)

pnh_popul <- plotNormalHistogram(LifeExp$Population, prob = FALSE, col="#B9D9EB", border="#003f5c",main = "Population Histogram",length = 10000, linecol="#ffa600", lwd=3)

pnh_popul_log <- plotNormalHistogram(log(LifeExp$Population), prob = FALSE, col="#B9D9EB", border="#003f5c",main = "Population Log Histogram",length = 10000, linecol="#ffa600", lwd=3)

```


### 1. Developing vs Developed Countries
```{r}
ggplot(LifeExp, aes(x = Status, fill = Status)) + 
  geom_bar() + 
  scale_fill_manual(values = c("#ffa600","#bc5090")) + 
  labs(title = "Status of Country", x = "Status", y = "Count") +
  theme(axis.text = element_text(size = 8), axis.title = element_text(size = 8), 
        axis.text.y = element_text(size = 8), axis.title.y = element_text(size = 8))
```


```{r}
ggplot(LifeExp, aes(x=Life.expectancy, fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "Life Expectancy by Status", x ="Life Expectancy", y="Density") +
  scale_fill_manual(values = c("#FFA600", "#A05195"))

```

### 2.Correlation between Health expenditure and Life expectancy.

Percentage Expenditure - Expenditure on health as a percentage of Gross Domestic Product per capita (%):
This indicator calculates the proportion of total health expenditure in relation to the Gross Domestic Product (GDP) per capita. It measures the share of the country's economic output (GDP) that is spent on healthcare per person. It provides insights into the financial commitment to healthcare relative to the economic well-being of the population. Higher values indicate a larger proportion of GDP per capita allocated to healthcare, suggesting greater investment in the health sector.

Total Expenditure - General government expenditure on health as a percentage of total government expenditure (%):
This indicator focuses specifically on the proportion of government expenditure that is allocated to healthcare. It calculates the share of total government expenditure that is dedicated to healthcare services and programs. It helps assess the priority given to healthcare within the government's overall budget allocation. Higher values indicate a greater emphasis on healthcare within the government's expenditure decisions.

The key distinction lies in the denominator used to calculate the proportions. Percentage expenditure considers the Gross Domestic Product per capita as the benchmark, while total expenditure focuses on the proportion of government expenditure dedicated to healthcare. Both indicators provide different perspectives on the allocation of resources to healthcare and can be used to evaluate financial commitments and priorities in the healthcare sector.

```{r}
life_expectancy_vs_percentage_expenditure <-  ggplot(LifeExp, aes(percentage.expenditure, Life.expectancy)) + 
                                      geom_jitter(color = "yellow", alpha = 0.5) + theme_light()

life_expectancy_vs_Total_expenditure  <- ggplot(LifeExp, aes(Total.expenditure, Life.expectancy)) +
                                      geom_jitter(color = "purple", alpha = 0.5) + theme_light()

p <- plot_grid(life_expectancy_vs_percentage_expenditure, life_expectancy_vs_Total_expenditure) 
title <- ggdraw() + draw_label("Correlation between Health expenditure and Life expectancy", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))
```
We can see from the above graph that the Higher Life Expectancy is more concentrated when expenditure varies from 5k - 20k.


### 3.Correlation between Health expenditure and Immunization.

```{r}
life_expectancy_vs_Hepatitis_B <- ggplot(LifeExp, aes(Hepatitis.B, Life.expectancy)) + geom_jitter(color = "purple", alpha = 0.5) + theme_light()

life_expectancy_vs_Diphtheria  <- ggplot(LifeExp, aes(Diphtheria, Life.expectancy)) + geom_jitter(color = "orange", alpha = 0.5) + theme_light()
                              
life_expectancy_vs_Polio  <- ggplot(LifeExp, aes(Polio, Life.expectancy)) + geom_jitter(color = "pink", alpha = 0.5) + theme_light()

life_expectancy_vs_Measles  <- ggplot(LifeExp, aes(Measles, Life.expectancy)) + geom_jitter(color = "light green", alpha = 0.5) + theme_light()

p <- plot_grid(life_expectancy_vs_Hepatitis_B, life_expectancy_vs_Diphtheria, life_expectancy_vs_Polio, life_expectancy_vs_Measles) 
title <- ggdraw() + draw_label("Correlation between Immunizations and life expectancy", fontface='bold')
plot_grid(title, p, ncol=1, rel_heights=c(0.1, 1))
```
### 4. Life expectancy and Alcohol

Developed countries tend to have higher levels of alcohol consumption compared to developing countries. This can be attributed to various factors such as higher income levels, greater access to alcohol, more established alcohol industries, and different cultural norms surrounding alcohol.
Higher GDP at Developed countries can influence alcohol consumption patterns to some extent. As countries experience economic growth and an increase in GDP, there is often an associated rise in income levels and discretionary spending power. This can lead to increased alcohol consumption. 

```{r}
ggplot(LifeExp, aes(x=Alcohol, fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "Alcohol consumption by Status", x ="Alcohol", y="Density") +
  scale_fill_manual(values = c("#FFA600", "#A05195"))
```
```{r}
ggplot(LifeExp, aes(x=log(GDP), fill=Status)) +
    geom_density(alpha=.5) +
    labs(title  = "GDP by Status", x ="GDP", y="Density") +
  scale_fill_manual(values = c("#FFA600", "#A05195"))
```

```{r}
life_expectancy_vs_Alcohol  <- ggplot(LifeExp, aes(Alcohol, Life.expectancy)) + geom_jitter(color = "#A05195", alpha = 0.5) + theme_light()
life_expectancy_vs_Alcohol 
```

### 5. Correlation between Life expectancy and GDP
```{r}
life_expectancy_vs_GDP  <- ggplot(LifeExp, aes(GDP, Life.expectancy)) + geom_jitter(color = "lightblue", alpha = 0.5) + theme_light()
life_expectancy_vs_GDP 
```

### 6. Correlation between Life expectancy Thinness

```{r}
life_expectancy_vs_thin5_9  <- ggplot(LifeExp, aes(thinness.5.9.years, Life.expectancy)) + geom_jitter(color = "orange", alpha = 0.5) + theme_light()
                              
life_expectancy_vs_thin10_19 <- ggplot(LifeExp, aes(thinness.10.19.years, Life.expectancy)) + geom_jitter(color = "pink", alpha = 0.5) + theme_light()

m <- plot_grid(life_expectancy_vs_thin5_9, life_expectancy_vs_thin10_19) 
title <- ggdraw() + draw_label("Correlation between Thinness and Life expectancy", fontface='bold')
plot_grid(title, m, ncol=1, rel_heights=c(0.1, 1))
```


### 7. Correlation between Life expectancy and Income composition of resources

The term "Income composition of resources" refers to a component of the Human Development Index (HDI), which is a measure used to assess the overall development and well-being of a country's population. The Income composition of resources specifically focuses on income-related indicators and their contribution to human development.

In the context of the HDI, the Income composition of resources index is a numerical value that ranges from 0 to 1. It reflects the extent to which a country's income distribution contributes to overall human development.

A higher value of the Income composition of resources index indicates a more equitable distribution of income, where a larger proportion of the population has access to resources and opportunities for development. This suggests that income is shared more evenly among individuals within the country.

Conversely, a lower value of the index indicates a more unequal income distribution, with a smaller proportion of the population having access to resources and opportunities for development. This suggests that income is concentrated in the hands of a smaller segment of the population.


```{r}
life_expectancy_vs_incomecomp <- ggplot(LifeExp, aes(Income.composition.of.resources, Life.expectancy)) + geom_jitter(color = "purple", alpha = 0.5) + theme_light()

life_expectancy_vs_incomecomp
```
By this scatterplot we can see that The Income composition of resources and their contribution to human development positively influences the Life Expectancy.

## Constructing Correlation Matrix

```{r}
# Compute the correlation matrix
numeric_vars <- LifeExp[, sapply(LifeExp, is.numeric)]
cor_matrix <- cor(numeric_vars)

# Plot the correlation matrix using corrplot
corrplot::corrplot(cor_matrix, method = "color", type = "upper", tl.cex = 0.7)

# Adjust the text size

```




